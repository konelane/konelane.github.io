<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[算法准备|刷题，变强]]></title>
    <url>%2F2021%2F10%2F12%2F%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[努力力扣-都得学一学啊，不刷题被好几位面试官gank了。我也真是可以，一路过关斩将，与那么多智者不谋而合，项目还行，算法题的思路全对，但是代码一个不会，一写就废。大概是眼高手低了。最难受是面试官反问：“你难道没有刷题吗？”的时候，察觉机会从眼前溜走。 哪怕再忙碌，也要做题啊。绝知此事要躬行！ 力扣 日期 题目 难度 备注 10.12 78. 子集 中 遍历迭代(快)/回溯(官方) 10.13 90. 子集 II 中 去重，利用排序巧妙去重 10.13 117. 填充每个节点的下一个右侧节点指针 II 中 层序遍历-队列BFS，进阶要求O(1)的空间复杂度，相当于多一个指针next，检查当前节点有没有下一个可连的，并随之移动 10.13 46. 全排列 中 来自移动研究院笔试……递归调用排列/或者用那个xy换翻的四步法，如果想要字典序，则大概需要sort一下 10.13 442. 数组中重复的数据 易 来自室友的笔试（非算法岗）……力扣上AC了但牛客却无论如何只给50% 10.14 572. 另一棵树的子树 易 其实是两道题-两棵树是否一样+寻找是否子树（学习一下递归） 10.14 47. 全排列 II 中 巩固一下昨天的知识……还是不熟，采用指针换位法 10.16 438. 找到字符串中所有字母异位词 中 如果先用回溯搞全排列就慢了，应该用滑动窗口+双指针/ord给字母定位的妙用（有一个很耗时的操作，字符串转list） 10.16 713. 乘积小于K的子数组 中 没写出来，对于滑动窗口中加一个元素的变化不够熟悉 10.16 209. 长度最小的子数组 中 和上一题不太一样，注意for循环与while循环的位置 10.26 剑指 Offer 19. 正则表达式匹配 难 百融云创面试题，dp 10.27 39. 组合总和 中 组合问题依然是dfs+回溯，组合与排列的不一样的地方是遍历的次数增加了，要包含初始位置 10.28 509. 斐波那契数 易 dp的入门 40. 组合总和 II 17. 电话号码的字母组合 22. 括号生成 79. 单词搜索 213. 打家劫舍 II 55. 跳跃游戏 45. 跳跃游戏 II 62. 不同路径 5. 最长回文子串 413. 等差数列划分 91. 解码方法 139. 单词拆分 300. 最长递增子序列 673. 最长递增子序列的个数 1143. 最长公共子序列 583. 两个字符串的删除操作 72. 编辑距离 322. 零钱兑换 343. 整数拆分 201. 数字范围按位与 位运算 384. 打乱数组 202. 快乐数 149. 直线上最多的点数 面试/笔试的一些题 日期 题目 备注 09.22 找到一个数组的中位数，要求时间复杂度O(n) 快排 09.23 梯度下降法解根号5的精确值。 重要的是写损失函数的导数]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习|站在巨人的肩膀上——迁移学习]]></title>
    <url>%2F2021%2F09%2F22%2F210922transfer%2F</url>
    <content type="text"><![CDATA[又是一学期过去了，博客也许久未更新，生活在鸡零狗碎中仓皇逃窜，留下一地的残篇断页，无人捡拾。而我，总算是鼓起勇气，拿起笔开始写一些总结了。 本文介绍的是深度学习中迁移学习的入门级知识。项目是迪士尼公主图像识别。 苦了我了！ 数据预处理说起来，我作为一个大老爷们，从来没有注意过迪士尼公主的事，直到有一天，在这门名叫“大数据统计建模与深度学习”的课上，我与四位女同学组队，找感兴趣的话题时，才第一次知道迪士尼有这么多公主（我本身的推荐是识别军用船和民用船，被直接pass=.=），而队友们跃跃欲试，我大呼上当快跑，可为时已晚。 本文用到的数据，竟然是我们几个现场爬下来的（因此，预期也不是很高），百度、bilibili的视频截图，一共十四类，2333张。 公主们的名字：乐佩 宝嘉康蒂 灰姑娘 爱洛公主 艾莎 茉莉公主 蒂安娜公主安娜 梅丽达公主 爱丽儿 白雪公主 花木兰 莫安娜 贝儿公主 这里随口一提，tf2.0与keras组合绝对是萌新的不二之选。 123456789101112131415161718192021222324252627282930313233import tensorflow as tffrom keras.layers import Activation, Conv2D, BatchNormalization, Dense, add # 实现相加的模块from keras.layers import Dropout, Flatten, Input, MaxPooling2D, ZeroPadding2D, AveragePooling2D,GlobalAveragePooling2Dfrom keras import Modelfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.optimizers import Adamfrom keras import backend as K# 基础import osimport pickleimport pandas as pdimport numpy as np# 画图from matplotlib import pyplot as pltplt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签# 将中文转成拼音import pinyin.cedict#### -------- TF-GPU --------gpus = tf.config.experimental.list_physical_devices('GPU')if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs") except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) 调包的过程其实可以一段一段来，我们进行了整理。如果需要使用GPU，可以用这段代码查看是否有可用的GPU。（当然，需要TensorFlow-gpu版本。） 12345678910# 指定公主列表princess_list = ['艾莎','爱丽儿','爱洛','安娜','白雪','宝嘉康蒂','贝儿','蒂安娜', '花木兰','灰姑娘','乐佩','梅丽达公主','茉莉公主','莫安娜']# 设定数据文件夹和保存output文件夹# data_dir = "/mnt/princess/data"data_dir = './data' # yuanhe# output_path = "/mnt/princess/output"output_path = './output' # yuanheif os.path.exists(output_path) == False : os.mkdir(output_path) 上面的代码是对图片数据的位置进行了整理。都是准备工作。下面正片就要开始了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 数据集拆分函数(由于每个网络的输入size不同,需要指定target_size)def load_data(im_size, batch_size=64): # 数据增强 datagen = ImageDataGenerator( rescale=1./255, shear_range=0.5, # 拉伸变换 rotation_range=30, # 左右旋转角度 zoom_range=0.2, # 放大和缩小的比例不超过1.2 width_shift_range=0.2, # 水平方向上平移的尺度 height_shift_range=0.2, # 垂直方向 horizontal_flip=True, validation_split = 0.2 # 训练集和验证集拆分比例 ) # 训练集数据 train_generator = datagen.flow_from_directory( data_dir, target_size=(im_size, im_size), batch_size=batch_size, classes=princess_list, class_mode='categorical', seed=1024, subset='training') # set as training data # 验证集数据 validation_generator = datagen.flow_from_directory( data_dir, target_size=(im_size, im_size), batch_size=batch_size, classes=princess_list, class_mode='categorical', shuffle=False, seed=1024, subset='validation') # set as validation data return (train_generator,validation_generator) # 拆分训练集(train_generator,validation_generator) = load_data(im_size=224, batch_size=64)X_train,y_train = next(train_generator)X_test ,y_test = next(validation_generator)# 一个batch的数据集数量print(X_train.shape)print(y_train.shape)# 显示标签对应字典train_generator.class_indices 数据增强是一种常见的扩大训练集的方式，通过旋转、镜像、变化、裁切等方式进行数据扩充。 最终输出如下： Found 1873 images belonging to 14 classes.Found 460 images belonging to 14 classes.(64, 224, 224, 3)(64, 14){‘艾莎’: 0,‘爱丽儿’: 1,‘爱洛’: 2,‘安娜’: 3,‘白雪’: 4,‘宝嘉康蒂’: 5,‘贝儿’: 6,‘蒂安娜’: 7,‘花木兰’: 8,‘灰姑娘’: 9,‘乐佩’: 10,‘梅丽达公主’: 11,‘茉莉公主’: 12,‘莫安娜’: 13} 12345678910111213# 预览图片plt.figure()fig,ax = plt.subplots(2,7)fig.set_figheight(5)fig.set_figwidth(15)ax = ax.flatten()for i in range(14): ax[i].imshow(X_train[i,:,:,:]) # 标题显示公主名字 prin_name = princess_list[np.where(y_train[i]==1)[0][0]] # ax[i].set_title(prin_name) # 用拼音显示 ax[i].set_title(pinyin.get(prin_name, format="strip", delimiter=" ")) 公主们的样貌也展示如下。 模型应用本节均使用通过imagenet预训练好的VGG16、ResNet、InceptionV3、MobileNet卷积层参数，并在此基础上训练14分类的全连接层。 VGG16123456789101112131415161718192021222324# 重新启动keras内存from keras import backend as KK.clear_session()from keras.applications.vgg16 import VGG16# vgg16默认输入尺寸是 224x224，指定im_size为224(train_vgg16,validation_vgg16) = load_data(im_size = 224, batch_size = 64)# 构建模型# 导入已训练参数vgg_base = VGG16(weights='imagenet', include_top=False)vgg_x = vgg_base.outputvgg_x = GlobalAveragePooling2D()(vgg_x)vgg_x = Dense(128,activation='relu')(vgg_x)vgg_pred = Dense(14,activation='softmax')(vgg_x)vgg_model = Model(inputs=vgg_base.input, outputs=vgg_pred)# 不再训练前面层for layer in vgg_base.layers: layer.trainable = Falsevgg_model.summary() 输出如下（即VGG16网络的结构）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152Model: &quot;model_1&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) (None, None, None, 3) 0 _________________________________________________________________block1_conv1 (Conv2D) (None, None, None, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, None, None, 64) 36928 _________________________________________________________________block1_pool (MaxPooling2D) (None, None, None, 64) 0 _________________________________________________________________block2_conv1 (Conv2D) (None, None, None, 128) 73856 _________________________________________________________________block2_conv2 (Conv2D) (None, None, None, 128) 147584 _________________________________________________________________block2_pool (MaxPooling2D) (None, None, None, 128) 0 _________________________________________________________________block3_conv1 (Conv2D) (None, None, None, 256) 295168 _________________________________________________________________block3_conv2 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________block3_conv3 (Conv2D) (None, None, None, 256) 590080 _________________________________________________________________block3_pool (MaxPooling2D) (None, None, None, 256) 0 _________________________________________________________________block4_conv1 (Conv2D) (None, None, None, 512) 1180160 _________________________________________________________________block4_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________block4_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________block4_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________block5_conv1 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________block5_conv2 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________block5_conv3 (Conv2D) (None, None, None, 512) 2359808 _________________________________________________________________block5_pool (MaxPooling2D) (None, None, None, 512) 0 _________________________________________________________________global_average_pooling2d_1 ( (None, 512) 0 _________________________________________________________________dense_1 (Dense) (None, 128) 65664 _________________________________________________________________dense_2 (Dense) (None, 14) 1806 =================================================================Total params: 14,782,158Trainable params: 67,470Non-trainable params: 14,714,688_________________________________________________________________ 训练的过程就是让原有的权重对现有数据进行过拟合的过程。 1234567891011121314151617181920212223# 训练vgg_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'] )vgg_history = vgg_model.fit_generator(train_vgg16 ,validation_data=validation_vgg16# ,epochs=100 ,epochs=1 ) # 保存结果# 创建结果保存文件夹if os.path.exists(output_path + '/model') == False : os.mkdir(output_path + '/model')if os.path.exists(output_path + '/acc') == False : os.mkdir(output_path + '/acc')# 保存正确率结果with open(output_path + '/acc/vgg16_history.txt', 'wb') as f: pickle.dump(vgg_history.history, f)# 保存模型结果vgg_model.save(output_path + "/model/vgg16.h5") ResNet1234567891011121314151617181920212223242526# 重新启动keras内存K.clear_session()from keras.applications.resnet import ResNet50# resnet50默认输入尺寸是 224x224(train_resnet50,validation_resnet50) = load_data(im_size=224, batch_size = 64)# 构建模型# 导入已训练参数resnet_base = ResNet50(weights='imagenet',include_top=False)resnet_x = resnet_base.outputresnet_x = GlobalAveragePooling2D()(resnet_x)# 在最后加入14分类的全连接层resnet_x = Dense(128,activation='relu')(resnet_x)resnet_pred = Dense(14,activation='softmax')(resnet_x)resnet_model = Model(inputs = resnet_base.input ,outputs = resnet_pred)# 只训练最后一层for layer in resnet_base.layers: layer.trainable = Falseresnet_model.summary() 我本来想对模型进行最完整的展示，但其实适合放一张图上来。中间的网络细节就不展示了。 1234567Model: &quot;model_1&quot; ==================================================================================================Total params: 23,851,790Trainable params: 264,078Non-trainable params: 23,587,712__________________________________________________________________________________________________ 12345678910111213141516171819202122# 训练resnet_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'] )resnet_history = resnet_model.fit_generator(train_resnet50 ,validation_data=validation_resnet50# ,epochs=100 ,epochs=1 ) # 保存结果# 创建结果保存文件夹if os.path.exists(output_path + '/model') == False : os.mkdir(output_path + '/model')if os.path.exists(output_path + '/acc') == False : os.mkdir(output_path + '/acc')# 保存正确率结果with open(output_path + '/acc/resnet_history.txt', 'wb') as f: pickle.dump(vgg_history.history, f)# 保存模型结果resnet_model.save(output_path + "/model/resnet.h5") 其他模型（InceptionV3）甚至，后面的代码我想一并贴出来。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 重新启动keras内存K.clear_session()from keras.applications.inception_v3 import InceptionV3(train_InceptionV3, validation_InceptionV3) = load_data(im_size=224, batch_size= 64)# 构建模型# 导入已训练参数inceV3_base = InceptionV3(weights='imagenet',include_top=False)inceV3_x = inceV3_base.outputinceV3_x = GlobalAveragePooling2D()(inceV3_x)# 在最后加入14分类的全连接层inceV3_x = Dense(128,activation='relu')(inceV3_x)inceV3_pred = Dense(14,activation='softmax')(inceV3_x)InceptionV3_model = Model(inputs = inceV3_base.input ,outputs = inceV3_pred)# 只训练最后一层for layer in inceV3_base.layers: layer.trainable = False InceptionV3_model.summary()# 训练InceptionV3_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'] )InceptionV3_history = InceptionV3_model.fit_generator(train_InceptionV3 ,validation_data=validation_InceptionV3 ,epochs=100# ,epochs=1 ) # 保存结果# 创建结果保存文件夹if os.path.exists(output_path + '/model') == False : os.mkdir(output_path + '/model')if os.path.exists(output_path + '/acc') == False : os.mkdir(output_path + '/acc')# 保存正确率结果with open(output_path + '/acc/InceptionV3_history.txt', 'wb') as f: pickle.dump(InceptionV3_history.history, f)# 保存模型结果InceptionV3_model.save(output_path + "/model/InceptionV3.h5") MobileNet 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 重新启动keras内存K.clear_session()from keras.applications import MobileNet(train_MobileNet, validation_MobileNet) = load_data(im_size=224, batch_size=64)# 构建模型# 导入已训练参数mobile_base = MobileNet(weights='imagenet',include_top=False)mobile_x = mobile_base.outputmobile_x = GlobalAveragePooling2D()(mobile_x)# 在最后加入激活函数和14分类的全连接层mobile_x = Dense(128, activation='relu')(mobile_x)MobileNet_pred = Dense(14, activation='softmax')(mobile_x)MobileNet_model = Model(inputs = mobile_base.input ,outputs = MobileNet_pred)# 只训练最后一层for layer in mobile_base.layers: layer.trainable = False MobileNet_model.summary()# 训练MobileNet_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])MobileNet_history = MobileNet_model.fit_generator(train_MobileNet ,validation_data=validation_MobileNet ,epochs=100# ,epochs=2 )# 保存结果# 创建结果保存文件夹if os.path.exists(output_path + '/model') == False : os.mkdir(output_path + '/model')if os.path.exists(output_path + '/acc') == False : os.mkdir(output_path + '/acc')# 保存正确率结果with open(output_path + '/acc/MobileNet_history.txt', 'wb') as f: pickle.dump(MobileNet_history.history, f)# 保存模型结果MobileNet_model.save(output_path + "/model/MobileNet.h5") 训练的过程千篇一律，因为我们站在巨人的肩膀上，相当于只是对最后一层全连接进行了一定程度的“过拟合”。 而出乎所有人预料的是，效果竟然还可以。 模型结果评价模型学习曲线123456789101112# 输出指定模型的学习曲线def plot_learning_curves(history): pd.DataFrame(history.history).plot(figsize = (8,5)) plt.grid(True) plt.gca().set_ylim(0.2,1.2)# plt.gca().set_xlim(0,100) plt.show() plot_learning_curves(vgg_history)plot_learning_curves(resnet_history)plot_learning_curves(InceptionV3_history)plot_learning_curves(MobileNet_history) 上面的图是我训练时候的（草稿里的），粘过来也看看。 总的来说，还是mobilenet最好，既满足了轻量化需求，又显示出超高的准确率，验证集上效果也不错。只是图有点糊了……没找到原图。 模型准确率1234567891011121314151617181920212223242526272829303132333435363738# 从保存的结果导入每个模型的正确率model_name = []train_acc = []val_acc = []path_model_result = output_path + "/acc" # 模型结果存储路径# 遍历取文件for file_name in os.listdir(path_model_result): model_name.append(file_name.split('_')[0]) #文件名都以_分隔，并且第一个元素都是模型名称 with open(os.path.join(path_model_result, file_name), 'rb') as f: model_history = pickle.load(f) train_acc.append(model_history["accuracy"]) val_acc.append(model_history["val_accuracy"]) # 测试集和验证集的正确率对比plt.figure(dpi=100)epoch_list = range(1, 101)color_set=['red','blue','green','yellow']for model_loc in range(len(model_name)): plt.plot(epoch_list, train_acc[model_loc], marker='o', # color=color_set[model_loc], label=model_name[model_loc]+'Train') # 训练集正确率 plt.plot(epoch_list, val_acc[model_loc], marker='*', # color=color_set[model_loc], label=model_name[model_loc]+'Validation') # 验证集正确率plt.ylim(0.2, 1.0)plt.legend() # 让图例生效plt.xticks(epoch_list) # 横坐标显示为整数plt.xlabel(u"epoch") # X轴标签plt.ylabel("Accuracy") # Y轴标签plt.title("Comparison of Accuracy") # 标题plt.show() 什么，我的代码这里竟然出现了些问题，存档时候丢了什么，没能跑出来qwq。请读者自行脑补（bushi 不放图的理由增加了，拒绝所有“云丹师”。 比较有学习意义的是下面这个，名叫“错分分析”的部分。目的是看一看这些模型的混淆矩阵。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from sklearn.metrics import confusion_matrixdef plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Greens,#这个地方设置混淆矩阵的颜色主题 normalize=True): from itertools import product plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签 accuracy = np.trace(cm) / float(np.sum(cm)) misclass = 1 - accuracy if cmap is None: cmap = plt.get_cmap('Blues') plt.figure(figsize=(15, 12)) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() if target_names is not None: tick_marks = np.arange(len(target_names)) plt.xticks(tick_marks, target_names, rotation=45) plt.yticks(tick_marks, target_names) if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] thresh = cm.max() / 1.5 if normalize else cm.max() / 2 for i, j in product(range(cm.shape[0]), range(cm.shape[1])): if normalize: plt.text(j, i, "&#123;:0.4f&#125;".format(cm[i, j]), horizontalalignment="center", color="white" if cm[i, j] &gt; thresh else "black") else: plt.text(j, i, "&#123;:,&#125;".format(cm[i, j]), horizontalalignment="center", color="white" if cm[i, j] &gt; thresh else "black") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label\naccuracy=&#123;:0.4f&#125;; misclass=&#123;:0.4f&#125;'.format(accuracy, misclass)) #这里这个savefig是保存图片，如果想把图存在什么地方就改一下下面的路径，然后dpi设一下分辨率即可。 plt.savefig('./confusionmatrix_14.png',dpi=1000) plt.show()def plot_confuse(predictions,true_label,target_names): # predictions = model.predict(x_val).argmax(axis=-1) truelabel = true_label conf_mat = confusion_matrix(y_true=truelabel, y_pred=predictions) plt.figure() plot_confusion_matrix(conf_mat, normalize=False,target_names = target_names,title='Confusion Matrix') 123456789101112131415161718192021222324252627# 读取模型from keras.models import load_modelmodel_path = output_path + "/model"# 选择一个最好的模型vgg_model = load_model(model_path + "/vgg16.h5")# resnet_model = load_model(model_path + "/resnet.h5")# InceptionV3_model = load_model(model_path + "/InceptionV3.h5")# MobileNet_model = load_model(model_path + "/MobileNet.h5")# 并读取对应的数据集(不用的模型im_size不同而已)model = vgg_model(train_generator,validation_generator) = load_data(im_size=224, batch_size=64)# 输出每个图像的预测类别pred = model.predict_generator(validation_generator, verbose=1)predicted_class_indices = np.argmax(pred, axis=1) # 画图也要用到# 测试集的真实类别true_label= validation_generator.classes#使用pd.crosstab来简单画出混淆矩阵table=pd.crosstab(true_label ,predicted_class_indices ,colnames=['Predict label'] ,rownames=['True label'],)print(table) 输出如下： 12345678910111213# 输出每个图像的预测类别pred = model.predict_generator(validation_generator, verbose=1)predicted_class_indices = np.argmax(pred, axis=1) # 画图也要用到# 测试集的真实类别true_label= validation_generator.classes#使用pd.crosstab来简单画出混淆矩阵table=pd.crosstab(true_label ,predicted_class_indices ,colnames=[&apos;Predict label&apos;] ,rownames=[&apos;True label&apos;],)print(table) 课程作业的收尾自然是画一张有趣的图，混淆矩阵。用来具体分析判错样本的类别。 1234567princess_list = list(validation_generator.class_indices.keys())# 用之前写的函数画混淆矩阵plot_confuse(predicted_class_indices ,validation_generator.classes ,princess_list) 以及最后再补几句像模像样的分析。 123456789101112# 模型最爱的公主print('公主的数量:\n',table.sum(axis=1),'\n\n')print('预测的公主数量:\n',table.sum(axis=0),'\n\n')pred_true_ratio = table.sum(axis=0) / table.sum(axis=1) # 按行加是正确的数量names = list(validation_generator.class_indices.keys())print('各类的预测数量/真实数量的比例:\n', pred_true_ratio,'\n\n')print('公主名称:\n',pd.Series(names),'\n\n')print('比例最低的“小冷清”：',list(validation_generator.class_indices.keys())[np.argmin(pred_true_ratio)])print('比例最高的“大众脸”：',list(validation_generator.class_indices.keys())[np.argmax(pred_true_ratio)]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172公主的数量: True label0 311 562 203 224 305 446 527 208 339 2410 3311 3612 3613 23dtype: int64 预测的公主数量: Predict label0 111 1553 34 55 966 958 510 3311 5013 7dtype: int64 各类的预测数量/真实数量的比例: 0 0.3548391 2.7678572 NaN3 0.1363644 0.1666675 2.1818186 1.8269237 NaN8 0.1515159 NaN10 1.00000011 1.38888912 NaN13 0.304348dtype: float64 公主名称: 0 艾莎1 爱丽儿2 爱洛公主3 安娜4 白雪公主5 宝嘉康蒂6 贝儿公主7 蒂安娜公主8 花木兰9 灰姑娘10 乐佩11 梅丽达公主12 茉莉公主13 莫安娜dtype: object 比例最低的“小冷清”： 安娜比例最高的“大众脸”： 爱丽儿 看一看错分样本，尝试找到错分的原因。主要还是训练集风格不统一，有2D有3D，有官方作品有同人作品，有漫画作品有电影作品…… 12345678910111213141516171819202122232425262728293031# 展示错判的图（错分样本）datapred = predicted_class_indices.tolist()datatrue = validation_generator.classes.tolist()wrong_len = validation_generator.n - list(map(lambda x: x[0]-x[1], zip(datapred, datatrue))).count(0)print('一共错误预判了 %d 张图片' % wrong_len) ## 一共错误预判了 45 张图片# 重置数据集validation_generator.reset()X,Y = next(validation_generator)# 设置画布plt.figure()fig,ax = plt.subplots(wrong_len//5+1,5)fig.set_figheight(wrong_len)fig.set_figwidth(15)ax = ax.flatten()# 遍历输出每一张错判的图wrong_list = []k = 0for i in range(validation_generator.n): # 每一个X里只有64张图片 如果想打100以外的index 得再next一下 if i % 64 == 0 and i != 0: X,Y = next(validation_generator) if list(map(lambda x: x[0]-x[1], zip(datapred, datatrue)))[i] != 0: wrong_list.append(i) ax[k].imshow(X[i%64,:,:,:]) ax[k].set_title('pred:'+ str(princelist[datapred[i]]) + 'true:' + str(princelist[datatrue[i]])) k += 1 分类正确率最高的为灰姑娘(100%:12/12)，最低的为爱洛(40%:4/10)；最容易被错认的公主为莫安娜(8次)，最不容易被错认的公主为茉莉公主(1次)。 其中，白雪公主有4个样本被错判成莫安娜，错分次数最高，而白雪公主的7个错判样本中，有6个都为黑发黑人公主，白雪公主为黑发白人公主，因此可以看出，发色是分类学习的重要特征之一。 双向被错判的公主为，乐佩和爱洛、宝嘉康蒂和爱丽儿、艾莎和爱丽儿、白雪公主和茉莉公主、花木兰和蒂安娜。其中，乐佩和爱洛同为白人金色长发公主，花木兰和蒂安娜同为深肤色黑色长发公主，具有较大相似性，白雪公主和茉莉公主同为黑发，艾莎和爱丽儿同为白人公主，也具有相似性。宝嘉康蒂和爱丽儿，为什么会被认错QAQ？ 除了同肤色同发色的公主外，可以看到背景也是分类学习的重要特征，比如莫安娜的训练集图片背景都为海边，被错判的图片背景为白色，因此被错误分类为宝嘉康蒂。 Fine-Tune解放固有，来点属于自己的东西 既然是站在巨人的肩膀上，那索性做一些大胆的尝试。比如解放最后的卷积层，让模型重新训练一次，看看能不能有更好的效果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 选择之前效果最好的模型from keras.applications.inception_v3 import InceptionV3# 导入数据(train_finetune, validation_finetune) = load_data(im_size=224, batch_size = 64)# 构建模型# 导入已训练参数base_model = InceptionV3(weights='imagenet',include_top=False)x = base_model.outputx = GlobalAveragePooling2D()(x)# 在最后加入14分类的全连接层x = Dense(128,activation='relu')(x)predictions = Dense(14,activation='softmax')(x)finetune_model = Model(inputs=base_model.input, outputs=predictions)# 解冻最后三层for layer in base_model.layers[:len(base_model.layers)-3]: layer.trainable = Falsefor layer in base_model.layers[len(base_model.layers)-3:]: layer.trainable = True finetune_model.summary()## 这里输出依然略过# 训练finetune_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'] )finetune_history = finetune_model.fit_generator(train_finetune ,validation_data=validation_finetune ,epochs=1# ,epochs=100 ) # 保存结果# 创建结果保存文件夹if os.path.exists(output_path + '/model') == False : os.mkdir(output_path + '/model')if os.path.exists(output_path + '/acc') == False : os.mkdir(output_path + '/acc')# 保存正确率结果with open(output_path + '/acc/finetune_history.txt', 'wb') as f: pickle.dump(finetune_history.history, f)# 保存模型结果finetune_model.save(output_path + "/model/finetune.h5") 对比的结果不重要（其实也是找不到了）。但迁移学习给了我们很多的可能性，比如利用更好的资源做训练，而把训练好的模型参数放入轻量化的应用中。 最终验证集的正确率有60%+，着实出人意料。这或许就是神经网络的有趣之处。 记得老师的总结：“这组同学先用图片训练了一下组里唯一的男生，我估计这位男同学以后去迪士尼玩能当导游。”可惜我过了一个暑假就几乎全忘了呢，而迁移学习竟然能够把学习成果一直保留！（废话） 偶尔也想做个机器人呢。 （完，请享受深度学习之旅）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据|分布式计算课程笔记（持续更新）]]></title>
    <url>%2F2021%2F01%2F04%2F20-21%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%B1%87%E6%80%BB%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[还是熟悉的feng.li老师，还是熟悉的瓜皮禾禾，哈哈哈哈。 没想到研究生依然能听李丰老师的课。欢迎参观李丰老师主页，课程主页 李丰老师合著的参考书依然在编 10.07记：被李丰老师表扬了！！甚至还被打赏了（笑 继续努力啊小禾禾 分布式0917-遍历检索的多进程初试水 分布式0924-分布式服务器基础Linux中主机的远程交互(ssh) 分布式1007-Map-Reduce的文字流 分布式1015-1021-分布式回归分析 分布式1102-Hi,Hive!Hi,Spark! 分布式1112-Spark简单功能补充介绍 分布式1119-Spark做些实战-以及为之后的实战铺路 分布式1126-Spark文本分析 分布式1210-Spark与Scala 做个综述-Why Distributed Statistical Computing？现成的统计软件提供了常用的计算方式。 我们学牛顿迭代，学QR分解，可以用在广义线性模型里。 这些方法，可以适用于各种模型。并非是重复应用现成统计工具。统计计算旨在让你理解工具，并且实现新的目标。一个线性回归，也有很多的东西。 上面是传统的统计计算的目标。 下来类推到分布式计算。 spark里也可以做线性回归与逻辑回归。Spark.ml（里有机器学习，有回归） 为何要花时间学hadoop版本的线性回归，目的是能够把知识平行迁移到分布式平台上。 不仅仅是告诉我们ml下有什么模型，这个是初级阶段。 如果某一天，spark里没有机器学习模块了，而我们仍然有能力写自己的模型。 在很多时候，能够写出自己的模型是有优势的。 如，现有一份稠密的数据，我想做一个分布式的模型。我想做一个ar模型，我想做screening，我想做分位数回归，可是spark上没有。但是作为统计学背景的，我们知道很多统计方法，结合现在这个分布式计算的时代，适当地借助现有的工具，在spark上实现这样的操作，那么这门课就步入中级了。 如果自己开发了个spark，那就是高级了。（氦核：啊这） r软件里写这样的相对容易。spark里则是有难度的。如果你不善于整理归纳，统计学很多方法、算法可能看起来非常混乱。真正意义上的统计学只有八十年，统计计算开始于上世纪90年代。 1.数据状态（静态线下数据—动态实时大样本全量数据） 2.计算模式（单机存储单机计算—分布式存储分布式计算） 3.数据存储（统计模型与数据一体化—统计模型部署到数据）对y和x没要求，分布式中很挑剔，rdd还是dataframe，是稀疏矩阵还是稠密矩阵是sparkml（dataframe形式）还是mllab（rdd形式） 4.计算逻辑（单个模型对应单个算法实现—所有模型一体化计算框架） 5.需求实现（模型评估与应用—实时模型评估与预测需求） 我们对数据要有一定的认识。比如数据必须保存成特征（feature）+标签（label）的形式。 先写一个目标函数，然后优化。常见的优化算法，列出来12345。计算机背景的人，首先定义一个损失函数Loss，第二选一个合适的数据，第三做优化（牛顿迭代，梯度下降）。一二三就是一个流水线，通过管道（pipe）来进行。 我们搞统计的，就应该适应这种管道形式的建模思路，分布式的建模策略。 很多时候，我们的模型假设性太强，但现在的分布式平台中，根本没法满足传统假设。原来的统计方法还能否适用？传统的统计学家不关心。这好吗，这不好。要迁移到分布式的计算上来，要有很多新的观念上的转变，要有持续的转变思维。 比如，时间序列数据在分布式平台上就很缺乏。如果我们做开发，那么我们的平台/接口一定要有计算性+可延展性：即需要易用性、通用性，能使用户拥有比较统一的输入和输出，全流程都能解决，通过统一的分布式接口，使你的平台将来能更好地被别人所接受。 统计学，每个学科都要学。计算机，经济学，管理学，等等都要学统计。 ——李丰老师 我们要有新的平台，在最后做一个组合，能够完成大多数场景的统计计算，Hadoop就是一个足够好的分布式计算平台，但是hadoop不是足够好的统计计算平台。有一天你也能开发出足够好的统计计算平台。 海量数据下，现有应用场景，再想在应用场景下的工具。产品经理与算法工程师都是如此，用什么方法不重要，重要的是能够支持负载，能把这些都算出来，保证不宕机。 当前的情况不一样了，举一个例子：后台监管中，可能刻意回避一些东西。曾经有个跨境交易不能超过20w美元，风险点。于是各大银行会把跨境的交易全部拆成199美元。这些东西必须要学习模型来识别。核酸检测也是如此，识别feature。再比如，人类的基因组计划，ATCG的特征，每个人都要产生很大的核酸数据。 三步走： 应用场景：稳定性检测预警+动态监测与预报+样本实时监测 1.实时数据 2.统计计算 3.需求：分布式模型选择，分布式模型选择准则，约束下模型选择与决策 实时数据源：历史数据集+实时采集数据集 现在不像以前那么简单了。不过，蓝图应如此。 老师的研究DLSA分布式的最小二乘近似 如何在现有平台上开发出自己的东西呢？ 最小二乘近似，这是面对统计计算的接口。 能够使各种各样的统计模型都能应用在分布式平台上。 数据不动，最后算完合并。性质不好。 NIPS讨论了以更好方式重新对参数进行聚合，保证聚合的效果。2014 用在具体模型中，比如主成分。2017（范剑青老师的文章） 每次开发成本较高。每次一个新模型都要适配。 若采用一次性的估计，效率很有问题。而采用多次的时候迭代中通讯成本较高。 如果想把大数据合成小块，其实有个默认假设：数据随机分布。 一个桶倒满之后，再倒第二个桶，是增量的形式。如果完全利用one-shot（之前有介绍，是一种稀疏数据储存方式）会损失很多信息。 老师就希望提供一个解决方案。希望提供一个有效地估计，在计算节点上也有效，且有普适性，能够迁移到其他算法中。 此时，就轮到DLSA分布式的最小二乘近似出场了。（严格证明略） 给定某一个参数theta，似然函数就是当前数据所有特征最大的体现。 如果n个样本，这个似然函数可能是对n个密度的求和。 如果让似然函数除以n，则变成了一个损失函数。此时与机器学习中的方案对应起来了。 对似然函数没有任何假设。线性、非线性、时间序列都行。 对似然函数二阶导展开，展开后变换，之后新的似然函数会变成原始的似然减去新的目标，再加上一个常数c。就能近似写成某个形式。最后等价于一个加权最小二乘的表达式。极大化似然估计，就等价于如何对公式做最小二次近似。 这是一个标准的思路：统计理论，实现，组合成可应用形式。 spark上能不能做到逻辑回归，是否有你的方法好，是否贡献了全新的接口，是否解决了问题（推荐航班（此处指那时候刚结束的air-delay数据清洗工作），不延误，省钱）。 还有很多工作待发掘，未来可期。 还发现一个repo，没来的及看：分布式上的时间序列数据darima （完）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1210-Spark与Scala]]></title>
    <url>%2F2020%2F12%2F10%2F201210hadoop%2F</url>
    <content type="text"><![CDATA[spark的基础语法-Scala，做了一些思想性的介绍。 具体代码参考李丰老师课件L10.1-Introduction-to-Scala。 一、Scala介绍scala语言，面向大数据编程的语言。 这个语言只有十来年的历史。这个语言没有排在前二十的常见语言上，但确实进步最快的语言。因为它可以对海量数据进行高强度高密度的计算。 c语言是编译语言，快，但是写的麻烦。89十年代改用写起来快，跑起来快的python之流。现在就是写起来好写，且跑起来快。 新版本的spark，底层语言大部分是scala了。java不支持交互式输入，必须编译成机器码。而现在的编程习惯更多是交互式的。一部分习惯于python，另一部分习惯于scala。下面来比较一下这两种语言。 scala比python快十倍以上。spark比python快100倍（nb）。使用了java虚拟机机制，允许程序在运行中进行编译，比起python这种纯解释性的动态语言要快一个量级。 传统的c和java快，但是机器语言不好写。 对应的库上看，spark同时有scala和python的库，很多最新的特性都从scala上出现，再传递给python。 scala可以当做普通变成语言，也可以单机上使用。可以与hadoop结合。 考虑scala的学习曲线，python很好上手，有循环基础基本上1周就能写程序。scala比python略复杂，scala有一些特殊特性是原来不具有的（保证了速度的提升）。如果简单操作，完全可以用python写。如果计算速度会成为项目的瓶颈，那么可以选择scala。 语法最简单的是c语言，c++更难，java甚之，python处于三者之中，好写，慢。 比起python，scala更擅长处理复杂工作模式。 易用性上，python上有很多机器学习库可以使用，scala没有那么多。 很难把python语句导入scala。在多线程中，python经常阻塞，出问题。scala是内置的。 比起java，scala对内存要求不贪婪。spark就是个例子。scala可以快速释放内存，能较好地管理内存。 对于用户，需要知道一些常用的库，上手之后就比较舒服了。但有些环境很少工具，离不开原来的环境。比如自然语言处理用的都是python，scala需要很多操作。spark上python就很好用。 最后，定义一下： scala面向对象，高级语言。静态语言，语法有时候比python更可读。（主要是，python的numpy、pandas用法读法都不一样，不同的库用法也不同。） 二、一些实用操作（上参考网站更全面更系统一些）12345//注释/*可以注释多行*/ scala在定义变量时，有个特有类型：mutable 与 immutable变量，广播的变量时不可修改的，是immutable的。可以定义更加方便使用的变量。val是value，凡是用val定义的，都不可修改，凡是用var定义的，都是可修改的。 spark里直接输入scala，可以进入scala的交互对话框。 1.0双精度，迭代次数很多时求导会接近0。因此需要一些超长精度的。 1234567Val b = 0 +: ints //往右侧加:+ // 往左侧加if（A）&#123;&#125;else&#123;B&#125; scala比python快，但是不一定比scala易用。但将来一定更易用。 （具体方法省略了很多，更多内容请上参考网站。scala在平时学习中用的不多，且有一定门槛）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1126-Spark文本分析]]></title>
    <url>%2F2020%2F11%2F26%2F201126hadoop%2F</url>
    <content type="text"><![CDATA[spark的文本分析功能。 具体代码参考李丰老师课件L07.2-Text-Processing-with-Spark。 一、大数据文本分析的需求利用分布式系统，进行文本处理。 现在有很多文本型数据，到底会遇见什么不一样的地方，spark又提供了哪些工具？ 从基本概念和流程开始。 术语： 语料库。corpora。 语料库是一个包含大量感兴趣文本的集合，比如说，人民日报创刊以来所有的新闻社论。每个版2-3篇文章，一天做成一个行向量（可以是个很长的字典/列表），写入系统。 语料库最早是语言学家使用，处理语言问题。如研究50年代的语法，用于习惯。有从各种角度建立的语料库：经济学角度，政治角度，统计学角度，等等。 几十年前，如果研究人文类，会成为纯文科的事。而今，可以利用计算机，做词频统计等等，理科人也能掺和进来了。 来源，可能是工作就有，可能是自己采集，甚至可以我们自己构建语料库。 n乘k的语料矩阵，可以转成n乘m的语义数值阵，这一步很难，且有争议。解读和设定都比较主观，没有统一的标准。如何提取稳定的信息，就需要统计模型。 语料和数值型差距较大。 一个外国（说英语的）大学生，词汇量有两万多。构成文章，是这些词的排列组合，会有非常非常多的可能，计算机无法处理。也就无法将词作为基本单元来处理了。 于是需要化简，在处理中就是：断句分词。 逗号之间，段落内拆分成很小很多的单元。 我们学句读，就是在训练自己，把自己变成解释器。 ——李丰老师 拆分成单词，就会丢失句子的信息。 而当今很多语言模型不能就“序”进行建模。 我爱北京天安门，北京天安门爱我。这两句在家语言模型中可能是相同的。红黄蓝，蓝黄红，如果顺序有个权重，就完蛋了。（氦核：完蛋，全完蛋） 中文更特别，单词间没有空格，于是需要拆分词。在线翻译依然很垃圾，主要原因是文本实在是太难了。 新闻信息：语音录制工具（记录，转文本），做摘要，重新做新的填空，再做快报。 庭审记录员：解放书记员，书记员的记录会出错/有倾向性。背后有语义模型。 体育赛事：捕捉球员的用语，捕捉球员的兴奋状态。NBA以及是统计模型的竞赛了。 好的文本处理工具，可以让我们对语言不再束手无策。 ——李丰老师 停词中，把相类似相近的东西都替换成相关的内容。语言处理需要大量经验。斯坦福的自然语言处理，中国中科院，哈工大，都有自己的语料库。有趣而无聊的操作。 很多互联网公司提供了免费的api，根据其语料库分词。造就了当今输入法。 举个海底捞的例子，海底捞商标，如果我注册一个河底捞商标，侵权了吗。（氦核：老师这举的啥例子。。笑） 二、Spark的解决方案现在看看上面的操作，spark如何操作。 英文：对每一行都做了split拆分，得到一个词频矩阵。 1HashingTF(inputCol = 'words',outputCol = 'rawFeatures') 另一个工具叫IDF，每天要过滤很多网络信息，很多没用的信息。 三千封邮件中，有两份出现了“爆炸”“枪”，两个词同时出现。关联性的信息很强。就要适当放大权重。（这就是通过IDF来实现的）把那些我们在常用词里不关心，但低频词在多个文档都出现的，增加权重。 再比如，有很多政治新闻，会影响原油价格。我想回归，把政治新闻当做协变量。谷歌找了一个办法，把文本信息做成向量Word2VecModel（word2vec），里面还要信号强度。通过两层的神经网络，重新转化成一个数值型向量。 去除停词StopWordsRemover 一些新的想法：扩大相关性： 以一个词为中心，向左组词，向右组词，这个情况叫bi-gram，i向右扩1个，扩2个……信息的离散度越高。确定性的信息越少。不能无限扩大，常见的是2或3，能够体现相关性，重新构建词频矩阵。 我爱北京天安门 1我2爱3北京4天安门 5我爱6爱北京7我爱北京（567以爱为中心） LDA根据相似性聚合在一起，这个过程叫狄利克雷过程，也叫中国餐馆过程。哈哈。 如果这里面每一个客人，都是单词，我们就能通过统计学的聚类工具，自动把文章分为不同的主题。 主题模型建模发展目前正趋于成熟。 三、实战略过建立sc的步骤 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200textFile = sc.textFile("test.txt")textFile.first()## 展示结果： ## 'Title: The Romance of Wills and Testaments'type(textFile)## 展示结果： ## pyspark.rdd.RDD# 去空行print(textFile.count())text0 = textFile.filter(lambda x: len(x)&gt;1) # 留下符合要求的行print(text0.count())## 展示结果： ## 1129## 492from pyspark.ml.feature import HashingTF, IDF, Tokenizertokenizer = Tokenizer(inputCol="sentence", outputCol="words")tokenizer## 展示结果： ## Tokenizer_39744db6461e# 注意数据格式'''sentenceData = spark.createDataFrame([ (0.0, "Hi I heard about Spark"), (0.0, "I wish Java could use case classes"), (1.0, "Logistic regression models are neat")], ["label", "sentence"])'''# 此处要用 开始创建的sparksession创建DataFramesentenceData= ss.createDataFrame( list(zip(list(map(float,range(len(text0.collect())-1))), text0.collect())) # 元祖信息,['label','sentence'])sentenceData## DataFrame[label: double, sentence: string]# 载入分词引擎wordsData = tokenizer.transform(sentenceData)wordsData.show()## 展示结果： ## +-----+--------------------+--------------------+## |label| sentence| words|## +-----+--------------------+--------------------+## | 0.0|Title: The Romanc...|[title:, the, rom...|## | 1.0|Author: Edgar Vin...|[author:, edgar, ...|## | 2.0| PREFACE| [preface]|## | 3.0|By way of preface...|[by, way, of, pre...|## | 4.0|As in death, so i...|[as, in, death,, ...|## | 5.0|Different types a...|[different, types...|## | 6.0|It is desired to ...|[it, is, desired,...|## | 7.0|Again, there are ...|[again,, there, a...|## | 8.0|Especial acknowle...|[especial, acknow...|## | 9.0|The idea of this ...|[the, idea, of, t...|## | 10.0|Since these essay...|[since, these, es...|## | 11.0|Scattered about t...|[scattered, about...|## | 12.0|Other references ...|[other, reference...|## | 13.0| E. VINE HALL.| [e., vine, hall.]|## | 14.0| Wimbledon.| [wimbledon.]|## | 15.0| CHAPTER I| [chapter, i]|## | 16.0|THE ROMANCE OF WILLS|[the, romance, of...|## | 17.0|��The older I gro...|[��the, older, i,...|## | 18.0|The words of the ...|[the, words, of, ...|## | 19.0|Historically they...|[historically, th...|## +-----+--------------------+--------------------+## only showing top 20 rowshashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=20)featurizedData = hashingTF.transform(wordsData)featurizedData.show()## 展示结果： ## +-----+--------------------+--------------------+--------------------+## |label| sentence| words| rawFeatures|## +-----+--------------------+--------------------+--------------------+## | 0.0|Title: The Romanc...|[title:, the, rom...|(20,[6,10,11,12,1...|## | 1.0|Author: Edgar Vin...|[author:, edgar, ...|(20,[1,2,7,8],[1....|## | 2.0| PREFACE| [preface]| (20,[2],[1.0])|## | 3.0|By way of preface...|[by, way, of, pre...|(20,[1,2,3,4,6,7,...|## | 4.0|As in death, so i...|[as, in, death,, ...|(20,[0,1,2,3,4,5,...|## | 5.0|Different types a...|[different, types...|(20,[0,1,3,4,5,6,...|## | 6.0|It is desired to ...|[it, is, desired,...|(20,[0,1,2,3,4,5,...|## | 7.0|Again, there are ...|[again,, there, a...|(20,[0,1,2,3,4,5,...|## | 8.0|Especial acknowle...|[especial, acknow...|(20,[0,1,2,3,4,5,...|## | 9.0|The idea of this ...|[the, idea, of, t...|(20,[0,1,2,3,4,5,...|## | 10.0|Since these essay...|[since, these, es...|(20,[0,1,2,3,5,6,...|## | 11.0|Scattered about t...|[scattered, about...|(20,[1,2,3,4,5,6,...|## | 12.0|Other references ...|[other, reference...|(20,[0,1,3,4,5,6,...|## | 13.0| E. VINE HALL.| [e., vine, hall.]|(20,[0,2,18],[1.0...|## | 14.0| Wimbledon.| [wimbledon.]| (20,[8],[1.0])|## | 15.0| CHAPTER I| [chapter, i]|(20,[9,16],[1.0,1...|## | 16.0|THE ROMANCE OF WILLS|[the, romance, of...|(20,[6,11,15,17],...|## | 17.0|��The older I gro...|[��the, older, i,...|(20,[0,1,2,3,5,6,...|## | 18.0|The words of the ...|[the, words, of, ...|(20,[0,1,2,3,4,5,...|## | 19.0|Historically they...|[historically, th...|(20,[0,1,3,4,5,7,...|## +-----+--------------------+--------------------+--------------------+## only showing top 20 rows# alternatively, CountVectorizer can also be used to get term frequency vectorsidf = IDF(inputCol="rawFeatures", outputCol="features")idfModel = idf.fit(featurizedData)rescaledData = idfModel.transform(featurizedData)rescaledData.select("label", "features").show()## 展示结果： ## +-----+--------------------+## |label| features|## +-----+--------------------+## | 0.0|(20,[6,10,11,12,1...|## | 1.0|(20,[1,2,7,8],[0....|## | 2.0|(20,[12],[0.07923...|## | 3.0|(20,[2],[1.060380...|## | 4.0|(20,[12],[0.07923...|## | 5.0|(20,[1,2,3,4,6,7,...|## | 6.0|(20,[12],[0.07923...|## | 7.0|(20,[0,1,2,3,4,5,...|## | 8.0|(20,[12],[0.07923...|## | 9.0|(20,[0,1,3,4,5,6,...|## | 10.0|(20,[12],[0.07923...|## | 11.0|(20,[0,1,2,3,4,5,...|## | 12.0|(20,[12],[0.07923...|## | 13.0|(20,[0,1,2,3,4,5,...|## | 14.0|(20,[12],[0.07923...|## | 15.0|(20,[0,1,2,3,4,5,...|## | 16.0|(20,[12],[0.07923...|## | 17.0|(20,[0,1,2,3,4,5,...|## | 18.0|(20,[12],[0.07923...|## | 19.0|(20,[0,1,2,3,5,6,...|## +-----+--------------------+## only showing top 20 rows# word2Vecfrom pyspark.ml.feature import Word2Vec# 每个向量代表文档的词汇表中每个词语出现的次数。# Input data: Each row is a bag of words from a sentence or document.documentDF = ss.createDataFrame([ ("Hi I heard about Spark".split(" "), ), ("I wish Java could use case classes".split(" "), ), ("Logistic regression models are neat".split(" "), )], ["text"])'''textsplit = []## 太慢了，慎重运行for i in range(len(text0.collect())-1): textsplit.append((text0.collect()[i].split(' '),))textsplit'''# 对上一行操作的替代textsplit = text0.map(lambda x:x.split(' ') )textsplit## 展示结果： ## [(['Title:', 'The', 'Romance', 'of', 'Wills', 'and', 'Testaments'],),## (['Author:', 'Edgar', 'Vine', 'Hall'],),## (['PREFACE'],),## (['By',## 'way',## 'of',## 'preface',## 'it',## 'is',## 'necessary',## 'to',## 'explain',## 'the',## 'sources',## 'from',## 'which'...# word2vecfrom pyspark.ml.feature import Word2VecdocDF = ss.createDataFrame(list( zip(textsplit.collect()))).toDF("text")word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text", outputCol="result")model = word2Vec.fit(docDF)result = model.transform(docDF)for row in result.collect(): text, vector = row print("Text: [%s] =&gt; \nVector: %s\n" % (", ".join(text), str(vector)))## 展示结果： ## Text: [Title:, The, Romance, of, Wills, and, Testaments] =&gt; ## Vector: [0.07510965237660067,0.10043827923280851,0.19377085047640968]## ## Text: [Author:, Edgar, Vine, Hall] =&gt; ## Vector: [-0.07176287146285176,-0.10652108257636428,-0.012149352580308914]## ## Text: [PREFACE] =&gt; ## Vector: [0.006923258304595947,-0.002445697784423828,0.12241993099451065]## ## Text: [By, way, of, preface, it, is, necessary, to, explain, the, sources, from, which, the, material, for, the, following, pages, is, taken., The, chief, feature, of, these, essays, consists,, I, think,, in, the, large, amount, of, original, matter, rescued, from, the, multitudinous, MS., volumes, of, wills,, &amp;c.,, which, are, preserved, at, Somerset, House, and, elsewhere.] =&gt; ## Vector: [0.030354710200939463,0.10534696077445038,0.05815259900582195] （氦核：在本地看，我贴的代码其实很整齐的QAQ （未完待续）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1119-Spark做些实战-以及为之后的实战铺路]]></title>
    <url>%2F2020%2F11%2F19%2F201119hadoop%2F</url>
    <content type="text"><![CDATA[讲解作业，炒鸡复杂（其实也罢了）的air-delay数据清洗。 附带可以认识spark的强大之处。 一、作业集锦上一次作业里提出，我们从kaggle上download了一份巨大的数据，一共有五百万行，但只有19列。老师希望大家能处理好这个数据，清洗到能够建模的地步。 我记录了一些汇报亮点，但是大部分都消散在那节课中了。 1.数据展示12air.groupby(&apos;Month&apos;).count().collect() ## collect可以看到所有列# count默认不排序 累计求和百分比，计算占比较大的类，作为重要的变量结合sql语句spark命令结合sql 2.哑变量处理陈曦同学：对老师的代码的理解：students/2020211004chenxi/1112work 3.引入sql周童给出了引入sql的写法： 1234sql_accumulated = f&quot;&quot;&quot;&#123;参数&#125; select * from ( select &#123;col_name&#125;&quot;&quot;&quot; 4.分类变量的问题注意，3分类只要两个变量，否则有共线性。有k个变量都有可能哑变量，总体应该drop掉2的k次方-k个。全都是0-1的话，就如此。计算机里叫onehot，统计就叫哑变量。 特别地，onehot存储的形式就会改变：[40,38]一共40个数，第38个位为1 5.小心使用toPandas有一种储存方式是选择将sdf转化成pandas，toPandas对于count都是单机的操作。如果数据量不大，可以这样，因为这样会把数据存上master。 6.某种转化因子变量的方法（我的期末作业里是另一种方式）用一个if else，把所有factor变成0-1，不过这样生成的矩阵就不是稀疏矩阵（spark里可以） 某同学构建了：是否延误-各种定性变量的不同取值情况列联表。 列联表，这看着像统计人干的。 ——李丰老师 7.自己写个新函数get_sdummiespandas里有个getdummy函数，于是老师写了一个sdummies，即get_sdummies。 输入spark的df，只能具体的哪一个dummycol做修改，保持累计比例，自动删除那一列，最后有一个dummy_info=[] 如何在spark上自己生成？ 1，清理 2，多少行，对所有dummy列循环 如果info空，则创建一个新的，放入所有变量 3，spark里的数据框根据对应的dummycol做一个计数和排序。 对于所有count从上往下求和，分母是所有的行 1Window.partitionby.orderby.rowsbetween(-sys.maxsize,0) 就能获取前%多少的dummy变量 cumperc是累计求和除以总行数。累计百分比只保留（filter）小于我的top值 于是就能找到topdummy，且不用算到结束，算到出结果就停止 二、成果展示下面是李丰老师与cx同学代码的解析，太强了，点个赞！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310#! /usr/bin/env python3&apos;&apos;&apos;500000*19 去掉缺失值转化成delay 判断是否延误 √dlsa-project 参考常用变量继承，增加dummy列（航空公司）通过500000*180&apos;&apos;&apos;###################################################开启import findsparkfindspark.init(&apos;/usr/lib/spark-current&apos;)from pyspark.sql import SparkSession##session封装了confspark = SparkSession.builder.appName(&quot;chenxi session&quot;).getOrCreate()###################################################读入数据#处理schemafrom pyspark.sql.types import *schema_sdf = StructType([ StructField(&apos;Year&apos;, IntegerType(), True), StructField(&apos;Month&apos;, IntegerType(), True), StructField(&apos;DayofMonth&apos;, IntegerType(), True), StructField(&apos;DayOfWeek&apos;, IntegerType(), True), StructField(&apos;DepTime&apos;, DoubleType(), True), StructField(&apos;CRSDepTime&apos;, DoubleType(), True), StructField(&apos;ArrTime&apos;, DoubleType(), True), StructField(&apos;CRSArrTime&apos;, DoubleType(), True), StructField(&apos;UniqueCarrier&apos;, StringType(), True), StructField(&apos;FlightNum&apos;, StringType(), True), StructField(&apos;TailNum&apos;, StringType(), True), StructField(&apos;ActualElapsedTime&apos;, DoubleType(), True), StructField(&apos;CRSElapsedTime&apos;, DoubleType(), True), StructField(&apos;AirTime&apos;, DoubleType(), True), StructField(&apos;ArrDelay&apos;, DoubleType(), True), StructField(&apos;DepDelay&apos;, DoubleType(), True), StructField(&apos;Origin&apos;, StringType(), True), StructField(&apos;Dest&apos;, StringType(), True), StructField(&apos;Distance&apos;, DoubleType(), True), StructField(&apos;TaxiIn&apos;, DoubleType(), True), StructField(&apos;TaxiOut&apos;, DoubleType(), True), StructField(&apos;Cancelled&apos;, IntegerType(), True), StructField(&apos;CancellationCode&apos;, StringType(), True), StructField(&apos;Diverted&apos;, IntegerType(), True), StructField(&apos;CarrierDelay&apos;, DoubleType(), True), StructField(&apos;WeatherDelay&apos;, DoubleType(), True), StructField(&apos;NASDelay&apos;, DoubleType(), True), StructField(&apos;SecurityDelay&apos;, DoubleType(), True), StructField(&apos;LateAircraftDelay&apos;, DoubleType(), True) ])air00 = spark.read.options(header=&apos;true&apos;).schema(schema_sdf).csv(&quot;/data/airdelay_small.csv&quot;) #这是spark dataframe，不是pd的那个use_columns=[ &apos;ArrDelay&apos;, #double &apos;Year&apos;, #int &apos;Month&apos;, #int &apos;DayofMonth&apos;, #int &apos;DayOfWeek&apos;, #int &apos;DepTime&apos;, #double &apos;CRSDepTime&apos;, #double &apos;CRSArrTime&apos;, #double &apos;UniqueCarrier&apos;, #str &apos;ActualElapsedTime&apos;, #double&apos;, &apos;Origin&apos;,#str &apos;Dest&apos;, #str &apos;Distance&apos; #double]air=air00.select(use_columns).na.drop()#####################################################处理因变量########################def delay(x): if x&gt;0: return 1 else : return 0#参考 https://blog.csdn.net/wulishinian/article/details/105817409 spark中生成新列的各种方法，不能直接定义了import pyspark.sql.functions as Fyfunc = F.udf(delay, StringType())#类似apply的使用，对该列每个数做个操作air = air.withColumn(&quot;delay_or_not&quot;, yfunc(&quot;ArrDelay&quot;))#####################################################处理自变量##########################注意，pyspark好像识别不了空行和换行（在一行一行跑的时候）####################先把一些列转成others#使用老师给的代码统计哪些类别归入othersimport pickleimport pandas as pdimport numpy as npimport osfrom collections import Counterdef dummy_factors_counts(pdf, dummy_columns): &apos;&apos;&apos;Function to count unique dummy factors for given dummy columns pdf: pandas data frame dummy_columns: list. Numeric or strings are both accepted. return: dict same as dummy columns &apos;&apos;&apos; # Check if current argument is numeric or string pdf_columns = pdf.columns # Fetch data frame header dummy_columns_isint = all(isinstance(item, int) for item in dummy_columns) #isinstance() 判断item是否是int #all()用于判断给定的可迭代参数 iterable 中的所有元素是否都为 TRUE，如果是返回 True，否则返回 False if dummy_columns_isint: dummy_columns_names = [pdf_columns[i] for i in dummy_columns] else: dummy_columns_names = dummy_columns factor_counts = &#123;&#125; for i in dummy_columns_names: factor_counts[i] = (pdf[i]).value_counts().to_dict() #统计每一列里的不同值的个数 return factor_counts###合并两个字典，并计算同一key的和（两个字典都有子字典）def cumsum_dicts(dict1, dict2): &apos;&apos;&apos;Merge two dictionaries and accumulate the sum for the same key where each dictionary containing sub-dictionaries with elements and counts. &apos;&apos;&apos; # If only one dict is supplied, do nothing. if len(dict1) == 0: dict_new = dict2 elif len(dict2) == 0: dict_new = dict1 else: dict_new = &#123;&#125; for i in dict1.keys(): dict_new[i] = dict(Counter(dict1[i]) + Counter(dict2[i])) return dict_new#counter是python计数器类，返回元素取值的字典,且按频数降序def select_dummy_factors(dummy_dict, keep_top, replace_with, pickle_file): &apos;&apos;&apos;Merge dummy key with frequency in the given file dummy_dict: dummy information in a dictionary format keep_top: list &apos;&apos;&apos; dummy_columns_name = list(dummy_dict)#本身词典里就是取值 # nobs = sum(dummy_dict[dummy_columns_name[1]].values())#没用到 factor_set = &#123;&#125; # The full dummy sets——————注意，是空字典，不是集合 factor_selected = &#123;&#125; # Used dummy sets factor_dropped = &#123;&#125; # Dropped dummy sets factor_selected_names = &#123;&#125; # Final revised factors for i in range(len(dummy_columns_name)): column_i = dummy_columns_name[i] #给出列来 factor_set[column_i] = list((dummy_dict[column_i]).keys())#第i列的可能取值表 factor_counts = list((dummy_dict[column_i]).values())#第i列的值的个数 factor_cumsum = np.cumsum(factor_counts)#累加 factor_cumpercent = factor_cumsum / factor_cumsum[-1]#累积比率 factor_selected_index = np.where(factor_cumpercent &lt;= keep_top[i])#top这个是给定的 factor_dropped_index = np.where(factor_cumpercent &gt; keep_top[i]) factor_selected[column_i] = list( np.array(factor_set[column_i])[factor_selected_index])#一列有一堆可用取值 factor_dropped[column_i] = list( np.array(factor_set[column_i])[factor_dropped_index]) # Replace dropped dummies with indicators like `others` if len(factor_dropped_index[0]) == 0: factor_new = [] else: factor_new = [replace_with] factor_new.extend(factor_selected[column_i])#extend列表末尾一次性追加另一个序列中的多个值 factor_selected_names[column_i] = [column_i + &apos;_&apos; + str(x) for x in factor_new] dummy_info = &#123; &apos;factor_set&apos;: factor_set, &apos;factor_selected&apos;: factor_selected, &apos;factor_dropped&apos;: factor_dropped, &apos;factor_selected_names&apos;: factor_selected_names&#125; pickle.dump(dummy_info, open(os.path.expanduser(pickle_file), &apos;wb&apos;)) print(&quot;dummy_info saved in:\t&quot; + pickle_file) return dummy_info #返回了一个包含处理信息的字典&apos;&apos;&apos;pickle提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上pickle.dump(obj, file[, protocol]) 序列化对象，并将结果数据流写入到文件对象中。参数protocol是序列化模式，默认值为0，表示以文本的形式序列化。protocol的值还可以是1或2，表示以二进制的形式序列化。 pickle.load(file) 反序列化对象。将文件中的数据解析为一个Python对象。其中要注意的是，在load(file)的时候，要让python能够找到类的定义，否则会报错：&apos;&apos;&apos;def select_dummy_factors_from_file(file, header, dummy_columns, keep_top, replace_with, pickle_file): &apos;&apos;&apos;Memory constrained algorithm to select dummy factors from a large file 对大文件使用内存约束算法选择dummy，一个真正的分布式的算法 要输入文件路径、表头，要变成哑变量的列，保留的比例， &apos;&apos;&apos; dummy_dict = &#123;&#125; buffer_num = 0 with open(file) as f: while True: buffer = f.readlines( 1024000) # Returns *at most* 1024000 bytes, maybe less if len(buffer) == 0: break else: buffer_list = [x.strip().split(&quot;,&quot;) for x in buffer] buffer_num += 1 if ((buffer_num == 1) and (header is True)): buffer_header = buffer_list[0] buffer_starts = 1 else: buffer_starts = 0 buffer_pdf = pd.DataFrame(buffer_list[buffer_starts:]) if header is True: buffer_pdf.columns = buffer_header dummy_dict_new = dummy_factors_counts(buffer_pdf, dummy_columns) dummy_dict = cumsum_dicts(dummy_dict, dummy_dict_new) dummy_info = select_dummy_factors(dummy_dict, keep_top, replace_with, pickle_file) return (dummy_info)&apos;&apos;&apos;#####看一下情况确定要不要＋others列air.groupby(&apos;Month&apos;).count().show() #没有比例差异air.groupby(&apos;DayofMonth&apos;).count().show(31) #没有比例差异air.groupby(&apos;DayofWeek&apos;).count().show()#没有比例差异air.groupby(&apos;Year&apos;).count().collect()#没有比例差异air.groupby(&apos;UniqueCarrier&apos;).count().collect()#air.groupby(&apos;UniqueCarrier&apos;).count().orderBy(&apos;count&apos;).show(50)m=air.groupby(&apos;Origin&apos;).count()#air.groupby(&apos;UniqueCarrier&apos;).count().rdd.foreach(print) 为什么打印不出来？m.orderBy(-m(&apos;count&apos;)).collect()m.sort(desc(&apos;count&apos;)).collect().collect()&apos;&apos;&apos;if __name__ == &quot;__main__&quot;: # User settings file = os.path.expanduser(&quot;/home/devel/data/airdelay_small.csv&quot;) header = True dummy_columns = [&apos;UniqueCarrier&apos;, &apos;Origin&apos;, &apos;Dest&apos;] keep_top = [0.8, 0.8, 0.8] replace_with = &apos;00_OTHERS&apos; pickle_file = os.path.expanduser(&quot;/home/devel/students/2020211004chenxi/1112work/airdelay_dummy_info_latest.pkl&quot;) dummy_info = select_dummy_factors_from_file(file, header, dummy_columns,keep_top, replace_with,pickle_file)#得到应该记为others的列名drop_uc=dummy_info[&apos;factor_dropped&apos;][&apos;UniqueCarrier&apos;]drop_o=dummy_info[&apos;factor_dropped&apos;][&apos;Origin&apos;]drop_d=dummy_info[&apos;factor_dropped&apos;][&apos;Dest&apos;]sle_uc=dummy_info[&apos;factor_selected&apos;][&apos;UniqueCarrier&apos;]sle_o=dummy_info[&apos;factor_selected&apos;][&apos;Origin&apos;]sle_d=dummy_info[&apos;factor_selected&apos;][&apos;Dest&apos;]########################使用字典把很小的类别更改成others#生成字典drop_all=drop_uc+drop_o+drop_dsle_all=sle_uc+sle_o+sle_dv=[&quot;others&quot;]*len(drop_all)dic=dict(zip(sle_all+drop_all,sle_all+v))air11=air.na.replace(dic,1,&apos;UniqueCarrier&apos;)air11=air11.na.replace(dic,1,&apos;Origin&apos;)air11=air11.na.replace(dic,1,&apos;Dest&apos;)print(&apos;替换others后的数据\n&apos;)air11.show(10)#更改后的结果#报错好像是内存太小？###########################################################独热编码################&apos;&apos;&apos;samplefrom pyspark.ml.feature import OneHotEncoder,StringIndexerindexer = StringIndexer(inputCol=&apos;Month&apos;, outputCol=&apos;MonthIndex&apos;)model = indexer.fit(air)indexed = model.transform(air)onehotencoder = OneHotEncoder(inputCol=&apos;MonthIndex&apos;, outputCol=&apos;MonthVec&apos;)oncoded = onehotencoder.transform(indexed)oncoded.show(5)&apos;&apos;&apos;#https://github.com/spark-in-action/first-edition/blob/master/ch08/python/ch08-listings.py#先生成indexdef indexStringColumns(df, cols): from pyspark.ml.feature import StringIndexer #variable newdf will be updated several times newdf = df for c in cols: si = StringIndexer(inputCol=c, outputCol=c+&quot;-num&quot;) sm = si.fit(newdf) newdf = sm.transform(newdf).drop(c) newdf = newdf.withColumnRenamed(c+&quot;-num&quot;, c) return newdf#根据index进行独热编码def oneHotEncodeColumns(df, cols): from pyspark.ml.feature import OneHotEncoder newdf = df for c in cols: onehotenc = OneHotEncoder(inputCol=c, outputCol=c+&quot;-onehot&quot;, dropLast=False) newdf = onehotenc.transform(newdf).drop(c) newdf = newdf.withColumnRenamed(c+&quot;-onehot&quot;, c) return newdfcols=[&apos;Year&apos;,&apos;Month&apos;,&apos;DayofMonth&apos;,&apos;DayOfWeek&apos;,&apos;UniqueCarrier&apos;,&apos;Origin&apos;,&apos;Dest&apos;]dff=indexStringColumns(air11,[&apos;Year&apos;,&apos;Month&apos;,&apos;DayofMonth&apos;,&apos;DayOfWeek&apos;,&apos;UniqueCarrier&apos;,&apos;Origin&apos;,&apos;Dest&apos;])dfhot = oneHotEncodeColumns(dff, cols)print(&apos;编码后形式\n&apos;)dfhot.take(2)&apos;&apos;&apos;py4j.protocol.Py4JJavaError: An error occurred while calling o1290.transform.: java.lang.IllegalArgumentException: Field &quot;DayofWeek&quot; does not existAvailable fields: ArrDelay, DepTime, CRSDepTime, CRSArrTime, ActualElapsedTime, Distance, DayOfWeek, UniqueCarrier, Origin, Dest, Year, Month, DayofMonth&apos;&apos;&apos;#转换成能使用的形式from pyspark.ml.feature import VectorAssembler#把所有字符型向量转换成数值型的后，可以合并,能直接在MLlib里用va = VectorAssembler(outputCol=&quot;features&quot;, inputCols=dfhot.columns[0:])#取除最后一列外的所有值lpoints = va.transform(dfhot).select(&quot;features&quot;)print(&apos;最终结果\n&apos;)lpoints.take(2)&apos;&apos;&apos;问题1：独热编码没有办法设定基准组，默认count中的最后一个是基准组，并不是数据最少的那个————如果替换了others，问题应该不是很大问题2：要求是矩阵形式怎么办？——可以尝试vector分列，有合适的写法，但是没能实现###尝试把一个vector分列，但是报错没有numpy？#vectors = lpoints.select(&quot;features&quot;).rdd.map(lambda row: row.features)#PipelinedRDD#疑问：退出后就没有以前的操作了，怎么办？ 有没有类似screen或者保存工作空间的操作？&apos;&apos;&apos; 三、稍微讲了点新课1.需求机器学习，可分成一些步骤： Featurization-特征选取。 40个观测，并非特征越多，模型越好（要选，steplm之类） 下来变换清理数据。 0-1，降维……等等问题都涌现出来。 这些都叫“特征工程”，这个变量矩阵本身就是分布式的（spark）。 最后使用模型建模，得到想要的信息等等。 2.Pipelines（管道）spark通过管道把不同的流程结合起来。 pipline来自于python机器学习模块中scikit-learn。 persistence（工具性）模型存储，加载。 utilities：线性代数，统计学等等。 旧版本的mllib中，基于rdd形式。现在逐渐转化成df（好处是能和sql结合）。这是由于sql很难被直接用在rdd形式上。 不过，根据上面所说的，其实可以将df转化（Transformer、Estimator） Pipeline 提供了一个能够完整工作流的链（Parameter） 比如有个文本数据： pipeline： ————1.Tokenizer————2.hashingTF———3.logistic regression pipeline的流： 0.5Rawtext————1.5words————2.5feature vectors （数字表示时间顺序） 课件以逻辑回归为例，regparam是惩罚。fit结果，不用规定x和y，因为默认的需要标记y为label，x标记为features。（机器学习的默认规则，那些函数的默认参数都是features，头大） （未完待续，下节课讲文本处理）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1112-Spark简单功能补充介绍]]></title>
    <url>%2F2020%2F11%2F12%2F201112hadoop%2F</url>
    <content type="text"><![CDATA[对spark进行一些补充介绍。 两个函数可以选定 Cache() Persist() 主动将数据放到硬盘上-内存中 Data.persist() 旧数据放在内存里，新数据放硬盘，spark帮助中有persist 的水平（默认是全放进内存的cache，假设内存很大） lineLengths.unpersist() persist的参数水平 Memory_only Memory_AND_DISK Memory_AND_DISK_SER DISK_ONLY MEMORY_ONLY_2 内存不够则会报错 二、Spark的一些特别之处1. 广播分布式最重要的是“数据共享”使得不同节点之间能够用一个数据。 比如正态分布的概率密度函数，π就如此，共享，但不修改。 broadcastVar = sc.broadcast([1, 2, 3]) broadcastVar Sparkcontext.broadcast(v) 广播出去的变量不能修改，否则会乱。 Broadcast.value可以查看广播出去的变量。 spark中accumulator可以用于累积，在MapReduce中： accum = sc.accumulator(0) sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x)) # foreach有点像R的 accum.value 2. 懒人模式spark的懒人模式： 节约计算资源 x=3 , y = 4 , z = 5 提交任务 1.2x = ? 2.4y = ? 3.2x +4y = ? 或许前两步根本不用算，于是节约了资源。 spark使用DAG有向无环图，控制最后的结果本质上要求哪些计算。实现懒人模式。 分布式就是管理人和物的一种抽象。 —— 李丰老师 3. 线性代数1234567891011121314import numpy as npimport scipy.sparse as spsfrom pyspark.mllib.linalg import Vectors# Use a NumPy array as a dense vector.dv1 = np.array([1.0, 0.0, 3.0])# Use a Python list as a dense vector.dv2 = [1.0, 0.0, 3.0]# Create a SparseVector.sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])# Use a single-column SciPy csc_matrix as a sparse vector.sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape=(3, 1)) spark的线性代数模块很强大： pyspark.mllib.linalg spark专门提供的标签工具 做分类模型时就可以使用特有变量了 1234567from pyspark.mllib.linalg import SparseVectorfrom pyspark.mllib.regression import LabeledPoint# Create a labeled point with a positive label and a dense feature vector.pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])# Create a labeled point with a negative label and a sparse feature vector.neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0])) 允许导入各式各样的稀疏数据。有了local就有distributed。 如果要做个逻辑回归、线性回归，能否模拟一个线性回归的数据，将其存入矩阵。 小结spark集成了很多hive的优秀理念。 对于常见的数据框的操作，归类成不同类型的函数。 依赖于sparkSQL，有别于传统的RDD形式，因为在RDD上可以更底层地操作数据（矩阵向量……） sparkSQL与hive结合，可以把hive的sql查询直接应用在数据框上，也允许用户自己的函数。支持读取hdfs上的数据，是个通用的多接口的形式。 sparkRDD形式数据灵活，操作很琐碎。于是spark提供了自己的dataset集合。其实就是分布式数据的综合，通过java的jvm集成的（java虚拟机，用于快速计算的技术） dataset的api只支持scala和Java。 故如果想在spark上处理数据集，需要自己学习Scala语言（最后一节有讲，敬请期待）。 spark上的dataframe是分布式的，其实就是表，不同列之间可以是不同的数据类型。 可以对dataframe做清洗和操作。可以通过hive的表来构建，可以通过现成的表来构建。 各种语言都支持。 三、战斗案例目标，处理分布式的DataFrame，首先启动SC。 123456from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;Python Spark&quot;).getOrCreate()spark # test if Spark session is created or notsc = spark.sparkContext # make a spakr context for RDDsc spark有read函数 12sdf = spark.read.csv(&quot;/opt/apps/ecm/service/spark/2.4.4/package/spark-2.4.4-bin-hadoop2.7/examples/src/main/resources/people.txt&quot;)sdf.show() # Displays the content of the DataFrame to stdout 这个就是分布式上的表。 json格式可以直接读取（spark.read.json） schema属于读取表格时的表头信息。名字，类型，缺失等等。 经常需要手写表头，因为自动容易出错。 12345678910111213141516171819202122232425262728293031323334# We specify the correct schema by handschema_sdf = StructType([ StructField(&apos;Year&apos;, IntegerType(), True), StructField(&apos;Month&apos;, IntegerType(), True), StructField(&apos;DayofMonth&apos;, IntegerType(), True), StructField(&apos;DayOfWeek&apos;, IntegerType(), True), StructField(&apos;DepTime&apos;, DoubleType(), True), StructField(&apos;CRSDepTime&apos;, DoubleType(), True), StructField(&apos;ArrTime&apos;, DoubleType(), True), StructField(&apos;CRSArrTime&apos;, DoubleType(), True), StructField(&apos;UniqueCarrier&apos;, StringType(), True), StructField(&apos;FlightNum&apos;, StringType(), True), StructField(&apos;TailNum&apos;, StringType(), True), StructField(&apos;ActualElapsedTime&apos;, DoubleType(), True), StructField(&apos;CRSElapsedTime&apos;, DoubleType(), True), StructField(&apos;AirTime&apos;, DoubleType(), True), StructField(&apos;ArrDelay&apos;, DoubleType(), True), StructField(&apos;DepDelay&apos;, DoubleType(), True), StructField(&apos;Origin&apos;, StringType(), True), StructField(&apos;Dest&apos;, StringType(), True), StructField(&apos;Distance&apos;, DoubleType(), True), StructField(&apos;TaxiIn&apos;, DoubleType(), True), StructField(&apos;TaxiOut&apos;, DoubleType(), True), StructField(&apos;Cancelled&apos;, IntegerType(), True), StructField(&apos;CancellationCode&apos;, StringType(), True), StructField(&apos;Diverted&apos;, IntegerType(), True), StructField(&apos;CarrierDelay&apos;, DoubleType(), True), StructField(&apos;WeatherDelay&apos;, DoubleType(), True), StructField(&apos;NASDelay&apos;, DoubleType(), True), StructField(&apos;SecurityDelay&apos;, DoubleType(), True), StructField(&apos;LateAircraftDelay&apos;, DoubleType(), True) ])oridat = spark.read.options(header=&apos;true&apos;).schema(schema_sdf).csv(&quot;/data/airdelay_small.csv&quot;) # spark dataframe air.describe().show()就相当于简单的描述统计 air.describe([‘ArrDelay’]).show() 看具体的列 Data.collect()可以避开懒人模式直接计算 四、作业 air-delay数据清洗五百万 * 十九列 转化成新的df，一类是0、1，告诉大家有没有延误 arrivedelay设置成0-1变量。现有的列可以使用：里程，是不是US（0-1），是不是AA（0-1），诸如此类，相当于把原变量修改成哑变量了。 最后得到————&gt;五百万 * 一百八十列 不要超过这么多列。（在老师github上/dlsa/blob/master/projects/logistic…） 作业，整理好这个数据。 下节课对这个数据做逻辑回归。]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1102-Hi,Hive!Hi,Spark!]]></title>
    <url>%2F2020%2F11%2F02%2F201102hadoop%2F</url>
    <content type="text"><![CDATA[hadoop以及MapReduce暂告一段落！ 这一节我们做个过渡，讲一讲Hive以及Spark。 一、Hive介绍Hive是分布式的数据库。对于结构化数据存储、查询的高效工具。 查询特殊的行、列，数据库操作非常重要。 数据库就像计算器一样。 ——李丰老师 不管什么类型的数据库，都有sql，数据库查询语言。能够对很大的表格进行类似excel的操作。 数据库面向不同的对象： 计算机：设计高效数据库，快速存储海量数据。 使用者：快速得到想要的数据。 大数据时代，数据库进入瓶颈，因为其不可扩展。轻量级的数据应用就不太喜欢数据库，转而使用hdfs。但hdfs没有数据库那样便利，不能select…from… 就诞生了基于hdfs的数据库工具。 初衷：为了使企业能够快速部署使用结构化表格与操作。能将hdfs上的表格当做数据库的表格来使用（如csv）。 只要用sql语句就可以在hdfs上处理结构化数据。其实Hive是把sql解析成了MapReduce语句，最后传递回用户。 Hadoop（系统）hdfs（文件管理系统） hive-SQL（可操作的客户端）complie+optimize+execute（很快啊！） hive其实有小缓存，如果大量/经常查询同一条，就会被缓存下来，方便直接调用。 不会hadoop也可以用hive，只需要sql语句就行了。 hive可以做spark中dataframe的查询引擎。hive的功能就是能用sql的客户端。（氦核：这一点在之后的工作中显示了其强大与方便） hive将sql解析成XML语句（标记位置） hive完全是模拟了sql的写法，sql用的最广，python之流不适合这样的工作。 二、启动一个Hive任务Hive + 回车（进入交互界面） Exit （退出） hive允许执行很多命令同时推出 如： Hive -e “dfs -ls /;” e是执行。引号内是hive语句，分号是模仿sql的结束符号。 传递给hadoop就是hadoop fs -ls / 结果再传回hive 也可以写成文件： Hive -f /path/to/file/withqueries.hql hql（hive的文本文件，与sql区分） 输入hive可以进入终端 操作时，在终端中可以直接输入dfs -ls /; （别忘了分号） 注：sql语言的特点——对大小写不敏感，语句中可以小写可以大写。但是对表的字段依然敏感。 Show databases; 可以看分布式集群上有什么数据库 Show databases like ‘d*’; Create database if not exists mydb; if not exists（是一个条件，如果有了就不会再创建，如果没有的话就创建） 数据库下有表，mydb不指定文件夹，就在warehouse的文件夹下。 location参数可以指定创建的位置 Create database if not exists fff location ‘user/lifeng/hive’ 创建一个指定位置的数据库fff Drop database fff; 删库，跑路！（危） hive里如何创建表12345678910111213CREATE TABLE IF NOT EXISTS mydb.employees ( name STRING COMMENT &apos;Employee name&apos;, salary FLOAT COMMENT &apos;Employee salary&apos;, subordinates ARRAY&lt;STRING&gt; COMMENT &apos;Names of subordinates&apos;, deductions MAP&lt;STRING, FLOAT&gt; COMMENT &apos;Keys are deductions names, values are percentages&apos;, address STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt; COMMENT &apos;Home address&apos;)COMMENT &apos;Description of the table&apos;TABLEPROPERTIES (&apos;creator&apos;=&apos;me&apos;, &apos;created_at&apos;=&apos;2012-01-02 10:00:00&apos;); 列名，comment记录注释，最后一行只创建了表头、创建者、创建时间等 hive记录了表的信息 因为刚刚用了use wyh_db;现在Show tables;就能看有哪些表。 hive提供了专门工具，把外部的csv文件链接进数据库。 hive可以创建外部表： 1234567891011create external table if not exists stocks ( symbol string, ymd string, price_open float, price_high float, price_low float, price_close float, volume int, price_adj_close float )row format delimited fields terminated by &apos;,&apos; location &apos;/user/lifeng/data&apos;; 逗号分隔，原来是csv文件，要加一个逗号分隔（倒数第二行） linux里有个内置命令awk脚本工具，能够 Awk -f 5，4啥的，能够选择打印第5和第4列 主要是不想导入python中处理。 如可以使用pig工具，合并两个表或多个表（我们用的不多） pig有自己的书写习惯，计算机架构的工程师经常用 hbase是对谷歌bigtable的开源实现 能够按行更新（如python的append） 能做基于内存的文件缓存，加快速度 不支持sql的查询，但是hive目前有工具可以与之通行。 （HIVE的简介到此结束，后面有任务再见） 三、Sparkspark， Speed - Run workloads 100x faster. Ease of Use - Write applications quickly in Java, Scala, Python, R, and SQL. Generality - Combine SQL, streaming, and complex analytics. Runs Everywhere - Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. It can access diverse data sources. 伯克利的博士生，针对hadoop的问题重新写成了spark（老师也希望我们做这样的博士生，扶额） Databricks公司，官网上提供了简单交互学习的平台。 spark在计算过程中非常快，是hadoop速度的一百倍 可以使用python，r，java，还有spark自带的scala语言。Scala语言有java的特性，又像r一样好写。 可以将spark当做python的一个模块来使用。（氦核：事实证明，pyspark很强，还自带深度学习模块，不过没那么顺手。）用户只需要学一点点就能用起来。 sparkR可以启动r。r语言设计用于统计分析，但是spark需要计算机组件，但是r没有，python有。 spark与hadoop的区别hadoop是分布式的框架，spark对MapReduce看的更少，结果算的很快。 还有，hadoop不能交互；spark可以交互式操作一个对象。可以创建分布式对象， 在不同的节点上都存在，同时保证应用性和速度 spark也有通用性和广泛性，可以把sql集成进spark。 spark也可以处理流数据。 spark甚至可以机器学习和深度学习。 spark可以看做是分布式系统上更方便操作的hadoop客户端。 可以运行在hadoop上，可以当做独立的分布式系统。 spark可以接受hdfs等来源的文件，也可以自己建立dataframe。 spark可以用数据框组织数据。 spark有内置机器学习库，叫ml_lib；图形处理GraphX；流数据处理Spark Streaming（企业常用）。 spark也可以运行在其他分布式平台上，各种各样的平台上都有spark接口。易用性使其广泛普及。 spark提供了hive的集成。不过select不能写很复杂。但是通过spark可以先让spark执行select，再让hive执行。 快的原因： 1.很多worker节点，worker之间交互很快（通过交换机）2.worker上可以运行很多任务（executor）3.每个worker上都有一些存储最常用数据的内存 劣势： 需要很大的内存 — hadoop最不耗内存 spark的官方建议，需要原始数据2-5倍的内存才能保证平稳运行 有名的spark错误：OOM错误 Out of memory（哈哈哈） 加内存，扩容，烧钱啊。 尽可能让代码写快一些，用最少的资源得到最大的价值。 ——李丰老师 之后课程计划讲：数据类型、机器学习的库（老版本基于rdd，新版本基于数据框）、streaming 不讲：计算机视觉相关包 四、启动spark启动：.py .r 123456Spark-submit \ --class --master #什么节点(yarn) --deploy-mode --executor-memory 20G \ --conf Spark案例 使用spark-summit提交上去就能执行了。在spark运算前，要想想全部需要多少运算资源，给每个executor分多少内存。 Tiny fat 两种分配资源想法 Tiny 模式给每个核分一个executor，32g给8核，只能拿到4g，加上系统消耗就会更少。这种情形适用于cpu负荷大，如迭代类型，更多时候是迭代的分布式要跑起来。 Fat 模式，要是对IO很多的时候，我们不一定需要很多executor，八核可能只用两个executor，每个executor就有4核、16g使用。 python有个**x，即将列表展开按位置放入 x的长度不超过256，但是python3.8现在允许任意长度参数传入 X = [1,2,3,…,1000] Sumfunc(**x) 登录服务器，输入spark-shell 使用pyspark要输入神秘代码（就是python的版本，配置时一定要与spark版本对应起来，比如服务器只能用3.6版本的python） 或者直接在python中import pyspark，有时候需要先import findspark。 Spark的APIrdd是什么（resilient distributed dataset）RDD是Spark里最重要的一个概念。 任何一个spark都可以通过rdd对象连接起来，也允许用户部署任何计算。 通过rdd转化成驱动程序，放入计算节点上计算，如传统的数学计算都可以转化。 spark-rdd提供了数据上的一个抽象，提供了海量数据的拆分机制，也可以通过不同的方式创建。 如分布式hdfs上有个文件，可以直接转化（好比hive上有个表），它也能判断哪些数据应该放入内存、硬盘。来提高效率。 其次，rdd也监视了每个计算节点的数据完整性。复活的基本机理与hadoop相似。闲暇时间拷贝。 spark能进行机器学习的理由：spark允许变量进行共享，每个节点都有想同的变量。 1.常量，都用得着，不需修改。通过广播（broadcast variables）来传递给每个节点 2.传递到节点后，还可能要修改（如迭代算法）会消耗资源（accumulators） 如何创建spark context123456import findsparkfindspark.init(&quot;/usr/lib/spark-current&quot;)import pysparksc.stop()sc = pyspark.SparkContext(&quot;local&quot;, &quot;My First Spark App&quot;) 使用了sc.parallelize就会传上spark，是一个对象。如果数据在本地或hdfs，都可以上传。 给一个自己的作业超简单案例123456789101112#! /usr/bin/env python3.6import pysparkconf = pyspark.SparkConf().setAppName(&quot;Hehe First Spark RDD APP&quot;).setMaster(&quot;local&quot;) # “yarn”sc = pyspark.SparkContext(conf=conf)sc.stop()sc = pyspark.SparkContext.getOrCreate()licenseFile = sc.textFile(&quot;2020210995wangyuanhe/reg/stocks.txt&quot;)lineLengths = licenseFile.map(lambda s: len(s))totalLength = lineLengths.reduce(lambda a, b: a + b)print(totalLength) （未完待续，下次再细讲Spark，学吧，学无止境，太深了）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1015-1021-分布式回归分析]]></title>
    <url>%2F2020%2F10%2F21%2F201015hadoop%2F</url>
    <content type="text"><![CDATA[一、开始！今日信息量巨大大佬们展示肌肉。 回归部分还需要些数学根底。 代码后面也有一丢丢正文。 先给一个linux服务器的方便功能。 Screen -DR 加个名字，可以开启永远运行的窗口。 Ctrl A + C 创建新窗口 N 切换下一个窗口 D 回到主界面（并未关闭窗口）Exit 命令退出 同学们的学习能力很强，可以打开vim编辑器并且退出了。（笑 不不，是有与数据对话的能力了。 希望脱颖而出，代码能力肯定比不上，但是我们有专业优势，即对数据分析的能力。我们懂模型，懂预测。 至于为什么要用hadoop，因为数据多了，非常大。 给出一个场景场景：美国二手车，kaggle us-used-car。一共300w条记录，66个变量。 因变量：Price，最主要的任务就是探究price受谁的影响。 首先这么多变量中，存在很多数据缺失问题。去缺失。传到服务器上后，挑出一些缺失值少的变量。 今天作业：用这个数据，清理出一份可以回归的变量来。r里面有很多现成的东西。 二、分布式上的回归分析如何在分布式上进行回归分析？区别在哪？（按行读取） 原来的数据n乘p维，n很小100，p很小10。 现在的数据n乘p维，n很大300w，p有66列，实际上会比这多得多，比如多个水平的哑变量就会占很多列。p很可能大于1k。 原始数据9gb，存成双精度需要60g的内存。需要双倍的空间才能执行任务，单机不可能。但是我们有分布式。 Y = x \times \beta + \epsilonbeta不大，但是帽子阵根本求不了。要想解决这个问题，最难的在于计算： （X^{t}X）^{-1}，X^{t}有了目标，剩下的就很简单了。 第一个问题：如何构造把X^{t}Y求出来？ 如果x仅有一列，相当于 $ 1n $ 与 $ n1 $相乘，代数运算即一一对应相乘求和，放在转置前看，即每行的元素相乘。如果x有两列，最终结果是2乘1的两个数，第一行为x第一列与y的对应乘积求和，第二行为x第二列与y的对应元素乘积求和。（内积） 第二个问题：如何把X^{t}X构造出来？ 最终得到的是p乘p维的矩阵，第xij位置的元素，为x第i列与x第j列对应元素的乘积（内积）。i可以等于j。 看到所有问题的答案，我们发现，所有的计算都是行内部的计算！那不是很舒服？分行计算就行啊！ 三、作业代码1. 简单线性回归生成回归数据的r文件就不贴了。来看看我写的又臭又长的估计法。 1234readme.txt先使用Rscript reg.r建立reg.csv（回归数据集）再使用MapReducemapper与process文件都是分行操作，使用1个process（reducer）求和就行 下面是生成数据用的R代码。代码中控制了beta的值，可以与最后结果比较。 123456789101112131415#! /usr/bin/env Rscriptn = 1000p = 10x = matrix(rnorm(n*p), n, p)e = rnorm(n)beta = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)y = x%*%beta+0.3*emydata = cbind(x, y)dim(mydata)write.table(mydata, &quot;linear.csv&quot;, sep = &quot;,&quot; , row.names = FALSE, col.names = FALSE)colnames(mydata) = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;x10&quot;, &quot;y&quot;)mydata = data.frame(mydata)myfit &lt;- lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, mydata)myfit$coefficients 这段代码是mapper。 123456789101112131415161718192021222324252627282930313233343536#! usr/bin/env python3# 目标是做一些读取的工作，R做不了不同类型数据的存储import sysimport numpy as np# 先按“读取，把文件里字符型数据中的逗号替换成其他符号def commakiller(abc): i = 1 while(i&lt;len(abc)): abc[i] = abc[i].replace(&quot;,&quot;,&quot;***&quot;) a = &apos;&quot;&apos; abc[i] = a + abc[i] + a i = i + 2 b = &quot;&quot; qline = b.join(abc) return(qline)#reader = csv.reader(sys.stdin)#next(reader)times = 1for line in sys.stdin: abc = line.split(&apos;&quot;&apos;) data = commakiller(abc).split(&quot;,&quot;) p = len(data) if p &lt;= 1: continue #if times == 1 : names = data;times = times + 1;continue if p &gt; 1 : data[p-1] = data[p-1][:-1] # 每行后的换行符 datak = list(map(float, data)) xty = [] for i in range(p-1): xty.append(datak[i] * datak[p-1]) # 默认第p个是因变量 print(&quot;*&quot;,&quot;,&quot;.join(str(i) for i in xty)) xtx = np.outer(datak[0:(p-1)],datak[0:(p-1)]) # 外积 print(&quot;,&quot;.join(&quot;,&quot;.join(str(k) for k in qq) for qq in xtx.tolist())) mapper把数据用逗号分隔，标准输出在屏幕上。用管道将mapper的输出结果能够被吸入process.py（reducer，如下段代码） 123456789101112131415161718192021222324252627282930313233343536#! usr/bin/env python3import sysimport numpy as npxtx = [];xty = [];temp = []for line in sys.stdin: data = line.split(&apos;,&apos;) #print(data) p = len(data) data[p-1] = data[p-1][:-1] if line[0] == &quot;*&quot;: # 说明是标记的xty data[0] = data[0][2:] p1 = len(data) data = list(map(float, data)) if len(xty) == 0: xty = data continue else: xty = xty + np.array(data) # 在新的xtx出现之前 if len(xtx) == 0: xtx = temp else: xtx = np.array(xtx) + np.array(temp) # bug len2 = len(temp) temp = [] # 循环结束初始化 else: # 其他都是xtx data = list(map(float, data)) if len(temp) == 0: temp = data else: temp = temp + data # 连接 xtx = np.asarray(xtx).reshape(int(p1),int(len2/p1))print(np.dot(np.linalg.inv(xtx),np.array(xty))) 原理与之前讲的相似，先计算xtx与xty，求逆（生成的矩阵有时候会奇异，那就重新生成一波） 最后是我们的main主函数shell文件，这个没啥变化 (不要直接跑，我改了文件名) 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bashPWD=$(cd $(dirname $0); pwd)cd $PWD 1&gt; /dev/null 2&gt;&amp;1hadoop fs -put linear.csv /user/devel/hehe/regTASKNAME=linear-heheHADOOP_INPUT_DIR=/user/devel/hehe/reg/linear.csvHADOOP_OUTPUT_DIR=/user/devel/hehe/output/1020outputecho $HADOOP_HOMEecho $HADOOP_INPUT_DIRecho $HADOOP_OUTPUT_DIRhadoop fs -rm -r $HADOOP_OUTPUT_DIRhadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \-D mapred.job.name=$TASKNAME \-D mapred.job.priority=HIGH \-D stream.memory.limit=1000 \-D mapred.reduce.tasks=1 \-D mapred.job.map.capacity=100 \-D mapred.job.map.capacity=100 \-input $&#123;HADOOP_INPUT_DIR&#125; \-output $&#123;HADOOP_OUTPUT_DIR&#125; \-mapper &quot;$PWD/mapper.py&quot; \-reducer &quot;$PWD/process.py&quot; \-file &quot;$PWD/mapper.py&quot; &quot;$PWD/process.py&quot;if [ $? -ne 0 ]; then echo &apos;error&apos; exit 1fihadoop fs -touchz $&#123;HADOOP_OUTPUT_DIR&#125;/donehadoop fs -ls $HADOOP_OUTPUT_DIR | catexit 0 2. 清洗二手车数据二手车数据来自kaggle 给个链接 1234567891011本文件仅适用于二手车数据仅进行了OLS回归，GLM需要在帽子阵计算中加权，未实现本地5w行数据计算成功，hadoop上还未测试na.py用于清洗数据，计算变量均值标准差，并给出适合的列。对300万原数据得到的结果存入vars.txtmapper.py用于简单正态插补，计算帽子矩阵reducer.py用于计算系数阵估计值betahat，得到的结果存入result1.txt中 先看na.py，这个代码对数据na等情况做了处理，有点长，这个文件在处理时单独运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#! usr/bin/env python3## 任务：计算每列的na、均值、方差、总量与内部情况times = 1import sysimport re # 正则化import numpy as npimport mathfrom operator import itemgetter# 先按“读取，把文件里字符型数据中的逗号替换成其他符号def commakiller(abc): i = 1 while(i&lt;len(abc)): abc[i] = abc[i].replace(&quot;,&quot;,&quot;***&quot;) a = &apos;&quot;&apos; abc[i] = a + abc[i] + a i = i + 2 b = &quot;&quot; qline = b.join(abc) return(qline)na_count = &#123;&#125;num_count = &#123;&#125;;var_count = &#123;&#125;strvalue = &#123;&#125;for line in sys.stdin: data = line.split(&apos;&quot;&apos;) items = commakiller(data).split(&apos;,&apos;) #print(&quot;,&quot;.join(str(k) for k in items)) if(times == 1): times += 1 names = items # 后续遍历使用 continue if(times &gt;= 2): ## na的计算与插补需要把全数据遍历 for key, value in enumerate(items): count = int((value == &apos;&apos;) | (value == &quot;--&quot;)) na_count[key] = na_count.get(key, 0) + count # 每行统计缺失值 # get函数，如果对应key是空值，则返回0（设置的默认值），有缺失会被上一行count记录下来 # 计算其他列属性，描述统计 if ((value != &quot;&quot;) &amp; (value != &quot;--&quot;)): #if len(value) &gt; 25: # 超长取值一般都无法处理，删除 # continue try: val_num = float(value) # 如果是数值型变量 num_count[key] = num_count.get(key,0) + val_num # 数值型直接求和 var_count[key] = var_count.get(key,0) + val_num ** 2 # 平方求和 except(ValueError): try: # 数值型变量带单位的，如下处理 if ((&quot;in&quot; in value[-5:])&amp;(re.findall(&apos;[a-z]in&apos;,value)==[])): val_num = float(re.sub(&apos;in&apos;,&apos;&apos;,value,1)) num_count[key] = num_count.get(key,0) + val_num var_count[key] = var_count.get(key,0) + val_num ** 2 elif ((&quot;seats&quot; in value[-8:])&amp;(re.findall(&apos;[a-z]seats&apos;,value)==[])): val_num = float(re.sub(&apos;seats&apos;,&apos;&apos;,value,1)) num_count[key] = num_count.get(key,0) + val_num var_count[key] = var_count.get(key,0) + val_num ** 2 elif ((&apos;gal&apos; in value[-6:])&amp;(re.findall(&apos;[a-z]gal&apos;,value)==[])): val_num = float(re.sub(&apos;gal&apos;,&apos;&apos;,value,1)) num_count[key] = num_count.get(key,0) + val_num var_count[key] = var_count.get(key,0) + val_num ** 2 #elif &apos;RPM&apos; in value[-3:]: # val_num = float(re.sub(&apos;RPM&apos;,&apos;&apos;,value,1)) # num_count[key] = num_count.get(key,0) + val_num # 带单位的只有这几个，数值化后，全存进num_count的字典中 # 出了点问题，&apos;148 lb-ft @ 200 RPM&apos; 这什么意思（于是这列被删了） # 下面处理所有字符类型的变量，用字典存储元素，并计算种类和数量 #print(num_count) else: strvalue.setdefault(key,&#123;&#125;) # 设定每个变量默认字典初始值为空 strvalue[key][value] = strvalue[key].get(value,0) + 1 # 每次更新对应元素的value，+1 except(ValueError): #print(&apos;转换失败 第%s列\t%s&apos;%(key,names[key])) continue times += 1### 1.处理 naabort = [];fix = [];perf = []print(&quot;,&quot;.join(str(k) for k in names)) # 变量名-1行sorted_na_count = sorted(na_count.items(), key=itemgetter(0))for num, count in sorted_na_count: na01 = times - count - 1 print(&apos;%s\t%s\t%s&apos; % (num, count,times)) # 每列缺失值-66行 value = int(count) key = int(num) lendata = times - 1 # 数据长度 if key == 0: abort.append(key) continue # ID列直接加入废弃 if (value/lendata) &gt;= 0.3: abort.append(key) # 把大于30% 的缺失列号加入废弃 elif ((value/lendata &gt; 0) &amp; (value/lendata &lt; 0.3)): fix.append(key) elif(value == 0): perf.append(key)print(&quot;需要丢掉的列号：%s\n需要插补的列号：%s\n完美列号：%s&quot; % ((&quot;,&quot;.join(str(k) for k in abort)),(&quot;,&quot;.join(str(k) for k in fix)),(&quot;,&quot;.join(str(k) for k in perf))))### 2.数值型变量sorted_num_count = sorted(num_count.items(), key=itemgetter(0)) for num,count in sorted_num_count: if int(num) in abort: print(&apos;丢掉第%d列\t%s\t-是首列或因na过多&apos;%(num,names[num])) continue xbar = count/na01 if xbar &gt; 100000: print(&apos;丢掉第%d列\t%s\t-xbar大于100000&apos;%(num,names[num])) continue sdlist = math.sqrt(var_count[num]/na01 - xbar**2) # EX2 - (EX)2 print(&apos;%s\t%s\t%s\t%s&apos; % (num, names[num], xbar, sdlist)) # num是列号，count是全元素和 ## 问题，会出现很多大均值的列，不清楚为什么，需要筛选### 3.字符型变量for key in strvalue.keys(): # 把所有字符型的key遍历一遍 (都是列号) if int(key) in abort: try: print(&apos;丢掉第%d列\t%s\t-na过多&apos;%(key,names[key])) continue except(TypeError): print(&apos;丢掉第%d列\t%s\t-数据出界&apos;%(key,names[key])) continue if len(strvalue[key].keys()) &gt; 10 : # 字符型变量之内有个统计，也存成dict，现在要取值大于5类的变量都消灭掉 print(&apos;丢掉第%d列\t%s\t-类数大于10&apos;%(key,names[key])) continue # 分行操作时，可能有些时候会保留一些取值本来很多的变量，不过没关系 sorted_str_count = sorted(strvalue[key].items(), key=itemgetter(0)) # 变量内部的字典-再计数，根据变量名这个key放回到names中找原位置 print(&apos;%s&apos;%(&apos;第%d列&apos;%(key))) for num, count in sorted_str_count: try: print(&apos;%s\t%s\t%s&apos; % (num, names[key], count)) except(TypeError): print(&apos;上一行的有问题，丢掉第%d列\t%s\t-type_error了&apos;%(key,names[key])) continue 上面处理时，其实要十分了解原数据的含义和数据初始形式。在数据处理之前，尽可能选择取样观察，或者利用信息提前计划。 na.py的处理中，会给出需要删除/不需删除，各列的均值与标准差等。结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302[devel@emr-header-1 1026]$ cat vars.txtvin,back_legroom,bed,bed_height,bed_length,body_type,cabin,city,city_fuel_economy,combine_fuel_economy,daysonmarket,dealer_zip,description,engine_cylinders,engine_displacement,engine_type,exterior_color,fleet,frame_damaged,franchise_dealer,franchise_make,front_legroom,fuel_tank_volume,fuel_type,has_accidents,height,highway_fuel_economy,horsepower,interior_color,isCab,is_certified,is_cpo,is_new,is_oemcpo,latitude,length,listed_date,listing_color,listing_id,longitude,main_picture_url,major_options,make_name,maximum_seating,mileage,model_name,owner_count,power,price,salvage,savings_amount,seller_rating,sp_id,sp_name,theft_title,torque,transmission,transmission_display,trimId,trim_name,vehicle_damage_category,wheel_system,wheel_system_display,wheelbase,width,year0 0 30006001 242727 30006002 2980472 30006003 3000040 30006004 2579859 30006005 13543 30006006 2936507 30006007 0 30006008 491285 30006009 3000040 300060010 0 300060011 0 300060012 77901 300060013 100578 300060014 172383 300060015 100578 300060016 0 300060017 1426595 300060018 1426595 300060019 0 300060020 572568 300060021 175452 300060022 160673 300060023 82721 300060024 1426595 300060025 159733 300060026 491266 300060027 172383 300060028 2 300060029 1426595 300060030 2999953 300060031 2817055 300060032 0 300060033 2864591 300060034 0 300060035 159722 300060036 0 300060037 0 300060038 0 300060039 0 300060040 369087 300060041 200042 300060042 0 300060043 159766 300060044 144387 300060045 0 300060046 1517012 300060047 481415 300060048 0 300060049 1426595 300060050 0 300060051 40828 300060052 52 300060053 0 300060054 1426595 300060055 517782 300060056 64166 300060057 64166 300060058 115826 300060059 116293 300060060 2999953 300060061 146731 300060062 146731 300060063 159698 300060064 159746 300060065 0 300060066 0 300060067 0 300060068 0 300060069 0 300060070 0 300060071 0 300060072 0 300060073 0 300060074 0 300060075 0 300060076 0 300060077 0 300060078 0 300060079 0 300060080 0 300060081 0 300060082 0 300060083 0 300060084 0 300060085 0 300060086 0 300060087 0 300060088 0 300060089 0 300060090 0 300060091 0 300060092 0 300060093 0 300060094 0 300060095 0 3000600需要丢掉的列号：0,2,3,4,6,9,17,18,24,29,30,31,33,46,49,54,60需要插补的列号：1,5,8,12,13,14,15,20,21,22,23,25,26,27,28,35,40,41,43,44,47,51,52,55,56,57,58,59,61,62,63,64完美列号：7,10,11,16,19,32,34,36,37,38,39,42,45,48,50,53,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95丢掉第0列 vin -是首列或因na过多1 back_legroom 34.88554152025584 10.803048024003107丢掉第4列 bed_length -是首列或因na过多8 city_fuel_economy 18.973479961834286 11.63730044599695110 daysonmarket 76.0455595699392 108.8786389693851811 dealer_zip 50669.81150830218 27375.9471530072612 description 0.17846959890341893 255.254990288853114 engine_displacement 2797.285075413276 1481.021922075438416 exterior_color 2.260508282512925 442.692251286219121 front_legroom 39.72592702320007 10.0275677540074222 fuel_tank_volume 17.608113546641402 6.766881529532243525 height 62.35115022029862 16.54168482291875726 highway_fuel_economy 24.641676545249798 13.02037938974785527 horsepower 233.69342021376397 105.137992620342328 interior_color 2.2194684128069095 559.171300069515334 latitude 36.97614874552056 5.02565334927990435 length 183.34090699859522 47.80448637471194丢掉第38列 listing_id -xbar大于10000039 longitude -90.62260148008761 13.96776560999006943 maximum_seating 5.18447183379052 1.682642970903822444 mileage 29640.29273455067 73067.9356611649145 model_name 58.31043968221012 333.09392240515734丢掉第46列 owner_count -是首列或因na过多48 price 29926.953581444013 19568.71786522238550 savings_amount 550.8380996594346 1079.35114969556851 seller_rating 4.211379641169585 0.7130308234825646丢掉第52列 sp_id -xbar大于10000059 trim_name 0.04460616030332584 10.33117366974130263 wheelbase 109.1642761329817 29.52907776980030664 width 74.1947137895581 19.147609601416865 year 2017.2940512877597 29.89588340213624丢掉第0列 vin -na过多丢掉第5列 body_type -类数大于10丢掉第7列 city -类数大于10丢掉第12列 description -类数大于10丢掉第13列 engine_cylinders -类数大于10丢掉第15列 engine_type -类数大于10丢掉第16列 exterior_color -类数大于10丢掉第19列 franchise_dealer -类数大于10丢掉第20列 franchise_make -类数大于10丢掉第23列 fuel_type -类数大于10丢掉第28列 interior_color -类数大于10丢掉第32列 is_new -类数大于10丢掉第36列 listed_date -类数大于10丢掉第37列 listing_color -类数大于10丢掉第40列 main_picture_url -类数大于10丢掉第41列 major_options -类数大于10丢掉第42列 make_name -类数大于10丢掉第45列 model_name -类数大于10丢掉第47列 power -类数大于10丢掉第53列 sp_name -类数大于10丢掉第55列 torque -类数大于10丢掉第56列 transmission -类数大于10丢掉第57列 transmission_display -类数大于10丢掉第58列 trimId -类数大于10丢掉第59列 trim_name -类数大于10丢掉第61列 wheel_system -类数大于10丢掉第62列 wheel_system_display -类数大于10丢掉第17列 fleet -na过多丢掉第18列 frame_damaged -na过多丢掉第24列 has_accidents -na过多丢掉第29列 isCab -na过多丢掉第49列 salvage -na过多丢掉第54列 theft_title -na过多丢掉第31列 is_cpo -na过多丢掉第33列 is_oemcpo -na过多丢掉第6列 cabin -na过多丢掉第2列 bed -na过多丢掉第1列 back_legroom -类数大于10丢掉第3列 bed_height -na过多丢掉第4列 bed_length -na过多丢掉第8列 city_fuel_economy -类数大于10丢掉第9列 combine_fuel_economy -na过多丢掉第10列 daysonmarket -类数大于10丢掉第11列 dealer_zip -类数大于10丢掉第14列 engine_displacement -类数大于10丢掉第21列 front_legroom -类数大于10丢掉第25列 height -类数大于10丢掉第26列 highway_fuel_economy -类数大于10丢掉第27列 horsepower -类数大于10丢掉第22列 fuel_tank_volume -类数大于10丢掉第30列 is_certified -na过多丢掉第34列 latitude -类数大于10第35列 &apos;Backup Camera&apos; length 1 4-wheel antilock length 1 Traction control - ABS and driveline length 1 automatic high beam on/off|Glass length 1 body-color (Not available on Double Cab models.)|Glass length 1 driver 8-way power|Seats length 1 front passenger length 1 heated driver and front passenger|Console front center with 2 cup holders and storage length 1 includes rear storage drawer (Excludes storage drawer with (GAT) All Terrain with (ABD) 5-passenger seating.)|Power outlet length 2第38列 2 in front door panel listing_id 1 3-prong household style located on the rear of center console|Cup holders 2 in front center console listing_id 1 Xenon headlights&quot;***V6***3600.0***V6***Silver***True***False***False******42.1 in***19 gal***Gasoline***False***59.1 in***28.0***304.0***Gray (Dark Grey)***True*********False******31.8552***202 in***2020-08-27***SILVER***280340040***-106.028***https://static.cargurus.com/images/forsale/2020/08/26/00/06/2016_cadillac_xts-pic-1844722391038826724-152x114.jpeg***&quot;[&apos;Leather Seats&apos; listing_id 1 deep-tinted|Wipers listing_id 1 folding|Dead pedal listing_id 1 rear (Requires Crew Cab or Double Cab model. Deleted with (ZW9) pickup box delete.)|Bumper listing_id1 tilt and telescopic|Display listing_id 2 top|Tailgate listing_id 1第39列 &apos;Navigation System&apos; longitude 1 2 bottle holders in front door panel longitude 1 2 in front door panel longitude 1 EZ-Lift and Lower (Deleted when (ZW9) pickup box delete is ordered.)|Remote Locking Tailgate|Radio longitude 1 driver instrument information enhanced longitude 2 driver|Steering wheel longitude 1 front chrome|CornerStep longitude 1 front intermittent longitude 1第43列 &apos;Heated Seats&apos; maximum_seating 1 3-channel programmable|Defogger maximum_seating 2 3-passenger (includes child seat top tether anchor)|Instrumentation maximum_seating 1 HD|Floor covering maximum_seating 1 chrome|4X4 chrome badge (Included and only available with 4X4 models.)|Grille surround maximum_seating 1 driver instrument information enhanced maximum_seating 1 miles/kilometers|Driver Information Center maximum_seating 1 tilt and telescopic|Display maximum_seating 1第44列 &apos;Android Auto&apos; mileage 1 6-gauge cluster featuring speedometer mileage 1 chrome|Headlamps mileage 1 color-keyed carpeting|Driver Information Center mileage 1 driver instrument information enhanced mileage 1 enhanced mileage 1 one color|Sensor mileage 1 rear-window electric|Cup holders 2 in front center console mileage 2丢掉第46列 owner_count -na过多第48列 &apos;Bluetooth&apos; price 1 10 total|Lighting price 2 3-channel programmable|Air conditioning price 1 cargo box with switch on center switch bank (Deleted when (ZW9) pickup box delete is ordered.) (Deleted with (ZW9) pickup box delete.)|Fog lamps price 1 power price 1 power with driver and passenger Express-Down/Up|Cruise control price 1 right front passenger and rear seat occupants|Defogger price 1 voltage and oil pressure|Driver Information Center price 1第50列 &apos;Backup Camera&apos; savings_amount 1 cargo compartment savings_amount 2 halogen|Mirror caps savings_amount 1 inside rearview manual day/night|Lighting savings_amount 1 programmable|Pedals savings_amount 1 right front passenger and rear seat occupants (Dual-zone climate control when (GAT) All Terrain is ordered. Tri-zone climate control on all other models.)|Defogger savings_amount 1 steering wheel mounted|Mirror savings_amount 1 warning messages and vehicle information|Windows savings_amount 1第51列 &apos;Remote Start&apos;]&quot;***Cadillac***5 seats***64070.0***XTS***3.0***&quot;304 hp @ 6 seller_rating 1 body-color (Included and only available with (GAT) All Terrain HD Package.) (Included and only available with (GAT) All Terrain Package and mirror caps will be Black.)|Mirror caps seller_rating 1 inside rearview manual day/night seller_rating 1 interior with theater dimming seller_rating 1 power with driver express up and down and express down on all other windows|Visors seller_rating 1 power-adjustable for accelerator and brake|Climate control seller_rating 1 rear-window electric|Mirror seller_rating 1第52列 cargo compartment sp_id 1 chrome|Glass sp_id 1 driver and front passenger illuminated vanity mirrors|Assist handle sp_id 1 frameless|Visors sp_id 1 inside rearview auto-dimming|Lighting sp_id 1 second row reading lamps integrated into dome light sp_id 2 tri-zone automatic with individual climate settings for driver sp_id 1800 RPM&quot;***19950.0***False***65*********private seller***False***&quot;264 lb-ft @ 5 sp_id 1丢掉第60列 vehicle_damage_category -na过多第63列 &apos;Bluetooth&apos; wheelbase 1 GMC Smart Driver wheelbase 2 front reading lamps|Shift knob wheelbase 1 frontal and side impact for driver and front passenger driver inboard seat-mounted side-impact wheelbase1 interior with dome light wheelbase 1 rear child security|Teen Driver mode a configurable feature that lets you activate customizable vehicle settings associated with a key fob wheelbase 1 tachometer wheelbase 1第64列 &apos;Backup Camera&apos; width 1 Marketplace and more (Limitations apply. Not transferable. Standard connectivity available to original purchaser for ten years from the date of initial vehicle purchase for model year 2018 or newer GMC vehicles. See onstar.com for details and further plan limitations. Connected Access does not include emergency or security services. Availability and additional services enables by Connected Access are subject to change.)|Rear Vision Camera|Door locks width 2 driver side knee and head curtain side-impact for all rows in outboard seating positions (Always use safety belts and the correct child restraints. Children are safer when properly secured in a rear seat in the appropriate child restraint. See the Owner&apos;s Manual for more information.)|Rear Vision Camera|Door locks width 1 driver- and passenger-side door switch with delayed entry feature width 1 leather-wrapped|Brake width 1 to encourage safe driving behavior. It can limit certain vehicle features width 1 voltage and oil pressure|Driver Information Center width 1第65列 &apos;CarPlay&apos;]&quot;***Chevrolet***6 seats***42921.0***Silverado 2500HD***1.0******56995.0***False***4549***4.814814814814815***285608***Platinum Auto Group***False******A***Automatic***t78815***LT Crew Cab 4WD******4WD***Four-Wheel Drive***153.7 in***80.5 in***2019&quot; year 1 4.2-inch diagonal color display includes driver personalization year 1 and it prevents certain safety systems from being turned off. An in-vehicle report gives you information on your teen&apos;s driving habits and helps you to continue to coach your new driver|Tire pressure monitoring system|Horn year 1 cargo lights year 1 parking year 1 rear child security|Rear seat reminder|Teen Driver configurable feature that lets you activate customizable vehicle settings associated with a key fob year 2 rear child security|Teen Driver mode a configurable feature that lets you activate customizable vehicle settings associated with a key fob year 1第66列 上面的结果就是我们处理的标准。 下面看看mapper.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#! usr/bin/env python3# 目标：插补na，并计算乘积 xtx与xtyimport sysimport reimport numpy as np# 先按“读取，把文件里字符型数据中的逗号替换成其他符号def commakiller(abc): i = 1 while(i&lt;len(abc)): abc[i] = abc[i].replace(&quot;,&quot;,&quot;***&quot;) a = &apos;&quot;&apos; abc[i] = a + abc[i] + a i = i + 2 b = &quot;&quot; qline = b.join(abc) return(qline)num_list = &#123;&apos;1&apos;:[34.886,10.803],&apos;8&apos;:[18.973,11.637],&apos;10&apos;:[76.046,108.879],&apos;11&apos;:[50669.812,27375.947],&apos;14&apos;:[2797.2851,1481.0219],&apos;21&apos;:[39.726,10.028],&apos;22&apos;:[17.608,6.767],&apos;25&apos;:[62.351,16.542],&apos;26&apos;:[24.642,13.020],&apos;27&apos;:[233.693,105.138],&apos;35&apos;:[183.341,47.804],&apos;43&apos;:[5.184,1.683],&apos;44&apos;:[29640.293,73067.936],&apos;64&apos;:[74.195,19.148],&apos;51&apos;:[4.211,0.713],&apos;63&apos;:[109.164,29.529],&apos;48&apos;:[29926.954,19568.718]&#125;nafixed = []times = 1for line in sys.stdin: abc = line.split(&apos;&quot;&apos;) items = commakiller(abc).split(&quot;,&quot;) if(times == 1): times += 1 names = items # 列名，后续遍历使用 continue if(times &gt;= 2): times += 1 p = len(num_list.keys()) # 变量长度 ## 1. na fix for key, value in enumerate(items): if str(key) in num_list.keys() : if((value == &apos;&apos;) | (value == &quot;--&quot;)): # 每行统计缺失值 mu = num_list[str(key)][0] #期望 sigma = num_list[str(key)][1] #标准差 nafixed.append(np.random.normal(mu, sigma, 1)) else: nafixed.append(value) else: continue # 对变量取值处理 if p &lt;= 1: continue if p &gt; 1 : # 全部数值变量转换为浮点型数据 flag = 0 # 每行从0开始算 for value in nafixed: try: nafixed[flag] = float(value) flag += 1 except(ValueError): try: ## 1.数值型变量带单位的，如下处理 if ((&quot;in&quot; in value[-5:])&amp;(re.findall(&apos;[a-z]in&apos;,value)==[])): nafixed[flag] = float(re.sub(&apos;in&apos;,&apos;&apos;,value,1));flag += 1 elif ((&quot;seats&quot; in value[-8:])&amp;(re.findall(&apos;[a-z]seats&apos;,value)==[])): nafixed[flag] = float(re.sub(&apos;seats&apos;,&apos;&apos;,value,1));flag += 1 elif ((&apos;gal&apos; in value[-6:])&amp;(re.findall(&apos;[a-z]gal&apos;,value)==[])): nafixed[flag] = float(re.sub(&apos;gal&apos;,&apos;&apos;,value,1));flag += 1 else: nafixed[flag] = np.random.normal(list(num_list.values())[flag][0],list(num_list.values())[flag][1],1) flag += 1 except(ValueError): nafixed[flag] = np.random.normal(list(num_list.values())[flag][0],list(num_list.values())[flag][1],1) # 未经或无法转换的值当na，插补处理 flag += 1 continue ## 3. computing XTX &amp; XTY xty = [] for i in range(p-1): xty.append(nafixed[i] * nafixed[p-1]) # 默认第p个是因变量price (事先设定) print(&quot;*&quot;,&quot;,&quot;.join(str(i) for i in xty)) xtx = np.outer(nafixed[0:(p-1)],nafixed[0:(p-1)]) # 外积 print(&quot;,&quot;.join(&quot;,&quot;.join(str(k) for k in qq) for qq in xtx.tolist())) mapper算出各行的xtx与xty，标准输出时以开头有无“*”来判定。 下面看看reducer 1234567891011121314151617181920212223242526272829303132333435363738394041#! usr/bin/env python3import sysimport numpy as npxtx = [];xty = [];temp = []for line in sys.stdin: data = line.split(&apos;,&apos;) #print(data) p = len(data) data[p-1] = data[p-1][:-1] if line[0] == &quot;*&quot;: # 说明是标记的xty data[0] = data[0][2:] p1 = len(data) data = list(map(float, data)) if len(xty) == 0: xty = data continue else: xty = xty + np.array(data) # 在新的xtx出现之前 if len(xtx) == 0: xtx = temp else: xtx = np.array(xtx) + np.array(temp) # bug len2 = len(temp) temp = [] # 循环结束初始化 else: # 其他都是xtx data = list(map(float, data)) if len(temp) == 0: temp = data else: temp = temp + data # 连接 xtx = np.asarray(xtx).reshape(int(p1),int(len2/p1))print(xtx,&apos;\n&apos;)print(xty,&apos;\n&apos;)try: print(np.dot(np.linalg.inv(xtx),np.array(xty)))except(LinAlgError): continue 由于某些原因，分布式没跑成（大家都去运行，系统拥堵了），只用了五万数据单机测试了一下。最终结果如下： 12345[devel@emr-header-1 1026]$ cat result1.txt[-1.37866211e+01 -3.97949219e+00 4.95000000e+01 -1.43750000e+01 -1.50000000e+01 -1.48950195e+00 -8.51562500e-01 -3.92000000e+02 -6.60400391e-01 5.33750000e+01 1.35742188e-01 7.20000000e+03 -9.96875000e+00 -4.97070312e-01 9.94726562e+00 -8.00781250e-02] 最后附上main文件。 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bashPWD=$(cd $(dirname $0); pwd)cd $PWD 1&gt; /dev/null 2&gt;&amp;1hadoop fs -put used_cars_5w.csv /user/devel/2020210995wangyuanhe/regTASKNAME=lm-usedcars-heheHADOOP_INPUT_DIR=/user/devel/2020210995wangyuanhe/reg/used_cars_5w.csvHADOOP_OUTPUT_DIR=/user/devel/2020210995wangyuanhe/output/1026outputecho $HADOOP_HOMEecho $HADOOP_INPUT_DIRecho $HADOOP_OUTPUT_DIRhadoop fs -rm -r $HADOOP_OUTPUT_DIRhadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \-D mapred.job.name=$TASKNAME \-D mapred.job.priority=NORMAL \-D mapred.reduce.tasks=1 \-file &quot;$PWD/mapper.py&quot; &quot;$PWD/reducer.py&quot; \-input $&#123;HADOOP_INPUT_DIR&#125; \-output $&#123;HADOOP_OUTPUT_DIR&#125; \-mapper &quot;$PWD/mapper.py&quot; \-reducer &quot;$PWD/reducer.py&quot; if [ $? -ne 0 ]; then echo &apos;error&apos; exit 1fihadoop fs -touchz $&#123;HADOOP_OUTPUT_DIR&#125;/donehadoop fs -ls $HADOOP_OUTPUT_DIR | catexit 0 至此，分布式的运用基本结束了。 四、总结先放一个系统卡住的样子 同学的代码写的很糟，放上去跑不动，系统没资源……理由有很多，总之就是卡了。 上面代码漏说了一个技巧： Tail -n 99 used_cars_data.csv就可以只取一部分行 我们可以把大文件一条一条读，与stdin一样，这样就能对数据按行操作。 Slice 指每次读入多大的数据，如1024k，2000行。你想使用集成化工具时，可以这样做，当while循环跑到最后一行时停止。 但这样太慢了。因为每次都在找，不能存入内存 不过，SAS软件允许在很有限的内存中处理大量数据，每次只操作一行（有钱任性） 分布式系统上，最好是stdin的形式，标准输入输出。 处理中新的问题：我现在有个很长的数据，其中有一列我知道是哑变量。 需要统计：有多少个哑变量，有多少种type，占比如何？如何自动识别呢（频数统计） 设置一个“其他”类，如果我要保证设定的“其他”类的占比大于20%， 大家的问题：1.面向python的编程，而非面向MapReduce的编程 hadoop可以做到streaming，成为数据流。所有操作都应该在第一个循环下操作！这样才能完成对所有数据的处理，如果不能再这个缩进下操作，则代码不能面对分布式。 2.介绍了python中的log模块，记录了一些信息。不要随便把过程打印出来。 老师的程序：在大数据集上找到全部哑变量，并且把哑变量的top取出来 可以用这个 MapReduce如何在分布式系统上呈现的最常听的：键值对。以标准输入输出来理解。key-value，将任何的行拆分成这两部分。必须尊重这两部分的对应关系。一般情况下，键值的对应有一个标准形式。 Map(key,value) ——&gt; list(key2,value2) 拿到了学号的姓名，现在要数一下名字有几画。拿到的是（学号-姓名），输出是（学号-笔画）。 如何体现键值对的影响呢？比如有个数据，记录了某个地区的温度，以及记录温度的设备。位置信息就作为了键，对应的温度就是值。（csv数据是碰巧有换行符作为间隔值） 如果我们把关心的数据拿出来（举个栗子：） 1950，01950，221950，-11 这不是键值，这是一行中两个值，并非是键与值。但经过计算之后，就能得到新的键值对！ map过的key可能变了，不再是原来的key。csv其实是打印换行符对应的一行数据，平时的cat也是一行一换。如果数据是不换行的键值对，那么就需要自己识别key，写自己的map函数。 如果数据很大，那我们不能放进内存。 比如放入：swap交换分区，缓存，页面文件……都在硬盘上。map出来的结果，需要做一定的排序（主要是打乱），打乱之后，数据均匀，负载平衡。 最后，代码的路径应该是： Input —-&gt; Map —-&gt; shuffle —-&gt; reduce &gt; output 操作都以行为单位，都是以标准的键值对形式实施！map与reduce之间可以（且必要）加入排序，这个过程需要硬盘，或者需要很大内存的机器，读写频繁。 MapReduce可以拆分开来，只有map，没有reduce。 数据清洗时，这个很重要。只需要拆分，不需要合并。化整为零，所有资源就能一起打工（bushi）！ 也可以很多mapper很多reduce，拆分成很多份，但每一份都一定有键值对应关系。也可以很多mapper，但只使用一个reduce，此时reduce任务不重，可以在这里合并。 （未完待续）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据之学|交叉验证相关理论介绍]]></title>
    <url>%2F2020%2F10%2F13%2F201010CV%2F</url>
    <content type="text"><![CDATA[交叉验证相关理论介绍2020.10.13 1.1 场景构建源禾同学和正阳同学在某次考试都考了100分，正阳同学实力强劲，学习踏实，掌握核心科技，考了100是实力的体现，因为卷子上只有100分。而源禾同学考100分，因为源禾使用了败者食尘，他课下做了这张卷子的所有题，背了题，考了100分是因为记性好。 谁才是老师喜爱的同学呢？ 在对模型进行测试时，不可以照着“考卷”疯狂练习。尤其是在样本量小的时候，换一张卷子就会原形毕露。 那么不妨站在老师的角度想，如果题库只有这么多题，该怎么出题才能真正考察学生的实力呢？ 1.2 模型/算法的选择在建模时，可选用的模型很多，我们想选用何种模型，此时就需要对模型的“泛化误差”（generalization-error，指在独立测试样本上的期望预测误差，也称测试误差test-error或预测误差prediction-error）进行评估。 在实际建模中，很少能得到样本的精确分布，也无法直接计算泛化误差。基于训练样本得到的样本上平均损失的训练误差是它的一个直接估计，可训练误差会随着模型复杂度（光滑度，自由度）的增加而减小，直至减小到0。（训练均方误差 vs 测试均方误差） 如果训练均方误差很小但测试均方误差较大时，我们称该数据被过拟合。（模型开始背题了） 源禾：任何难处都可以靠增加数据量来解决。（当然解决不了的除外） 事实上，这里涉及到偏差方差权衡（bias-variance trade-off）的问题，如果一个统计模型被称为测试性能好，那么要去该模型具有较小的方差和较小的偏差。直觉上我们会选择有极小偏差可是有很大方差的方法（如画一条通过所有观测的曲线，下图的绿线） 源禾：虽然没有完美的模型，但是有完美模型的传说。 为一个模型来选择何适的光滑度的过程即模型选择。这个问题在训练集较小时，被产生的过拟合现象加大了难度。 1.3 出路 - 重抽样（resampling）应当找到一个方法解决过拟合，而唯一的限制是，没那么多数据。 于是人们想了个办法：让这个测试集来源于训练集。我反复从训练集中抽取样本，对每一个样本重新拟合一个模型，来获取关于拟合模型的附加信息。这就是重抽样方法。在拟合过程中，保留（holding out）训练观测的一个子集，然后对保留的观测运用统计学习方法，从而来估计其测试错误率（test error rate）。 源禾：预测集神圣不可侵犯。 2.1 验证集方法（validation set approach）首先随机地把观测集分成两部分：一个训练集（training set），一个验证集（validation set），或者叫保留集（hold-out set）。模型在训练集上拟合，然后用拟合的模型来预测验证集中观测的响应变量。最后得到的验证集错误率——通常用均方误差作为定量响应变量的误差度量——提供了对于测试错误率的一个估计。 附一个“上帝”的比例：70%的训练集，30%的测试集。 验证集方法原理简单，易于执行，但它有两个潜在的缺陷：1.测试错误率的验证法估计的波动很大，这取决于具体哪些观测被包括在训练集中，哪些观测被包括在验证集中。2.在验证法中，只有一部分观测被用于拟合模型，由于被训练的观测越少，统计方法的表现越不好，意味着验证集错误率可能会高估在整个数据集上拟合模型的测试错误率。 统计分析中通过多次重复试验来减小方差。 2.2 留一交叉验证（leave-one-out cross-validation）留一交叉验证（LOOCV）与验证集方法很相似，但这种方法尝试解决验证集方法遗留的缺陷问题。 LOOCV将观测集分为两部分，但不同于把观测集分为两个大小相当的子集，留一交叉验证法将一个单独的观测$(x_1, y_1)$作为验证集，剩下的观测$\{(x_2, y_2),(x_3, y_3), … ,(x_n, y_n)\}$组成训练集。由于拟合中没有用到$(x_1, y_1)$，所以$MSE_1 = (y_1 - \hat{y_1})^2$ 提供了对于测试误差的一个渐进无偏估计。 能看出，由于$MSE_1$是基于一个单独的观测计算得出的，故具有很高的波动性。 重复上面计算$MSE_1$的步骤，计算出全部的$MSE_1, MSE_2, …, MSE_n$，对测试均方误差的LOOCV估计是这n个测试误差估计的均值： CV_{(n)} = \frac{1}{n} \sum^{n}_{i=1}MSE_i相对于验证集方法，LOOCV方法更不容易高估测试错误率，也能彻底解决训练集和验证集分割时随机性导致的结果不同问题。 2.3 K折交叉验证（K-fold CV）k折交叉验证法是LOOCV的一个替代，这种方法将观测集随机地分成K个大小基本一致的组，或者说折（fold）。第一折作为验证集，然后在剩下的k-1折上拟合模型。均方误差$MSE_1$由保留的观测计算得出。 重复这个步骤k次（注意一般k大于2），每一次把不同的观测组作为验证集（分组只是第一次分）。整个过程会得到k个测试误差的估计，$MSE_1, MSE_2, …, MSE_k$。k折CV估计由这些值求平均计算得到： CV_{(k)} = \frac{1}{k} \sum^{k}_{i=1}MSE_i不难发现，k等于n时，LOOCV方法是k折交叉验证的一个特例。 k一般取5或10。不取n的原因果然还是因为好算啊。几乎对于任一种统计学习方法适用，都有更好的可行性。 源禾：在计算简便和尽可能减少估计的波动面前，一个能“我全都要”的方法谁不喜欢呢？ k折交叉验证的结果也会因观测分折的随机性产生一定波动。同时对$Err$估计时也会因训练集样本容量大小产生一定的高估。 由上图可知，训练集在150+时，训练效果已经不再随着训练集样本量增加而增加。但训练集样本容量在0至50时，会明显低估$1-Err$。 2.4 总结先画一张表在这： 方法 优点 缺点 计算复杂度 验证集方法(validation set approach) 原理简单，易于执行 A.测试错误率的验证法估计的波动很大，与分组关系很大。B.验证集错误率可能会高估在整个数据集上拟合模型的测试错误率。 计算最简单，方便对比称一个计算单步（对数据进行多次重复划分时计算复杂度会相应高） 留一交叉验证法(Leave-one-out cross-validation) A.偏差较小，不易高估错误率。训练模型最接近原始样本的分布。B.LOOCV方法能解决训练集和验证集分割的随机性。实验可复制。 A.模型需拟合n次，非常耗时。（但是最小二乘法来拟合线性或多项式回归时只消耗一个计算单步）B.方差较大。 除左栏提到的线性/多项式回归外，需要n个计算单步。大样本情况时，对于某些算法来说数据划分为n份也不可接受。svm和朴素贝叶斯分类器。 k折交叉验证(k-fold CV) A.偏差问题不大,方差较小。有效避免过拟合和欠拟合情况发生。B.计算方便，计算开销小。 A.选择K折交叉验证的“K”时比较随机。B.会产生一定波动。偏差大小会随训练集样本容量变化而改变。 k个计算单步 k折CV方法相对于LOOCV方法除了计算优势外，它对测试错误率的估计通常来说更加准确。 验证集方法 LOOCV方法 k折CV方法 偏差角度 高估 近似无偏 中等程度偏差 方差角度 k&lt;n时方差大于k折CV方法 k&lt;n时方差小于LOOCV方法 由上表可知，选择方法时，需要进行偏差-方差权衡。在选择k折CV的折数时，一般k=5或10使得测试错误率的估计不会有过大的偏差或方差。 2.5 补充一、 之前提到交叉验证方法可以应用于多个场景，举个例子，交叉验证在分类器模型的应用： 其实只需要修改“泛化误差”为“损失函数”（$MSE$ to $Err$），比如k折CV错误率的形式： CV_{(k)} = \frac{1}{k} \sum^{k}_{i=1}Err_i其中$Err_i = I(y_i \ne \hat{y_i}) $。LOOCV和验证集错误率也可类似定义。 二、 上文提到，在LOOCV方法中，最小二乘法来拟合线性或多项式回归时将只计算一次。 CV_{(n)} = \frac{1}{n} \sum^{n}_{i=1}(\frac{y_i - \hat{y_i}}{1-h_i})^2其中$\hat{y_i}$为用原始最小二乘拟合的第i个拟合值，$h_i$为杠杆统计量： h_i = \frac{1}{n} + \frac{(x_{i} - \bar{x})^2}{\sum^n_{i'}(x_{i'} - \bar{x})^2}区别仅在于第i个残差除了一个系数$(1-h_i)$。杠杆值的大小在0到1之间，反映了一个观测对自己拟合值的影响大小。因此，该公式表明高杠杆值的残差根据它本身偏离数据的程度进行了等量的放大。 三、 若样本量非常小，非常非常小，我们还可以使用重抽样的另一种方法：自助法（bootstrap）。 比如我们有m个样本（m较小），每次在这m个样本中随机采集一个样本，放入训练集，采样完后把样本放回。这样重复采集m次，我们得到m个样本组成的训练集。当然，这m个样本中很有可能有重复的样本数据。同时，用原始的m个样本做测试集。这样接着进行交叉验证。 参考文献[1]James G , Witten D , Hastie T , et al. An Introduction to Statistical Learning[M]. Springer New York, 2013. [2]杨柳,王钰.泛化误差的各种交叉验证估计方法综述[J].计算机应用研究,2015,32(05):1287-1290+1297. [3]范永东. 模型选择中的交叉验证方法综述[D].山西大学,2013. [4]Hastie, Trevor J. The Elements of Statistical Learning[M]. 世界图书出版公司, 2015.]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式1007-Map-Reduce的文字流]]></title>
    <url>%2F2020%2F10%2F07%2F201007hadoop%2F</url>
    <content type="text"><![CDATA[最后编辑于：20.10.15 开门见山地来一段，就一段，不会有人这个都没搞懂吧，不会吧不会吧（拖走 123456hadoop jar \ $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \ -input /user/devel/2020210995wangyuanhe/README.txt \ -output /user/devel/2020210995wangyuanhe/1007output \ -mapper &quot;/usr/bin/cat&quot; \ -reducer &quot;/usr/bin/wc&quot; 开始前再插一句题外话，被强大而可爱的丰丰老师表（da）扬（shang）了，动力+10086，继续努力啊小禾禾！！ 分布式1007-Map-Reduce的文字流1.程序运行情况介绍 这张运行图只是执行中间一部分，正常情况下无ERROR，map 100% reduce 100%（这里运行时看着最爽）。 图2中第一行文件并无内容，后7个文件是本次运行开启的7个mapper的结果，reducer在这几个文件中运行，并把结果写入这7个文件中，所有的结果求和即真正结果。 下面看看运行的文件情况： 上图中，wc函数第一列的和就是16，即行数（验证正确），第二列为单词数（字符串连在一起算一个单词），第三列为字节数。 单个mapper+单个reducer运行 每次cat：行数+1；单词+n；字节数+m 服务器上有很多个mapper，本次有17个（见上图），每个程序都做了cat函数（打印），7个reducer一起运行wc（计算行数）。Hadoop jar 中有这样一个参数，num.tasks，控制任务的数量。 2. 运行的相关介绍reducer结束的很慢，原因是启动时要花资源，map过程非常快（程序运行时有体会）。听说均分文件时会用到哈希code，现在很多算法都是哈希函数的进阶，不知真伪，之后问问。 在传输中，隐含了打乱shuffle和整理sort的过程: 平摊把数据随机打乱，$shuffle$，保证每个mapper接受的任务量相近。打乱顺序的任务再排序，$sort$，使每个程序尽可能找到较近数据。 由于，数据在HDFS上存储在分布式的硬盘上，必须主动从硬盘读到内存里，有I/O（input/output）的消耗。如果数据很多，读起来很慢。一般map很复杂，可能map的中间结果要写入硬盘，又产生I/O消耗，reduce也需要从硬盘中读取。 故HADOOP对硬件读写的要求很高，如此反过来也节约了内存资源（贵）。真正制约hadoop的大多是硬盘读写，因此很多服务器用SSD，但是SSD很容易坏，故需要做冗余（防止硬件坏掉）。 apply函数，groupby函数，都有map的感觉 3. Hadoop 与 Sparkhadoop擅长进行批处理，但不能进行实时计算（比如无人驾驶）、股票高频交易（短时间的计算），这种实时运算需要使数据保持“热状态”不存入硬盘，在map-reduce后立刻传出，与硬盘无关。 hadoop不擅长，但是spark擅长。spark写入硬盘的操作很有限，因此速度快。当然，上文也提到了，内存比硬盘贵，所以hadoop更廉价，两个框架各有胜负。 同时，hadoop不能实现迭代计算（牛顿迭代，神经网络，梯度下降，反向传播），几乎涵盖所有机器学习算法。迭代时需要大量循环，不能经常读写硬盘。 4. 标准输入输出 STDin &amp; STDout在计算机编程中，有一类输入输出只与屏幕有关： Stdout任何程序结果总是需要保存，但有一类输出直接打印在屏幕上。凡是能打印的都是stdout。举些例子：print函数(r,python)，cat函数(r,linux)，printf函数(c)等等等等。基本全部语言都能标准输出。 Stdin计算机能够接受打印的“文字流”（这也是hadoop streaming中streaming的含义！），举些例子：如linux和r的管道函数，python里open函数，都是打开文件把每一行读进来。 linux中很方便地组织你的文件，只要文件是文本文件，都可以用管道“吸入”。很多linux的函数都以cat开头（猫猫头）： 1cat txt | python wc.py 单机实验 再用R语言举个例子： 1234sink(&quot;想保存的文件名.txt&quot;,append = T, splt = T)abc = c(rnorm(100))abcsink() 写进hdfs就是另外一幅模样了： 12345678910111213141516#! /usr/bin/env Rscriptoptions(warn=-1)sink(&quot;/dev/null&quot;)input &lt;- file(&quot;stdin&quot;, &quot;r&quot;) # 用input 吸入来自linux的STDinwhile(length(currentLine &lt;- readLines(input, n=1, warn=FALSE)) &gt; 0)&#123; fields &lt;- unlist(strsplit(currentLine, &quot;,&quot;)) lowHigh &lt;- c(as.double(fields[3]), as.double(fields[6])) stock_mean &lt;- mean(lowHigh) sink() cat(fields[1], fields[2], stock_mean, &quot;\n&quot;, sep=&quot;\t&quot;) sink(&quot;/dev/null&quot;) # dev/null是linux的黑洞目录，扔进去就会消失呢！&#125;close(input) 运行时在linux中用rscript：1Rscript test.r 其实做开发时，java语言很好用（类操作，大型项目架构，内存管理），适合创建非常巨大的项目，运行很久不停止，其他语言不行。hadoop也是java编成。但java并非所有人都会，hadoop面向数据处理，会java的人不多。 由此，先辈们做了个能让各种语言都能识别的框架： hadoop做了很好玩的模块：streaming（标准输入输出的“文本流”）。提供了简单的接口，c，py，java，r……但凡能接受标准输入输出，就可以调用！使得map函数和reduce函数完全脱离了hadoop，只需要输入输出就能得到结果，影响速度的只有map和reduce的写法。 Hadoop不是编程语言，是分布式计算架构。 ——李丰老师 5. 我们的函数，部署！教练，我也想调用hadoop接口跑我自己的程序！ 完全没问题！ 很简单，首先要保证每个存储数据的节点上（worker节点）必须有函数cat、wc，我们自己写一个wchehe.py，然后放上服务器去就好啦。 如下，就是一个简单的读取行数的py程序，第一行一定要注明函数应该怎么找到运行的地方： 12345678#! /usr/bin/env python3import syslinecount=0data = []for line in sys.stdin: linecount += 1 data.append(line)Print(linecount) 可以用chmod +x wchehe.py 改一下运行权限。 下来，为了规范代码格式，我们用一个shell批处理文件作为我们的程序入口，也方便调整参数。 开头别忘了告诉sh函数这是个批处理。看到这篇文章的同学不要用原代码直接跑啊（ 12345678910111213141516171819202122#! /usr/bin/bashPWD=$(cd $(dirname $0); pwd)cd $PWDHADOOP_inputdir=/user/devel/2020210995wangyuanhe/ordertxtfiles/test-edit.txtHADOOP_outputdir=/user/devel/2020210995wangyuanhe/output/1007out01HADOOP_home=/share/hadoop/tools/lib/hadoop-straming-3.1.3.jarecho $HADOOP_homeecho $HADOOP_inputdirecho $HADOOP_outputdirhadoop fs -rm -r $HADOOP_outputdirhadoop jar \ $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \ -files $PWD/wchehe.py \ -input $&#123;HADOOP_inputdir&#125; \ -output $&#123;HADOOP_outputdir&#125; \ -mapper &quot;/usr/bin/cat&quot; \ -reducer &quot;python wchehe.py&quot; -jobconf 被替代为-D，-file 被替换成 -files 附上本次课程老师的代码和讲义，我还得好好研究一下，收获满满的一节课（虽然有点怀疑人生哈哈哈 附作业中可能用到的hadoop jar参数介绍，hadoop fs 参数介绍 （完）]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式0924-分布式服务器基础Linux中主机的远程交互(ssh)]]></title>
    <url>%2F2020%2F09%2F24%2F200924hadoop%2F</url>
    <content type="text"><![CDATA[大数据分布式计算 0924本次课程以上次思考题入手： 并行计算与分布式计算的区别？ 并行计算是一台计算机使用自身共享的内存和算力，用CPU的多核特点进行并行处理。 分布式计算是利用服务器的高算力，从远程交互终端中向服务器部署代码进行计算。 我的理解还是太片面，下面看看李丰老师的想法： 并行计算（parallel computing）是一个通常用于高性能计算（HPC）领域的术语。它具体指的是使用 多个处理器执行计算或模拟。超级计算机是为执行并行计算而设计的。这些系统不一定有共享内存。并 行系统使用MPI这样的工具将在超级计算机或者集群机器上的计算资源调度并实现多任务的同步计算。 并行计算在许多计算软件中都集成了一些基本的实现途径。比如R中的自带parallel包，Python标准 库中的multiprocessing是一个用与 threading 模块相似API的支持产生进程的包。这些程序模块允 许程序员充分利用机器上的多个核心。Unix 和 Windows 上都可以运行。结合OpenMP（Open Multi-Processing）等支持跨平台共享内存方式的多线程并发的编程API，使用现有编程语言可以在大 多数的处理器体系和操作系统中运行并行计算任务。具有并行计算能力的高性能计算平台往往被应用在 很多特定的科学领域，如超级计算机，密码破译，生物医学。 分布式计算（distributed computing）实际上是一个比并行计算更笼统的术语。人们可以将分布式计 算与并行计算的意义等同于并行计算，分布式特指的是将计算分布在许多不同的计算机之间。然而，分 布式计算处理的是并发性以外的其他方面。分布式计算具有并行计算所不具有的一些特性，包括计算一 致性、计算高可用性和高容错性能等。此外现在分布式计算平台的计算成本更低。像本书涉及到的 Hadoop或Spark这样的系统都是分布式计算系统，它们都有处理节点和网络故障的能力。不过，这两种 系统也都是为了执行并行计算而设计的。与MPI等HPC系统不同，这类新型系统即使其中一个计算节点出 现故障，也能继续进行海量计算。分布式计算主要应用在数据科学领域，如互联网、物联网、车联网、 数字金融。 在现代数据科学浪潮的冲击下，利用低成本硬件实现大规模分布式计算成为大数据应用的主流方向。世 界上各大数据科学公司都把分布式计算作为数据科学的核心技术与产品。最为大家熟知的有如亚马逊、 阿里巴巴各大云平台。在数据科学的应用中催生了大量分布式计算的优秀工具，如Hadoop, HDFS, Hive, Spark, Storm。 框架分布式中，有不同的框架： 资源调度器（分布式文件系统HDFS） + 资源管理器（管理计算机资源哪一部分做什么，YARN） + 管理框架（zookeeper &amp; AMBARI） 不同的领域侧重的框架也不同。 电商 - spark（计算）图片处理 - Hadoop存储数据财富 - hive…… 大数据分布式的特性分布式服务器一般由很多同质的软件和硬件构成。 服务器为“刀片式服务器”（不同于塔式服务器），每一个计算节点都是一个刀片，通过网络连接，非核心的节点损坏不影响整体。可以在计算资源空闲时慢慢修复。 masternode|——- workernode1…|——- workernode2…|——- workernode3… HDFS以来的两种逻辑组件，一个是起索引作用的namenode，一个是起存储作用的DataNode。二者数量和位置取决于worker节点的数量：worker很多时，要单独做成服务器，因为他们对算力要求很高；很少时可以置于masternode中。 优点：用廉价的硬件达到较高的存储性能。 缺点：随机存储，故不擅于做随机文件的搜索（会消耗大量算力查找索引），大文件被分成小份，存在不同的datanode中。 根据文件大小切割为相似大小的block：$/tmp/test.txt $|——- blocka|——- blockb &gt;blocka|——- datanode2|——- datanode1blockb|——- datanode1|——- datanode3 注意到，DataNode1被复制了两份，这在分布式服务器中是很常见的。DataNode之间会根据是否空闲以及是否存储了相关数据而进行并行处理。只要namenode在，哪怕一块硬盘坏了，也能恢复。 MapReduce组件位置：Hadoop中 作用：处理大量数据，能用forloop处理的，可以通过分布式发给程序，代码对象应当各自独立。 对服务器？的I-O性能要求较高（input-output） 老师提示：分布式程序中应有容错空间（未领会，无经历） 分布式的MapReduce如下： JobTracker|——- tasktracker1|——- tasktracker2 （注意，ttk1和ttk2之间也互相连通）ttk中容纳的是M-R两步的多个子程序 activeJobs(位置JobTracker)：joba|——- map task1|——- map task2jobb|——- reduce task1|——- reduce task2 服务器讲解远程操作终端：putty 账号密码和ip就略了（逃 1234567891011121314mkdircdlsrm -rf /user/studentstouch lifeng.txtvim lifeng.txtemacs lifeng.txtcat lifeng.txthadoop fs -ls /hadoop fs -put lifeng.txt /user/test hadoop fs -cat /user/test/lifeng.txthadoop fs -get /user/test/lifeng.txt lifeng2.txthadoop fs -ls /test &gt; hadoop-ls-return.txt]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式0917-遍历检索的多进程初试水]]></title>
    <url>%2F2020%2F09%2F17%2F200917hadoop%2F</url>
    <content type="text"><![CDATA[大数据分布式计算 0917本次课程内容讲述的几个注意的有意思的东西： 分布式是什么“数据向代码跑” / “代码向数据跑” 原本的流程：数据 -> cpu(code) -> answer 新流程：cpu -> 数据]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遍历山河|贵州]]></title>
    <url>%2F2020%2F09%2F15%2F200915%E8%B4%B5%E5%B7%9E%E4%B9%8B%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[第一次坐飞机。 夜航|氦核20.07.06 芸芸灯火糅中盘，一子冲天战正酣。班师星中我非客，翼稍挂月天外山。明灯未知夜深浅，颠簸可猜云浓淡。远电殷霞威颜厉，破雾勒马便坦然。 7日行程开始得急，昨夜飞机才至，今天清晨就踏上旅程。 第一次坐飞机的感觉很是神奇，以前总为自己在新时代还没能坐上飞机而感慨，全家也仅仅我一人没有乘坐过飞机出行。飞机这种交通工具于我总有某种遥不可及的神秘感，我也喜欢在文中用“飞”的意象，现在又少了一份能够为家人和我津津乐道的立场。飞机起飞时的加速让我猝不及防，高度变化时我也头晕目眩，甚至会因为身下无依无凭而突然感到害怕，但我依然震撼于夜航所见地面的万家灯火，震撼于穿云破雾时的一往无前，飞在天上的我觉得自己从星星中来，像极了将要下凡的天官。因为刚刚过十五，皎洁而饱满的月亮透过稀薄的云层高悬在机翼远端，注视着祝福着一飞机的旅人。如此种种感情纠缠在一起，甚至落地许久后也没能平复。 今天去了小七孔景区，一开始不明所以，后来才知是道光年间修起的七孔石桥，横跨响水河，天然去雕饰。这里的水泛着奇妙的青绿色，又不似别处清澈，更像翡翠和玉石，也带点“油”的感觉。水动处成涓流，水死处成碧潭，水高地飞落作瀑布，恰如一语“寒烟翠波”所言。飞瀑从山而下，如银龙攀附，面露凶光，口中喑嘶。 其实也没能仔细转完，走走停停拍照歇脚，转过卧龙潭，翠谷瀑布，石上森林，断桥飞瀑，再到拉雅瀑布，最后就见到小七孔了。小七孔没有介绍中那样“天然”，但经过了前面聒噪澎湃的飞瀑激流，此处的她更像无声胜有声，前面的澎湃被跋涉消磨将尽，到此这座饱经风雨的石桥张开怀抱，迎接这碧波末路，汇入大江。站在铜鼓桥上，面对滚滚江水，习习微风，我一时语塞，我消磨过的人事物太多太纷繁，也太匆匆，最终留下的还有什么呢？我总是不察得失，不知悔改。子在川上曰，逝者如斯夫。今日偶然得宽余，才能一览千山万水，于自然中体察得失。 其实旅行团也罢，自驾游也罢，但凡是省内打转的，免不了赶路的时间。今天在大巴车上度过了许久的时间，两小时才能换来二十分钟的放松，我们便会如囚鸟出笼，蜂拥而出。旅游分两种，一种看人文，另一种读山水，我这次出行，大抵属于第二种。可贵州山多，一山放过一山拦，眼前满溢的山光水色还是容易腻，此时我就不自觉地开始寻觅风景背后的些许生活与人味。 夜幕来临，我们总算赶到了千户苗寨。按理来说白天可以看这里勾心斗角的苗族建筑，但夜幕锁上了大部分文化。此处充满商业化的臭味，但总还是值得一看，十余座村寨合成的大寨灯火星点，我们趁着夜色还有时间登上山顶，眼前灯火宛如一盘迷局，十分壮观。 苗族竟是蚩尤氏部落后裔，也颇有来头，还有些奇妙的传说，诸如中元节的传说，苗药的神奇，以及用少女初潮制作的情蛊等等，都令我耳目一新，我还寻思着大概很多人今生盖也无机会制作情蛊，这番思索让我不得不感慨自己总是在这种稍显变态的地方好奇拉满。 不过，本地妹子也确实好看，尤其话语间轻柔的发音习惯让我听来很是舒服，我的语气也不禁缓和下来。当地人更是无比好客。虽然一路上扰我乱我的破烦事多，但听两首苗族祝酒歌，看一番高山流水情长酒，再多烦恼也便抛之脑后了。我还破例喝了几口米酒，或许有些醉，可他们说我没醉，我便也不知道我醉没醉了。 8日旅行也总是混杂着一些愉快和不愉快，我们愉快了，导游不一定愉快，反之亦然。这世界不能人人都舒坦，你舒坦时总有人不舒坦。 其实来之前我以为的旅游，行到水穷处，坐看云起时，举匏樽以相属，寄蜉蝣于天地，渺沧海之一粟。结果现实的旅游团给了我对贪婪更加深刻的认识，早饭是永远不变的鸡蛋馒头咸菜条，午饭好一些，一桌添一口炖菜火锅，几道油盐意难平的家常菜，真不如家里吃的好。导游于是鼓吹苗族长寿皆源于口味清淡的饮食，我也无语。 今天驱车前往一处闻所未闻的非遗博物馆，前几层倒是货真价实的苗族文化，服饰，用品，家装，都有模有样。苗族的银饰着实多种多样，玩出了花，但同时作为民族争斗中的败者，他们也只能在此一些无关紧要的地方花心思。其余银品相关的奇怪传言，但凡是受过高中教育的，都会一笑了之。 可是闹剧才刚刚开始，来到最后的展厅，讲解做了个墨水变色的实验，银碗甚至能改变水质的酸碱性，又大肆宣扬银离子杀菌作用高强，紧接着带着全团走进了远超博物馆规格的银饰品商城。虽然讲解那番反智言论令一个“醒来”的人不适，但我也不想做断人财路的傻事，顺其自然才是此时的最优解。但独善其身者又何止我，饭后上车，导游问大家在有多少消费，结果自然是寥寥无几，直接导致导游对后面的景点毫无讲解的兴致，放任全车人无知中来无知中去了。 好在所到之处是有名的军事重镇镇远，古镇青石板长街，烟雨中行人稀疏，尽管留下的游览时间并不充裕，但节奏依然很慢很舒服。我们踏入古镇侧面的巷子，迈进一座座飞檐叠廊，那即是当地人住的地方，墙上的有年头的方砖有些翻新痕迹，但也有部分破损了，爬满潮湿的青苔。在巷子深处一口井，井中投鱼防毒，我们看了一圈正要离去，恰巧碰见一位当地人拎着绳子和桶去汲水，我第一次见人井中打水，饶有兴致问了问，原来井边那条放绳子的凹痕是世世代代打水人磨出的，并非刻意为之。其实整个打水也没什么特别之处，第一次觉得景点和生活糅合在一起，生活即风景，如此自然天成。 镇远的舞阳河因为小雨变得稍显浑浊，这里人酒足饭饱便躺卧在河边长廊中纳凉，但其实这里找不到几个真正凉爽的地方，更多的是潮湿与闷热，行走时还好，站定就会收获停不下来的汗水。我在路边奶茶店买了一杯并不中意的奶茶，但是惊喜之处在于奶茶店中温柔的老板姐姐和两面贴满便利贴的许愿墙。老板姐姐自不必说，更值得称道的是许愿墙上纯真可爱的文字与愿望，有本地人有外地人，有成年人有小孩子，有痛苦和忧虑的祈祷，也有满心欢喜的纪念和憧憬。这一面许愿墙，并没有满满地写着人的欲望，更多的竟然是祝福和期待未来，我对贵州人风土人情也大概摸清一二了。 结束了旅程，又进入赶赴下一个地点的大巴车程。路上又开始寂寞，翻照片，看到烟雨半掩的古镇和青石板街，突然想念起一位朋友，可惜因为种种原因，我再无问候的立场了。晚上烤鱼也无味了，可能是我以前吃过家门口的麻辣重口烤鱼，对于眼前这条平淡的存在实在无感了吧。 9日贵州省会贵阳名副其实，今早起来果不其然又浓云密布。昨晚住的酒店旁边有条小溪，酒店也起了个恰如其分的名字叫“栖溪”，然而店家倒是惬意了，住客则要忍受经久不息的激流声，以及山中那无比潮湿的空气，早上起来诸位无一不是困意十足，料想昨晚一定默念了百遍“逝者如斯”。 昨晚经同学极力推荐，我们去吃了贵州的烤鱼，这算是几日来吃的最好的一次，虽然价格贵了点，可相当有滋味，配上小米酒冰红茶，很受用。独特的江口烤小豆腐脆皮软心，外表冷漠，内心却还是狂热的。虽然早有人给我打过预防针，贵州食物辣度非凡，可是我只觉得贵州人对于酸更加钟爱。本地甚至有“三日无酸，腿打捞酸”的警世名言，仅看遍地开花的酸汤鱼店以及逢菜必放的西红柿，可见一斑。昨晚烤鱼中剩下不少余料，土豆黄瓜锅巴粉什么的，正好也调剂了每日清晨的咸菜馒头。 其实昨晚睡得地方是梵净山山脚，离旅游区不过五分钟路。若说是仙境倒是夸大了，但梵净山确实不同于我以往去的任何地方。时间所限，摆渡车缆车把我们送上半山腰，刚开始谁都没有意识到，当缆车上看到远山刺透云层，而我们的缆绳也向着虚无的朦胧中无限延伸时，才恍然大悟周身大雾是高山云雾。云雾比通常所见大雾更加细腻，肉眼可见的小水滴浮在空中，不一会眼镜就全花掉了。 这么介绍下去，恐怕这篇文章也像受了潮气一样无趣。这两天最喜欢的两句诗，一句是林则徐的“风雨冥冥极漏天”，一句是毛主席的“胜似闲庭信步”，两句深得我心。在爬山过程中，我做先锋，一路高歌猛进，森林栈道拾级而上，没料到台阶太密太紧凑，让人疲惫不堪。蘑菇石这里奇石林立，那些方砖似的巨石层叠而起，如天外来客，危险诡妙地搭在山顶。视觉上无比险峻易碎，可真实情况是这里的地质无比稳定，经久不变。蘑菇石甚至目睹沧海桑田，区区人类文明不足道也。 孤独的旅途中，我攀上峰顶，突然想拍张照，可是一时间找不到人。四周没有深渊巨谷，只有逼人窄道和沾衣的云露，伴着早早来此工作的僧侣和清洁工，我发了会呆，这里大概不属于我，我亦不属于这里。我想求神拜佛让我忘记烦恼，可我不能求“无求”，而烦恼大多生于所求。我怎么能用欲望来战胜欲望，用烦恼来解决烦恼呢？然而事实只能如此，所以佛门离我还是太远，真希望有一天我也能找到属于我的答案。不过神奇的是，今天山上寺院中两位僧人师父看了我许久，可是欲言又止，不知是我犯了禁忌还是面露凶光，大家都喜欢对我欲言又止，这可苦坏了我这个蠢人。 路上，天气突然转晴，那是从未见过的蓝天白云，然而转瞬即逝了。 10日和父母出去，总不如自己一人出行更加完整，至少体验上看，不论是突然对某些问题发问，亦或是对拍照的执着，父母常常打断我游览的思路。如果说满状态的体验可以到达80分，那么父母主导的旅行最多50分。不过看在贵州山水的份上，这趟旅行还算没有失去灵魂。 今天早上仍然是无聊的购物环节，导游来之前大谈国学风水，还将迷信说成信仰，结果今早全都泡了汤，我们并没有领情。商场中宝石确实超出了我们的理解，在我看来毫无价值的石头，在偏僻无人烟的犄角旮旯这样一座藏宝宫里，竟然成倍地涨价，以至于完全消费不起。找到知情人了解了产业链，才知道这小小宝石养活了导游导购匠人还有批发商这么一群人，谁不想从中抽点东西分一杯羹呢？不过导购也十分尽职，至少她盯着我们走了一早上，甚至当我找不到同伴时，她会告诉我们此人所在。讲解与推销双管齐下，还专门挑出可能对我们口味的商品挨个询问，只可惜定价着实太高，都是些明摆着“谁买谁是大傻子”的东西，不然以导购之勤奋没有功劳也有苦劳，定能成功。 抛开购物不谈，今天的早中两顿饭都是极好的，四星酒店待遇真不同，早餐一改往日馒头咸菜，变成了多姿多彩的自助餐，一句诗来说“萧瑟秋风今又是，换了人间”，爸妈胃口大开，我还是量力而为。中午饭也不错，不再从头酸到脚，下饭者下饭，充饥者充饥，竟然各司其职起来，让我大快朵颐。 黄果树瀑布是好的。看完有诗为证： 白龙破水自巍吟，骤雨穿林渐希音。银河玉碎比拟俗，七进七出战袍轻。散落人间星满镜，厮磨耳鬓石衔青。闲庭信步岿然立，云露拌作落汤鸡。 其实一路上感慨良多，但思来想去都是些和人相关的复杂情感，在大自然面前我仍然只能乖乖做个孩子，除了敬畏以外也不剩太多痴心妄想。这些年人造之景愈发多了，本来无甚可看，但经过修修造造总算能凑齐一个景点，总的来说还是没必要，整体质量下降了不少。黄果树瀑布是中国第一大瀑布，至少是我贫瘠的旅游生涯中所见的最大的瀑布了，适逢汛期，瀑布水大，看完后水汽沾衣浑身湿透，却连连大呼震撼爽快。 晚上在贵阳市里闲逛，碰巧天晴，看到了夜晚的蓝天。这里仗着地质稳定，飞楼百米接二连三，高楼林立。甲秀楼南明河夜景倒是普普通通，不过南门口粉面不错，尤其是谐音常旺的肠旺面，肠与血配上油炸小肉丁别有风味，深得我心。到隔壁叫了一碗玫瑰绿豆冰粉，做冰粉的小姐姐和我攀谈，在我将别之时推荐了一些好吃好玩，可以说非常遗憾了。 这两天接触的东西太多太杂，导致我在记忆中筛选得不是特别仔细，加上每天烦恼多多，操心多多，倍感时间飞逝，天色已晚，不再赘述了。 （完） 9月15日整理时记：旅行的最后一天没有记述，行程紧张3点才到家。其实白天也看了不少风景。旱溶洞和水溶洞，还有多彩贵州城，都挺有见闻，可惜淡忘了。触景生情，朋友说只有恰当的时机和恰当的人才能碰撞出无与伦比的绝美感情，我想确实是这样的。至少现在的意义和旅游时的意义全然不同了。我曾经是个猛男，现在又一次变回了二五仔，哈哈。]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python联萌|pandas（国宝库]]></title>
    <url>%2F2020%2F02%2F06%2F200206pandaslearning%2F</url>
    <content type="text"><![CDATA[最后编辑于：2020.02.06 18:00 Pandas库Pandas是基于NumPy 的一种工具，其出现是为了解决数据分析任务。（氦核：个人觉得更像是探索工具，没有模型，简单分析。）Pandas吸纳了大量库和一些标准的数据模型，提供了高效操作大型数据集所需的工具。Pandas中的函数和方法能够使我们快速便捷地处理数据。它是使Python成为强大而高效的数据分析环境的重要因素之一。 http://pandas.pydata.org/pandas-docs/stable/api.html 本文参考万旷网教程。 123# 首先导入pandas库import numpy as npimport pandas as pd numpy详见上一篇文章。 一、序列Series序列Series是一个一维数组结构，可以存入任一种Python数据类型(integers, strings, floating point numbers, Python objects, 等等) 序列Series由两部分构成，一个是index，另一个是对应的值，注意两者的长度必须一样。序列Series和数组array很类似，大多数numpy的函数都可以直接应用于序列Series 序列Series也像一个固定大小的字典dict，可以通过index来赋值或者取值 1234print('通过数组来生成序列Series')s_array = np.random.randn(5)s = pd.Series(s_array, index = ['a','b','c','d','e'])s 通过数组来生成序列Series a -0.298058 b -1.095748 c 1.333607 d 1.119917 e 1.595123 dtype: float64 1234print('通过字典来生成序列Series')s_dict= &#123;'a':11,'b':1000,'c':123213,'d':-1000&#125;s = pd.Series(s_dict)s 通过字典来生成序列Series a 11 b 1000 c 123213 d -1000 dtype: int64 我们取一段金融时间序列给大家做更具体的分析： 12345from WindPy import *w.start()list1 = w.wsd("000001.SZ", "close", "2018-06-28", "2018-07-11", "")list1 Welcome to use Wind Quant API for Python (WindPy)! COPYRIGHT (C) 2017 WIND INFORMATION CO., LTD. ALL RIGHTS RESERVED. IN NO CIRCUMSTANCE SHALL WIND BE RESPONSIBLE FOR ANY DAMAGES OR LOSSES CAUSED BY USING WIND QUANT API FOR Python. .ErrorCode=0 .Codes=[000001.SZ] .Fields=[CLOSE] .Times=[20180628,20180629,20180702,20180703,20180704,20180705,20180706,20180709,20180710,20180711] .Data=[[8.92,9.09,8.61,8.67,8.61,8.6,8.66,9.03,8.98,8.78]] 123#将收盘价转为ss = pd.Series(list1.Data[0], index = list1.Times)ss 2018-06-28 8.92 2018-06-29 9.09 2018-07-02 8.61 2018-07-03 8.67 2018-07-04 8.61 2018-07-05 8.60 2018-07-06 8.66 2018-07-09 9.03 2018-07-10 8.98 2018-07-11 8.78 dtype: float64 1234# 可以通过index来查看序列Series中的元素print('查看序列中index为：',ss.index)print()print('查看序列中index为a的元素：',ss[0]) 查看序列中index为： Index([2018-06-28, 2018-06-29, 2018-07-02, 2018-07-03, 2018-07-04, 2018-07-05, 2018-07-06, 2018-07-09, 2018-07-10, 2018-07-11], dtype=&#39;object&#39;) 查看序列中index为a的元素： 8.92 123456# 基于index 可以修改序列s中的元素print('原序列：\n',ss)print()ss[0] = 11.4print()print('修改后的序列：\n',ss) 原序列： 2018-06-28 8.92 2018-06-29 9.09 2018-07-02 8.61 2018-07-03 8.67 2018-07-04 8.61 2018-07-05 8.60 2018-07-06 8.66 2018-07-09 9.03 2018-07-10 8.98 2018-07-11 8.78 dtype: float64 修改后的序列： 2018-06-28 11.40 2018-06-29 9.09 2018-07-02 8.61 2018-07-03 8.67 2018-07-04 8.61 2018-07-05 8.60 2018-07-06 8.66 2018-07-09 9.03 2018-07-10 8.98 2018-07-11 8.78 dtype: float64 1234567ss1 = pd.Series(np.random.randn(10))print('原序列：\n',ss)print()print('新序列：\n',ss1)print()# 大多数numpy的函数可以直接应用于 序列 Seriesprint('序列相加：\n',pd.Series(ss.values+ss1.values,index=ss.index)) 原序列： 2018-06-28 11.40 2018-06-29 9.09 2018-07-02 8.61 2018-07-03 8.67 2018-07-04 8.61 2018-07-05 8.60 2018-07-06 8.66 2018-07-09 9.03 2018-07-10 8.98 2018-07-11 8.78 dtype: float64 新序列： 0 -0.955700 1 0.391561 2 0.002435 3 -0.131621 4 -0.791321 5 0.841264 6 -0.058034 7 -0.486677 8 0.708875 9 1.841834 dtype: float64 序列相加： 2018-06-28 10.444300 2018-06-29 9.481561 2018-07-02 8.612435 2018-07-03 8.538379 2018-07-04 7.818679 2018-07-05 9.441264 2018-07-06 8.601966 2018-07-09 8.543323 2018-07-10 9.688875 2018-07-11 10.621834 dtype: float64 二、DataFrameDataFrame是一个二维数组结构，可以存入任一种Python数据类型(integers, strings, floating point numbers, Python objects, 等等)。DataFrame由三部分构成，一个是行索引index，一个是列名，另一个则是取值。 2.1 DataFrame的生成123456print('由字典来产生数据框')data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = pd.DataFrame(data)frame 由字典来产生数据框 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 1.7 2 Ohio 2002 3.6 3 Nevada 2001 2.4 4 Nevada 2002 2.9 123456print('由列表来产生数据框')data = [['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], [2000, 2001, 2002, 2001, 2002], [1.5, 1.7, 3.6, 2.4, 2.9]]frame = pd.DataFrame(data,index=['state','year','pop']).Tframe 由列表来产生数据框 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 1.7 2 Ohio 2002 3.6 3 Nevada 2001 2.4 4 Nevada 2002 2.9 2.2 DataFrame的基本性质我们取一段金融时间序列给大家做更具体的分析： 12list2 = w.wsd("000300.SH", "open,high,low,close", "2018-06-28", "2018-07-10", "")list2 .ErrorCode=0 .Codes=[000300.SH] .Fields=[OPEN,HIGH,LOW,CLOSE] .Times=[20180628,20180629,20180702,20180703,20180704,20180705,20180706,20180709,20180710] .Data=[[3434.9441,3431.9619,3504.4571,3410.4767,3398.7788,3365.5547,3347.0624,3378.9056,3464.9064],[3477.0565,3512.3834,3506.8996,3422.0398,3418.3311,3398.4852,3396.2458,3459.3153,3474.1396],[3416.9476,3425.2159,3383.5006,3319.2889,3359.0861,3330.7113,3295.7296,3378.9056,3437.2706],[3423.5255,3510.9845,3407.9638,3409.2801,3363.7473,3342.4379,3365.1227,3459.1837,3467.5155]] 123df = pd.DataFrame(list2.Data,columns=list2.Times,index=list2.Fields)df = df.Tdf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW CLOSE 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 2018-07-10 3464.9064 3474.1396 3437.2706 3467.5155 12345678910print('首先查看数据框的形状',df.shape)print()print('查看数据框的头部：')print(df.head()) print()print('查看数据框的尾部：')print(df.tail())print()print('查看数据框的索引index')print(df.index) 首先查看数据框的形状 (9, 4) 查看数据框的头部： OPEN HIGH LOW CLOSE 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 查看数据框的尾部： OPEN HIGH LOW CLOSE 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 2018-07-10 3464.9064 3474.1396 3437.2706 3467.5155 查看数据框的索引index Index([2018-06-28, 2018-06-29, 2018-07-02, 2018-07-03, 2018-07-04, 2018-07-05, 2018-07-06, 2018-07-09, 2018-07-10], dtype=&#39;object&#39;) 12345678print('查看数据框的列名')print(df.columns)print()print('查看数据框的值，其格式为数组array')print(df.values)print()print('查看数据框的基础描述性统计')print(df.describe()) 查看数据框的列名 Index([&#39;OPEN&#39;, &#39;HIGH&#39;, &#39;LOW&#39;, &#39;CLOSE&#39;], dtype=&#39;object&#39;) 查看数据框的值，其格式为数组array [[3434.9441 3477.0565 3416.9476 3423.5255] [3431.9619 3512.3834 3425.2159 3510.9845] [3504.4571 3506.8996 3383.5006 3407.9638] [3410.4767 3422.0398 3319.2889 3409.2801] [3398.7788 3418.3311 3359.0861 3363.7473] [3365.5547 3398.4852 3330.7113 3342.4379] [3347.0624 3396.2458 3295.7296 3365.1227] [3378.9056 3459.3153 3378.9056 3459.1837] [3464.9064 3474.1396 3437.2706 3467.5155]] 查看数据框的基础描述性统计 OPEN HIGH LOW CLOSE count 9.000000 9.000000 9.000000 9.000000 mean 3415.227522 3451.655144 3371.850689 3416.640111 std 49.780741 44.488942 49.698315 55.264867 min 3347.062400 3396.245800 3295.729600 3342.437900 25% 3378.905600 3418.331100 3330.711300 3365.122700 50% 3410.476700 3459.315300 3378.905600 3409.280100 75% 3434.944100 3477.056500 3416.947600 3459.183700 max 3504.457100 3512.383400 3437.270600 3510.984500 123# 在原有的数据框中新加入一列df['名称'] = ['HS300'] * len(df)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW CLOSE 名称 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 HS300 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 HS300 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 HS300 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 HS300 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 HS300 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 HS300 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 HS300 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 HS300 2018-07-10 3464.9064 3474.1396 3437.2706 3467.5155 HS300 12# 数据框的转置df.T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018-06-28 2018-06-29 2018-07-02 2018-07-03 2018-07-04 2018-07-05 2018-07-06 2018-07-09 2018-07-10 OPEN 3434.94 3431.96 3504.46 3410.48 3398.78 3365.55 3347.06 3378.91 3464.91 HIGH 3477.06 3512.38 3506.9 3422.04 3418.33 3398.49 3396.25 3459.32 3474.14 LOW 3416.95 3425.22 3383.5 3319.29 3359.09 3330.71 3295.73 3378.91 3437.27 CLOSE 3423.53 3510.98 3407.96 3409.28 3363.75 3342.44 3365.12 3459.18 3467.52 名称 HS300 HS300 HS300 HS300 HS300 HS300 HS300 HS300 HS300 2.3 DataFrame截取2.3.1 行截取氦核：不推荐使用ix进行截取，因为ix既可以对名称截取，又可以索引截取。如果index是整数，会很迷惑。一般使用loc和iloc函数。 12345print('查看df索引为1的行——方法一')print(df.ix[1]) # print(df.iloc[1]) 推荐使用print()print('查看df前3行')print(df[:3]) 查看df索引为1的行——方法一 OPEN 3431.96 HIGH 3512.38 LOW 3425.22 CLOSE 3510.98 名称 HS300 Name: 2018-06-29, dtype: object 查看df前3行 OPEN HIGH LOW CLOSE 名称 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 HS300 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 HS300 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 HS300 D:\anaconda\lib\site-packages\ipykernel_launcher.py:2: FutureWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated 2.3.2 列截取12345print('df的一列选取')print(df['OPEN'])print()print('df的两列同时选取')print(df[['OPEN','LOW']]) df的一列选取 2018-06-28 3434.9441 2018-06-29 3431.9619 2018-07-02 3504.4571 2018-07-03 3410.4767 2018-07-04 3398.7788 2018-07-05 3365.5547 2018-07-06 3347.0624 2018-07-09 3378.9056 2018-07-10 3464.9064 Name: OPEN, dtype: float64 df的两列同时选取 OPEN LOW 2018-06-28 3434.9441 3416.9476 2018-06-29 3431.9619 3425.2159 2018-07-02 3504.4571 3383.5006 2018-07-03 3410.4767 3319.2889 2018-07-04 3398.7788 3359.0861 2018-07-05 3365.5547 3330.7113 2018-07-06 3347.0624 3295.7296 2018-07-09 3378.9056 3378.9056 2018-07-10 3464.9064 3437.2706 2.3.3 DataFrame行列同时截取12print('截取df的前4行的close和low列')df.ix[:4,['CLOSE','LOW']] 截取df的前4行的close和low列 D:\anaconda\lib\site-packages\ipykernel_launcher.py:2: FutureWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated ​ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CLOSE LOW 2018-06-28 3423.5255 3416.9476 2018-06-29 3510.9845 3425.2159 2018-07-02 3407.9638 3383.5006 2018-07-03 3409.2801 3319.2889 2.3.4 DataFrame条件截取123456print('截取df CLOSE大于等于3500的记录')print(df[df['CLOSE']&gt;=3500]) print('')print('截取df CLOSE大于3300且LOW小于3400的记录')print(df[(df['CLOSE']&gt;3300)&amp;(df['LOW']&lt;3400)]) print('') 截取df CLOSE大于等于3500的记录 OPEN HIGH LOW CLOSE 名称 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 HS300 截取df CLOSE大于3300且LOW小于3400的记录 OPEN HIGH LOW CLOSE 名称 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 HS300 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 HS300 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 HS300 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 HS300 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 HS300 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 HS300 ​ 2.4 DataFrame缺失值处理例如下面这个数据框data，其中就存在缺失值 1234567data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;data = pd.DataFrame(data)data.loc[1,'pop'] = np.NaNdata.loc[3,'state'] = Nonedata .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 NaN 2 Ohio 2002 3.6 3 None 2001 2.4 4 Nevada 2002 2.9 12#删除含有缺失的行data.dropna() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 2 Ohio 2002 3.6 4 Nevada 2002 2.9 12#表示该行都为缺失的行才删除 注意是这一行中的每一个元素都为缺失才删除这一行data.dropna(how="all") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 NaN 2 Ohio 2002 3.6 3 None 2001 2.4 4 Nevada 2002 2.9 12#表示该列若都为缺失的列则删除,注意是这一列的每个元素都为缺失才会删除这一列data.dropna(how="all", axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 NaN 2 Ohio 2002 3.6 3 None 2001 2.4 4 Nevada 2002 2.9 12#表示保留至少存在3个非NaN的行，即如果某一行的非缺失值个数小于3个，则会被删除data.dropna(thresh=3, axis=0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 2 Ohio 2002 3.6 4 Nevada 2002 2.9 12#表示保留至少存在3个非NaN的列，即如果某一列的非缺失值个数小于3个，则会被删除data.dropna(thresh=3, axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 NaN 2 Ohio 2002 3.6 3 None 2001 2.4 4 Nevada 2002 2.9 1data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 NaN 2 Ohio 2002 3.6 3 None 2001 2.4 4 Nevada 2002 2.9 12print('用0填充数据框中的缺失值,0是可选参数之一')data.fillna(value=0) 用0填充数据框中的缺失值,0是可选参数之一 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 0.0 2 Ohio 2002 3.6 3 0 2001 2.4 4 Nevada 2002 2.9 12#填充缺失值 用缺失值所在列的前一个非NaN值来进行填充 data.fillna(method='ffill') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 1.5 2 Ohio 2002 3.6 3 Ohio 2001 2.4 4 Nevada 2002 2.9 12#用缺失值所在列的后一个非NaN来填充data.fillna(method="bfill") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year pop 0 Ohio 2000 1.5 1 Ohio 2001 3.6 2 Ohio 2002 3.6 3 Nevada 2001 2.4 4 Nevada 2002 2.9 氦核：这些填补都是什么鬼方法，无语。 2.5 DataFrame排序1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW CLOSE 名称 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 HS300 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 HS300 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 HS300 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 HS300 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 HS300 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 HS300 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 HS300 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 HS300 2018-07-10 3464.9064 3474.1396 3437.2706 3467.5155 HS300 12print('df按列OPEN降序排序')df.sort_values('OPEN') df按列OPEN降序排序 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW CLOSE 名称 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 HS300 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 HS300 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 HS300 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 HS300 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 HS300 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 HS300 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 HS300 2018-07-10 3464.9064 3474.1396 3437.2706 3467.5155 HS300 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 HS300 12print('df按列LOW升序排序')df.sort_values('LOW',ascending=True) df按列LOW升序排序 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW CLOSE 名称 2018-07-06 3347.0624 3396.2458 3295.7296 3365.1227 HS300 2018-07-03 3410.4767 3422.0398 3319.2889 3409.2801 HS300 2018-07-05 3365.5547 3398.4852 3330.7113 3342.4379 HS300 2018-07-04 3398.7788 3418.3311 3359.0861 3363.7473 HS300 2018-07-09 3378.9056 3459.3153 3378.9056 3459.1837 HS300 2018-07-02 3504.4571 3506.8996 3383.5006 3407.9638 HS300 2018-06-28 3434.9441 3477.0565 3416.9476 3423.5255 HS300 2018-06-29 3431.9619 3512.3834 3425.2159 3510.9845 HS300 2018-07-10 3464.9064 3474.1396 3437.2706 3467.5155 HS300 2.6 DataFrame的基本函数12print('按列求均值')df.mean() 按列求均值 OPEN 3415.227522 HIGH 3451.655144 LOW 3371.850689 CLOSE 3416.640111 dtype: float64 12print('按行求均值')df.mean(axis=1) 按行求均值 2018-06-28 3438.118425 2018-06-29 3470.136425 2018-07-02 3450.705275 2018-07-03 3390.271375 2018-07-04 3384.985825 2018-07-05 3359.297275 2018-07-06 3351.040125 2018-07-09 3419.077550 2018-07-10 3460.958025 dtype: float64 函数汇总下面的函数都是通过数据框.函数名(参数设置)来进行调用，一般的参数是axis=0/1，选择为0则是按行来实现函数，1则是按列来实现函数 序号 函数 函数含义 1 count 计数非na值 2 describe 针对Series或个DataFrame列基本描述统计 3 min、max 计算最小值和最大值 4 argmin、argmax 获取到最大值和最小值的索引位置（整数） 5 idxmin、idxmax 计算能够获取到最大值和最小值得索引值 6 quantile 计算样本的分位数（0到1） 7 sum 求和 8 mean 求平均数 9 median 求中位数（50%分位数） 10 mad 计算平均绝对离差 11 var 样本方差 12 std 样本标准差 13 skew 样本偏度（三阶矩） 14 kurt 样本峰度（四阶矩） 15 cumsum 样本累计和 16 cummin，cummax 样本累计最大值和累计最小值 17 cumprod 样本累计积 18 diff 计算一阶差分 19 pct_change 计算百分数变化 20 corr 计数相关性 2.7 DataFrame拼接下面介绍了三个函数来实现 DataFrame的拼接功能——concat函数，merge函数和join函数 2.7.1 DataFrame拼接—pd.concat通过Wind API可以获取到各种金融数据，可以使用代码生成器生成获取数据的函数代码。获取到数据后，可参考一下代码将数据转换为DataFrame格式 12345678from WindPy import *from pandas import DataFrame w.start()wsd_data = w.wsd("000001.SZ", "lastradeday_s,sec_name,open,high,low,close", "2017-11-01", "2017-11-05", "")data_df = DataFrame(wsd_data.Data,columns=wsd_data.Times,index=wsd_data.Fields)data_df = data_df.T #转置数据表data_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME OPEN HIGH LOW CLOSE 2017-11-01 2017-11-01 平安银行 11.56 11.59 11.32 11.4 2017-11-02 2017-11-02 平安银行 11.36 11.58 11.26 11.54 2017-11-03 2017-11-03 平安银行 11.49 11.68 11.35 11.39 1234wsd_data1 = w.wsd("000002.SZ", "lastradeday_s,sec_name,open,high,low,close", "2017-11-01", "2017-11-05", "")data_df1 = DataFrame(wsd_data1.Data,columns=wsd_data.Times,index=wsd_data.Fields)data_df1 = data_df1.T #转置数据表data_df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME OPEN HIGH LOW CLOSE 2017-11-01 2017-11-01 万科A 28.96 30.54 28.73 29.15 2017-11-02 2017-11-02 万科A 29.3 29.48 28.68 29.45 2017-11-03 2017-11-03 万科A 29.23 29.52 28.05 28.19 12print('按行拼接')pd.concat([data_df,data_df1],axis=0) 按行拼接 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME OPEN HIGH LOW CLOSE 2017-11-01 2017-11-01 平安银行 11.56 11.59 11.32 11.4 2017-11-02 2017-11-02 平安银行 11.36 11.58 11.26 11.54 2017-11-03 2017-11-03 平安银行 11.49 11.68 11.35 11.39 2017-11-01 2017-11-01 万科A 28.96 30.54 28.73 29.15 2017-11-02 2017-11-02 万科A 29.3 29.48 28.68 29.45 2017-11-03 2017-11-03 万科A 29.23 29.52 28.05 28.19 12print('按列拼接')pd.concat([data_df,data_df1],axis=1) 按列拼接 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME OPEN HIGH LOW CLOSE LASTRADEDAY_S SEC_NAME OPEN HIGH LOW CLOSE 2017-11-01 2017-11-01 平安银行 11.56 11.59 11.32 11.4 2017-11-01 万科A 28.96 30.54 28.73 29.15 2017-11-02 2017-11-02 平安银行 11.36 11.58 11.26 11.54 2017-11-02 万科A 29.3 29.48 28.68 29.45 2017-11-03 2017-11-03 平安银行 11.49 11.68 11.35 11.39 2017-11-03 万科A 29.23 29.52 28.05 28.19 2.7.3 DataFrame拼接—pd.mergepd.merge一般针对的是按列合并。 pd.merge(left, right, how=’inner’, on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=(‘_x’, ‘_y’), copy=True, indicator=False) left: 一个dataframe对象 right: 另一个dataframe对象 how: 可以是’left’, ‘right’, ‘outer’, ‘inner’. 默认为inner。 on: 列名，两个dataframe都有的列。如果不传参数，而且left_index和right_index也等于False，则默认把两者交叉/共有的列作为链接键（join keys）。可以是一个列名，也可以是包含多个列名的list。 left_on: 左边dataframe的列会用做keys。可以是列名，或者与dataframe长度相同的矩阵array。 right_on: 右边同上。 left_index: 如果为Ture，用左侧dataframe的index作为连接键。如果是多维索引，level数要跟右边相同才行。 right_index: 右边同上。 sort: 对合并后的数据框排序，以连接键。 suffixes: 一个tuple，包字符串后缀，用来加在重叠的列名后面。默认是(‘_x’,’_y’)。 copy: 默认Ture，复制数据。 indicator: 布尔型（True/FALSE），或是字符串。如果为True，合并之后会增加一列叫做_merge。是分类数据，用left_only, right_only, both来标记来自左边，右边和两边的数据。 参考：http://www.jianshu.com/p/dc8ba1c0eada 希望将上面两个 DataFrame left_data和right_data拼接起来，但要求是按照时间来进行拼接 12print('按LASTRADEDAY_S拼接，只保留共同的部分')pd.merge(data_df,data_df1,on='LASTRADEDAY_S') 按LASTRADEDAY_S拼接，只保留共同的部分 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME_x OPEN_x HIGH_x LOW_x CLOSE_x SEC_NAME_y OPEN_y HIGH_y LOW_y CLOSE_y 0 2017-11-01 平安银行 11.56 11.59 11.32 11.4 万科A 28.96 30.54 28.73 29.15 1 2017-11-02 平安银行 11.36 11.58 11.26 11.54 万科A 29.3 29.48 28.68 29.45 2 2017-11-03 平安银行 11.49 11.68 11.35 11.39 万科A 29.23 29.52 28.05 28.19 12print('按LASTRADEDAY_S拼接，但所有的数据都保留下来')pd.merge(data_df,data_df1,on='LASTRADEDAY_S',how='outer') 按LASTRADEDAY_S拼接，但所有的数据都保留下来 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME_x OPEN_x HIGH_x LOW_x CLOSE_x SEC_NAME_y OPEN_y HIGH_y LOW_y CLOSE_y 0 2017-11-01 平安银行 11.56 11.59 11.32 11.4 万科A 28.96 30.54 28.73 29.15 1 2017-11-02 平安银行 11.36 11.58 11.26 11.54 万科A 29.3 29.48 28.68 29.45 2 2017-11-03 平安银行 11.49 11.68 11.35 11.39 万科A 29.23 29.52 28.05 28.19 12print('LASTRADEDAY_S，但所有的数据都保留下来，且生成一列来表示数据的来源')pd.merge(data_df,data_df1,on='LASTRADEDAY_S',how='outer',indicator='数据来源') LASTRADEDAY_S，但所有的数据都保留下来，且生成一列来表示数据的来源 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S SEC_NAME_x OPEN_x HIGH_x LOW_x CLOSE_x SEC_NAME_y OPEN_y HIGH_y LOW_y CLOSE_y 数据来源 0 2017-11-01 平安银行 11.56 11.59 11.32 11.4 万科A 28.96 30.54 28.73 29.15 both 1 2017-11-02 平安银行 11.36 11.58 11.26 11.54 万科A 29.3 29.48 28.68 29.45 both 2 2017-11-03 平安银行 11.49 11.68 11.35 11.39 万科A 29.23 29.52 28.05 28.19 both 2.7.4 DataFrame拼接—.joinDataFrame.join(other, on=None, how=’left’, lsuffix=’’, rsuffix=’’, sort=False) other：一个DataFrame、Series（要有命名），或者DataFrame组成的list。 on：列名，包含列名的list或tuple，或矩阵样子的列 （如果是多列，必须有MultiIndex）。 跟上面的几种方法一样，用来指明依据哪一列进行合并。 如果没有赋值，则依据两个数据框的index合并。 how：合并方式， {‘left’, ‘right’, ‘outer’, ‘inner’}, 默认‘left‘。 lsuffix：字符串。用于左侧数据框的重复列。 把重复列重新命名，原来的列名+字符串。 【如果有重复列，必须添加这个参数。】 rsuffix：同上。右侧。 sort：布尔型，默认False。如果为True，将链接键（on的那列）按字母排序。 参考：http://www.jianshu.com/p/dc8ba1c0eada 12print('注意到两个拼接的数据框，含有相同的列LASTRADEDAY_S，故重新命名了这两个列')data_df.join(data_df1,lsuffix='_left',rsuffix='_right') 注意到两个拼接的数据框，含有相同的列LASTRADEDAY_S，故重新命名了这两个列 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LASTRADEDAY_S_left SEC_NAME_left OPEN_left HIGH_left LOW_left CLOSE_left LASTRADEDAY_S_right SEC_NAME_right OPEN_right HIGH_right LOW_right CLOSE_right 2017-11-01 2017-11-01 平安银行 11.56 11.59 11.32 11.4 2017-11-01 万科A 28.96 30.54 28.73 29.15 2017-11-02 2017-11-02 平安银行 11.36 11.58 11.26 11.54 2017-11-02 万科A 29.3 29.48 28.68 29.45 2017-11-03 2017-11-03 平安银行 11.49 11.68 11.35 11.39 2017-11-03 万科A 29.23 29.52 28.05 28.19 2.8 DataFrame重复值剔除有时候，希望能够剔除掉DataFrame中的重复记录。 123# 每股基本收益指标error_code,df3 = w.wsd("000001.SZ", "fa_eps_basic", "2017-08-07", "2017-08-15", "Days=Alldays;currencyType=",usedf=True)df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FA_EPS_BASIC 2017-08-07 0.31 2017-08-08 0.31 2017-08-09 0.31 2017-08-10 0.31 2017-08-11 0.68 2017-08-12 0.68 2017-08-13 0.68 2017-08-14 0.68 2017-08-15 0.68 12print('查看DataFrame中是否存在重复记录，标记为True的为重复记录')df3.duplicated() 查看DataFrame中是否存在重复记录，标记为True的为重复记录 2017-08-07 False 2017-08-08 True 2017-08-09 True 2017-08-10 True 2017-08-11 False 2017-08-12 True 2017-08-13 True 2017-08-14 True 2017-08-15 True dtype: bool 12print('剔除数据框中的重复记录')df3.drop_duplicates() 剔除数据框中的重复记录 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } FA_EPS_BASIC 2017-08-07 0.31 2017-08-11 0.68 2.9 DataFrame分组及透视表（groupby）2.9.1 分组——groupby函数氦核：这个函数很厉害。下面列几个用法： 1.根据DataFrame本身的某一列或多列内容进行分组聚合。 2.还可以利用for循环，对分组进行迭代。（下面举了个小栗子） for name,group in df.groupby(&#39;key1&#39;): print(name) print(group) 若仅使用一个变量name,会影响输出结果的索引层次表达方式，且结果为元组。 3.对聚合后的数据片段，进行格式类型转化 4.利用groupby，根据dtypes对列进行分组,此时，需指定axis=1，否则，groupby默认根据axis=0进行分组，而行数据由于类型不统一，故无法根据dtypes对列进行分组。 *#将聚合后df转化为字典格式，后根据df的数据类型对列进行分组* grouped=df.groupby(df.dtypes,axis=1) dict(list(grouped)) 我们设定，如果当天收益率大于0，我们标记为up，反之为down。把收盘价大于11的标记为good，反之为bad。正式举例。 12345678from WindPy import *w.start()# 先取一个金融时间序列，以DataFrame的形式error_code,df4 = w.wsd("000001.SZ", "open,close,pct_chg", "2018-04-20", "2018-04-30", "",usedf=True)df4['standard'] = df4.PCT_CHG.apply(lambda x: 'up' if x &gt; 0 else 'down')df4['expression'] = df4.CLOSE.apply(lambda x: 'good' if x &gt; 11 else 'bad')df4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN CLOSE PCT_CHG standard expression 2018-04-20 11.51 11.35 -1.046207 down good 2018-04-23 11.30 11.57 1.938326 up good 2018-04-24 11.63 11.86 2.506482 up good 2018-04-25 11.76 11.68 -1.517707 down good 2018-04-26 11.66 11.42 -2.226027 down good 2018-04-27 11.49 10.85 -4.991243 down bad 12grouped = df4['CLOSE'].groupby(df4['standard'])grouped &lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x0000020B26C63648&gt; 氦核：上面这行是聚合后不适用配合函数的输出。 这是由于变量grouped是一个GroupBy对象，它实际上还没有进行任何计算，只是含有一些有关分组键df[‘key1’]的中间数据而已，然后我们可以调用配合函数（如：.mean()方法）来计算分组平均值等。 因此，一般为方便起见可直接在聚合之后+“配合函数”，默认情况下，所有数值列都将会被聚合，虽然有时可能会被过滤为一个子集。 一般，如果对df直接聚合时，df.groupby([df[&#39;key1&#39;],df[&#39;key2&#39;]]).mean()（分组键为：Series）与df.groupby([&#39;key1&#39;,&#39;key2&#39;]).mean()（分组键为：列名）是等价的，输出结果相同。 但是，如果对df的指定列进行聚合时，df[&#39;data1&#39;].groupby(df[&#39;key1&#39;]).mean()（分组键为：Series），唯一方式。此时，直接使用“列名”作分组键，提示“Error Key”。 注意：分组键中的任何缺失值都会被排除在结果之外。 参考groupby用法博客链接 1grouped.mean() standard down 11.325 up 11.715 Name: CLOSE, dtype: float64 12means = df4['CLOSE'].groupby([df4['standard'],df4['expression']]).mean()means standard expression down bad 10.850000 good 11.483333 up good 11.715000 Name: CLOSE, dtype: float64 对两个键进行了分组，得到的Series具有一个层次化索引。 1means.unstack() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } expression bad good standard down 10.85 11.483333 up NaN 11.715000 你还可以将列名用作分组键。 1df4.groupby('standard').mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN CLOSE PCT_CHG standard down 11.605 11.325 -2.445296 up 11.465 11.715 2.222404 1df4.groupby([df4['standard'],df4['expression']]).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN CLOSE PCT_CHG standard expression down bad 11.490000 10.850000 -4.991243 good 11.643333 11.483333 -1.596647 up good 11.465000 11.715000 2.222404 我们还可以用size方法，返回一个含有分组大小的Series。 1df4.groupby(['standard','expression']).size() standard expression down bad 1 good 3 up good 2 dtype: int64 2.9.2 对分组进行迭代GroupBy对象支持迭代，可以产生一组二元元组。 123for name,group in df4.groupby('standard'): print(name) print(group) down OPEN CLOSE PCT_CHG standard expression 2018-04-20 11.51 11.35 -1.046207 down good 2018-04-25 11.76 11.68 -1.517707 down good 2018-04-26 11.66 11.42 -2.226027 down good 2018-04-27 11.49 10.85 -4.991243 down bad up OPEN CLOSE PCT_CHG standard expression 2018-04-23 11.30 11.57 1.938326 up good 2018-04-24 11.63 11.86 2.506482 up good 对于多重组件的情况，元素的第一个元素将会是有键值组成的元组： 123for (k1,k2), group in df4.groupby(['standard','expression']): print(k1,k2) print(group) down bad OPEN CLOSE PCT_CHG standard expression 2018-04-27 11.49 10.85 -4.991243 down bad down good OPEN CLOSE PCT_CHG standard expression 2018-04-20 11.51 11.35 -1.046207 down good 2018-04-25 11.76 11.68 -1.517707 down good 2018-04-26 11.66 11.42 -2.226027 down good up good OPEN CLOSE PCT_CHG standard expression 2018-04-23 11.30 11.57 1.938326 up good 2018-04-24 11.63 11.86 2.506482 up good 当然，你可以对这些数据片段做任何操作。将这些数据片段做成一个字典： 12pieces = dict(list(df4.groupby('standard')))pieces {&#39;down&#39;: OPEN CLOSE PCT_CHG standard expression 2018-04-20 11.51 11.35 -1.046207 down good 2018-04-25 11.76 11.68 -1.517707 down good 2018-04-26 11.66 11.42 -2.226027 down good 2018-04-27 11.49 10.85 -4.991243 down bad, &#39;up&#39;: OPEN CLOSE PCT_CHG standard expression 2018-04-23 11.30 11.57 1.938326 up good 2018-04-24 11.63 11.86 2.506482 up good} groupby默认是在axis=0上进行分组的。通过设置也可以对其他任何轴上进行分组。比如我们可以根据dtype对列进行分组： 1df4.dtypes OPEN float64 CLOSE float64 PCT_CHG float64 standard object expression object dtype: object 12grouped = df4.groupby(df4.dtypes,axis=1)dict(list(grouped)) {dtype(&#39;float64&#39;): OPEN CLOSE PCT_CHG 2018-04-20 11.51 11.35 -1.046207 2018-04-23 11.30 11.57 1.938326 2018-04-24 11.63 11.86 2.506482 2018-04-25 11.76 11.68 -1.517707 2018-04-26 11.66 11.42 -2.226027 2018-04-27 11.49 10.85 -4.991243, dtype(&#39;O&#39;): standard expression 2018-04-20 down good 2018-04-23 up good 2018-04-24 up good 2018-04-25 down good 2018-04-26 down good 2018-04-27 down bad} 2.9.3 选取一个或一组列1df4.groupby(['standard','expression'])[['CLOSE']].mean() # 在['CLOSE']前后再多加一组[]即可 氦核：再加一个中括号。 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CLOSE standard expression down bad 10.850000 good 11.483333 up good 11.715000 或者是以分组的Series: 12s_grouped = df4.groupby(['standard','expression'])['CLOSE']s_grouped.mean() standard expression down bad 10.850000 good 11.483333 up good 11.715000 Name: CLOSE, dtype: float64 2.9.4 通过字典或者Series进行分组除数组以外，分组信息还可以以其他形式存在。我们新构建一个DataFrame。 12error_code,df_wss = w.wss("000001.SZ,000088.SZ,002626.SZ,600021.SH,600036.SH", "open,high,low,volume,amt,pct_chg", "tradeDate=2018-05-29;priceAdj=1;cycle=1",usedf=True)df_wss .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW VOLUME AMT PCT_CHG 000001.SZ 10.58 10.63 10.35 88949497.0 9.303870e+08 -1.983003 000088.SZ 7.65 7.79 7.61 9731389.0 7.493758e+07 -0.260756 002626.SZ 19.13 19.72 18.97 7058799.0 1.367769e+08 -0.679561 600021.SH 8.15 8.20 8.10 2065681.0 1.685831e+07 -0.368098 600036.SH 28.65 28.98 28.41 48053415.0 1.376605e+09 0.486111 123# 添加几个空值NANdf_wss.ix[2:3, ['OPEN','HIGH']] = np.nandf_wss D:\anaconda\lib\site-packages\ipykernel_launcher.py:2: FutureWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated ​ 氦核：依然不建议使用ix。 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW VOLUME AMT PCT_CHG 000001.SZ 10.58 10.63 10.35 88949497.0 9.303870e+08 -1.983003 000088.SZ 7.65 7.79 7.61 9731389.0 7.493758e+07 -0.260756 002626.SZ NaN NaN 18.97 7058799.0 1.367769e+08 -0.679561 600021.SH 8.15 8.20 8.10 2065681.0 1.685831e+07 -0.368098 600036.SH 28.65 28.98 28.41 48053415.0 1.376605e+09 0.486111 假设已知列的分组关系，并希望根据分组计算列的总和： 1mapping = &#123;'OPEN':'r','HIGH':'r','LOW':'b','VOLUME':'b','AMT':'b','PCT_CHG':'o'&#125; 只需要将这个字典传给groupby即可：（行方向） 12by_colum = df_wss.groupby(mapping,axis=1)by_colum.sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } b o r 000001.SZ 1.019336e+09 -1.983003 21.21 000088.SZ 8.466897e+07 -0.260756 15.44 002626.SZ 1.438358e+08 -0.679561 0.00 600021.SH 1.892400e+07 -0.368098 16.35 600036.SH 1.424658e+09 0.486111 57.63 Series也有这样的功能。 12map_series = pd.Series(mapping)map_series OPEN r HIGH r LOW b VOLUME b AMT b PCT_CHG o dtype: object 1df_wss.groupby(map_series,axis=1).count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } b o r 000001.SZ 3 1 2 000088.SZ 3 1 2 002626.SZ 3 1 0 600021.SH 3 1 2 600036.SH 3 1 2 2.9.5 根据索引级别分组层次化索引数据最方便的地方就是在于它能够根据索引级别进行聚合。要实现该目的，通过level关键字传入级别编号或名称即可： 1df_wss .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } OPEN HIGH LOW VOLUME AMT PCT_CHG 000001.SZ 10.58 10.63 10.35 88949497.0 9.303870e+08 -1.983003 000088.SZ 7.65 7.79 7.61 9731389.0 7.493758e+07 -0.260756 002626.SZ NaN NaN 18.97 7058799.0 1.367769e+08 -0.679561 600021.SH 8.15 8.20 8.10 2065681.0 1.685831e+07 -0.368098 600036.SH 28.65 28.98 28.41 48053415.0 1.376605e+09 0.486111 1234columns = pd.MultiIndex.from_arrays([['行情','行情','行情','量','量','幅度'], ['OPEN','HIGH','LOW','VOLUME','AMT','PCT_CHG']],names=['s1','s2'])hier_df = pd.DataFrame(df_wss.values, columns=columns,index=df_wss.index)hier_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } s1 行情 量 幅度 s2 OPEN HIGH LOW VOLUME AMT PCT_CHG 000001.SZ 10.58 10.63 10.35 88949497.0 9.303870e+08 -1.983003 000088.SZ 7.65 7.79 7.61 9731389.0 7.493758e+07 -0.260756 002626.SZ NaN NaN 18.97 7058799.0 1.367769e+08 -0.679561 600021.SH 8.15 8.20 8.10 2065681.0 1.685831e+07 -0.368098 600036.SH 28.65 28.98 28.41 48053415.0 1.376605e+09 0.486111 1hier_df.groupby(level='s1',axis=1).count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s1 幅度 行情 量 000001.SZ 1 3 2 000088.SZ 1 3 2 002626.SZ 1 1 2 600021.SH 1 3 2 600036.SH 1 3 2 （完） 依然鸣谢：某大哥假粉。愿四下空虚的灵魂皆能得以慰藉。]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python联萌|今天康康numpy（怒骂朋友库]]></title>
    <url>%2F2020%2F02%2F05%2F200205numpy%2F</url>
    <content type="text"><![CDATA[最后编辑于：2020.02.05 12:30 Numpy库Numpy库是Python的一种开源的数值计算扩展。 Numpy可用来存储和处理大型矩阵，比Python自身的嵌套列表结构要高效很多。 据说Numpy将Pyhon变成了一种免费的更强大的Matlab系统。 本文介绍性文字转载自万旷网。氦核感觉notebook形式更适合学习，有机会把丘比特文件给大家附上。 Numpy库包含了： &gt; 强大的N维数组对象精密的函数连接C/C++和Fortran代码的工具常用的线性代数，傅里叶变换和随机数生成 12#首先导入 numpy 库import numpy as np 氦核：惯用的导入法，最好延用。 一、数组array数组array和列表list类似，但是数据array可以定义维度，且适合做数学代数运算 1.数组array生成数据使用windapi提取。 12345from WindPy import *w.start()a = w.wsd("000001.SZ", "close", "2018-07-05", "2018-07-11", "")b = w.wsd("000002.SZ", "close,open", "2018-07-05", "2018-07-11", "")b 氦核：取的是收盘价和开盘价。 .ErrorCode=0 .Codes=[000002.SZ] .Fields=[CLOSE,OPEN] .Times=[20180705,20180706,20180709,20180710,20180711] .Data=[[23.05,23.21,24.01,24.15,23.46],[23.02,23.34,23.37,24.2,23.48]] 123456789a1 = np.array(a.Data[0])a2 = np.array(b.Data)print('这是一个一维数组：\n',a1)print()print('这是一个二维数组：\n',a2)print()print('查看a1的长度：\n',len(a1))print()print('查看a2的长度：\n',len(a2)) 氦核：len可以求数组长度，指数组里有几个list。 这是一个一维数组： [8.6 8.66 9.03 8.98 8.78] 这是一个二维数组： [[23.05 23.21 24.01 24.15 23.46] [23.02 23.34 23.37 24.2 23.48]] 查看a1的长度： 5 查看a2的长度： 2 1a2 array([[23.05, 23.21, 24.01, 24.15, 23.46], [23.02, 23.34, 23.37, 24.2 , 23.48]]) 1a2[0,1] 23.21 1.2 数组array性质1234'''数组元素整数转化为浮点数'''print('数组类型：',a1.dtype)float_arr = a1.astype(np.int)print('改变数组类型后：',float_arr.dtype) 氦核：dtype可以查看类型，astype可以转换类型。不同类型变换后会产生不同结果。 数组类型： float64 改变数组类型后： int32 1234'''字符串数字转化为浮点数'''numeric_string = np.array(['1.11','2.22','3.33'])print(numeric_string, numeric_string.dtype)print(numeric_string.astype(np.float),numeric_string.astype(np.float).dtype) [&#39;1.11&#39; &#39;2.22&#39; &#39;3.33&#39;] &lt;U4 [1.11 2.22 3.33] float64 1234'''字符串数字转化为浮点数'''numeric_string = np.array(['1.11','2.22','3.33'])print(numeric_string, numeric_string.dtype)print(numeric_string.astype(np.float),numeric_string.astype(np.float).dtype) [&#39;1.11&#39; &#39;2.22&#39; &#39;3.33&#39;] &lt;U4 [1.11 2.22 3.33] float64 1234567891011print(a2)print()print('第二行第三列元素(第二行索引为1，第三列索引为2)：\n',a2[1,2]) print()print('倒数第一行(注意索引为-1)：\n',a2[-1,:]) print()print('第三列(索引为2)：\n',a2[:,2])print()print('a2形状：\n',a2.shape)print()print('a2形状重构：\n',a2.reshape(5,2)) [[23.05 23.21 24.01 24.15 23.46] [23.02 23.34 23.37 24.2 23.48]] 第二行第三列元素(第二行索引为1，第三列索引为2)： 23.37 倒数第一行(注意索引为-1)： [23.02 23.34 23.37 24.2 23.48] 第三列(索引为2)： [24.01 23.37] a2形状： (2, 5) a2形状重构： [[23.05 23.21] [24.01 24.15] [23.46 23.02] [23.34 23.37] [24.2 23.48]] 12345678910111213print('维度解锁：',a2.ravel()) # ravel()函数可以将高维数组转化为一维数组print()print('按列求和：',a2.sum(axis=0))print()print('按列求积：',a2.prod(axis=0))print()print('全局最大值：',a2.max(),'全局最小值：',a2.min())print()print('按行求最大值：',a2.max(axis=0),'按列求最小值：',a2.min(axis=1))print()print('按列求均值：',a2.mean(axis=0))print()print('按行求标准差：',a2.std(axis=0)) 维度解锁： [23.05 23.21 24.01 24.15 23.46 23.02 23.34 23.37 24.2 23.48] 按列求和： [46.07 46.55 47.38 48.35 46.94] 按列求积： [530.611 541.7214 561.1137 584.43 550.8408] 全局最大值： 24.2 全局最小值： 23.02 按行求最大值： [23.05 23.34 24.01 24.2 23.48] 按列求最小值： [23.05 23.02] 按列求均值： [23.035 23.275 23.69 24.175 23.47 ] 按行求标准差： [0.015 0.065 0.32 0.025 0.01 ] 氦核：这些计算都是可以接受方向的。按列或按行。 1234567print('原矩阵：\n',a2)print()print('按列求和：\n',a2.sum(axis=0))print()print('按行求均值：\n',a2.mean(axis=1))print()print('按行累加：\n',a2.cumsum(axis=1)) 原矩阵： [[23.05 23.21 24.01 24.15 23.46] [23.02 23.34 23.37 24.2 23.48]] 按列求和： [46.07 46.55 47.38 48.35 46.94] 按行求均值： [23.576 23.482] 按行累加： [[ 23.05 46.26 70.27 94.42 117.88] [ 23.02 46.36 69.73 93.93 117.41]] 123print('矩阵所有元素求指数：\n',np.exp(a2))print()print('矩阵所有元素求根号：\n',np.sqrt(a2)) 氦核：np.exp功能是“求e的幂次方”。 矩阵所有元素求指数： [[1.02444302e+10 1.20219502e+10 2.67553422e+10 3.07759692e+10 1.54364896e+10] [9.94166153e+09 1.36909381e+10 1.41078893e+10 3.23538868e+10 1.57483274e+10]] 矩阵所有元素求根号： [[4.80104155 4.81767579 4.9 4.91426495 4.84355242] [4.79791621 4.83114893 4.83425279 4.91934955 4.84561658]] 小数位数控制和取整 1print('小数位数：\n',a1.round(decimals=2)) #控制小数位数 小数位数： [8.6 8.66 9.03 8.98 8.78] 1234567print('原数组：\n',a1)print()print('向上取整：\n',np.floor(a1))print()print('向下取整：\n',np.ceil(a1))print()print('四舍五入(控制小数为2位)：\n',np.round(a1,2)) 原数组： [8.6 8.66 9.03 8.98 8.78] 向上取整： [8. 8. 9. 8. 8.] 向下取整： [ 9. 9. 10. 9. 9.] 四舍五入(控制小数为2位)： [8.6 8.66 9.03 8.98 8.78] 数组——一元函数 函数 说明 abs、fabs 计算整数、浮点数或复数的绝对值。对于非复数，使用fabs更快 sqrt、square、exp 计算各元素的平方根、平方、指数𝑒𝑥 log、log10、log2、log1p 自然对数、底数10的对数、底数2的对数、𝑙𝑛(1+𝑥) sign 计算各元素的正负号：正1,零0,负-1 ceil 计算各元素的取整：大于等于该数的最小整数 floor 计算各元素的取整：小于等于该数的最大整数 rint 各元素四舍五入最接近的整数，dtype不变 modf 将数组各元素的小数和整数部分以两个独立数组的形式返回 isnan、isfinite、isinf 判断各元素是否为NaN、是否有穷、是否为无穷 cos、cosh、sin、sinh、tan、tanh 一般和双曲型的三角函数 arccos、arccosh、arcsin、arcsinh、arctan、arctanh 反三角函数 sum、mean 数组全部或者按某个轴的方向进行求和、求均值 std、var 标准差、方差，自由度可以调整 min、max、argmin、argmax 最小和最大值、最小和最大元素的索引 cumsum、cumprod 数组全部或者按某个轴的方向进行累计和、累计积 1.3 数组array间运算123c, r = np.array([1,2,3,4]), np.array([2,3,4,5])print(c)print(r) [1 2 3 4] [2 3 4 5] 12345print('数组相加：',c + r)print('数组相乘：',c * r)print('数组乘方：',c **r)print('数组判断：',c &gt;= 2)print('向量内积：',c.dot(r.T)) 数组相加： [3 5 7 9] 数组相乘： [ 2 6 12 20] 数组乘方： [ 1 8 81 1024] 数组判断： [False True True True] 向量内积： 40 12print('取两个数组中的较大值组成新的数组：',np.maximum(c,r))print('取两个数组中的较小者组成新的数组：',np.minimum(c,r)) 取两个数组中的较大值组成新的数组： [2 3 4 5] 取两个数组中的较小者组成新的数组： [1 2 3 4] 1234567x1 = np.array([True,False,True])x2 = np.array([False,False,True])print(x1)print(x2)print(np.logical_and(x1,x2))print(np.logical_or(x1,x2))print(np.logical_xor(x1,x2)) 氦核：逻辑运算一样很简单，logical_xor是异或运算。一般逻辑函数用于检验数组内容，筛选出需要的元素（通常得到的是位置）。可以完成“检查a中元素是否存在于b中”这样的问题。简便操作详细见下面集合运算。 [ True False True] [False False True] [False False True] [ True False True] [ True False False] 数组——二元函数 函数 说明 add、multiply 数组中对应的元素相加、相乘 substract 第一个数组减去第二个数组中的元素 divide、floor_divide 除法、向下圆整除法(余数直接舍弃) power 对于第一个数组中的元素，根据第二个数组中的对应元素，进行幂运算 maximum、fmax 元素级的最大值、fmax功能相同只是忽略NaN minimum、fmin 元素级的最小值、fmin功能相同只是忽略NaN mod 元素级的求余 copysign 将第二个数组中的值的符号复制给第一个数组中的值 greater、greater_equal、less、less_equal、equal、not_equal 元素级的比较运算，产生True或者False为元素的数组 logical_and、logical_or、logical_xor 元素级的逻辑判断(且、或者、不等于) 1.4 数组array集合运算12345678910111213x = np.array([1,2,3,3,3,3,4,5,10,10,20,30])y = np.array([100,20,40,10,3,2,1])print('数组x中的唯一元素：\n',np.unique(x))print()print('数组x和y的公共元素：\n',np.intersect1d(x,y))print()print('数组x和y的并集：\n',np.union1d(x,y))print()print('数组x中的元素是否包含于y：\n',np.in1d(x,y))print()print('集合差_在x中而不在y中的元素：\n',np.setdiff1d(x,y))print()print('只存在某个数组中，而不同时存在于两个数组中：\n',np.setxor1d(x,y)) 数组x中的唯一元素： [ 1 2 3 4 5 10 20 30] 数组x和y的公共元素： [ 1 2 3 10 20] 数组x和y的并集： [ 1 2 3 4 5 10 20 30 40 100] 数组x中的元素是否包含于y： [ True True True True True True False False True True True False] 集合差_在x中而不在y中的元素： [ 4 5 30] 只存在某个数组中，而不同时存在于两个数组中： [ 4 5 30 40 100] 1.5 数组array切片进阶12#还是以000001收盘价为例a1 array([8.6 , 8.66, 9.03, 8.98, 8.78]) 12345# 如果我们想 按偶数来选取 即选择数组中的0,2,4,6,8print(a1[::2])# 如果想 按奇数来选择呢print(a1[1::2]) #这里的1表示从索引1开始截取 氦核：上面片段中的2代表步长。 [8.6 9.03 8.78] [8.66 8.98] 1234567#我们取多个指标看一下#五粮液from WindPy import *w.start()w = w.wsd("000858.SZ", "open,high,low,close,pct_chg", "2018-07-05", "2018-07-11", "")w 氦核：开，高，低，收。 .ErrorCode=0 .Codes=[000858.SZ] .Fields=[OPEN,HIGH,LOW,CLOSE,PCT_CHG] .Times=[20180705,20180706,20180709,20180710,20180711] .Data=[[71.69,69.9,71.58,73.57,71.2],[72.71,71.98,73.55,74.13,72.64],[69.7,68.88,70.4,72.08,70.93],[70.82,70.62,73.54,73.37,72.08],[0.16973125884014886,1.5822798147378172,4.134806003964902,-0.23116671199347652,-1.7582118031893383]] 12w1 = np.array(w.Data)w1 array([[71.69 , 69.9 , 71.58 , 73.57 , 71.2 ], [72.71 , 71.98 , 73.55 , 74.13 , 72.64 ], [69.7 , 68.88 , 70.4 , 72.08 , 70.93 ], [70.82 , 70.62 , 73.54 , 73.37 , 72.08 ], [ 0.16973126, 1.58227981, 4.134806 , -0.23116671, -1.7582118 ]]) 12345print('截取第1行第4,5个元素：\n',w1[0, 3:5])print()print('截取第5行至最后，第5列至最后的元素：\n',w1[4:, 4:])print()print('截取第3，5行，第1,3,5列\n',w1[2::2, ::2]) 截取第1行第4,5个元素： [73.57 71.2 ] 截取第5行至最后，第5列至最后的元素： [[-1.7582118]] 截取第3，5行，第1,3,5列 [[69.7 70.4 70.93 ] [ 0.16973126 4.134806 -1.7582118 ]] 1.6 数组排序123456from WindPy import *w.start()# 000001.SZ收益率为例sy = w.wsd("000001.SZ", "pct_chg", "2018-07-05", "2018-07-11", "")sy .ErrorCode=0 .Codes=[000001.SZ] .Fields=[PCT_CHG] .Times=[20180705,20180706,20180709,20180710,20180711] .Data=[[-0.11614401858303243,0.6976744186046501,4.272517321016162,-0.5537098560354228,-2.2271714922049117]] 12sy2 = np.array(sy.Data[0])sy2 array([-0.11614402, 0.69767442, 4.27251732, -0.55370986, -2.22717149]) 12# 对一个数组array，想找到其中大于0的数所在的索引位置 可以用where函数print('大于0元素所在的索引：\n',np.where(sy2&gt;0)) 大于0元素所在的索引： (array([1, 2], dtype=int64),) 12# 对于exp这个数组，希望对其按元素大小进行排序print('从小到大排序：\n',np.sort(sy2)) 从小到大排序： [-2.22717149 -0.55370986 -0.11614402 0.69767442 4.27251732] 1print('排序后元素所在的原索引位置',np.argsort(sy2)) 排序后元素所在的原索引位置 [4 3 0 1 2] 1.7 数组拼接1a1 array([8.6 , 8.66, 9.03, 8.98, 8.78]) 12a3 = np.array(w.wsd("000858.SZ", "close", "2018-07-05", "2018-07-11", "").Data)[0]a3 array([70.82, 70.62, 73.54, 73.37, 72.08]) 123print('纵向拼接:\n',np.vstack((a1,a3)))print()print('横向拼接:\n',np.hstack((a1,a3))) 纵向拼接: [[ 8.6 8.66 9.03 8.98 8.78] [70.82 70.62 73.54 73.37 72.08]] 横向拼接: [ 8.6 8.66 9.03 8.98 8.78 70.82 70.62 73.54 73.37 72.08] 使用np.r_和np.c_也可以实现拼接的功能 注意纵向拼接的时候，np.c_产生的结果是5∗2，而np.r_产生的结果是2∗5 123print('横向拼接：\n',np.r_[a1,a3])print()print('纵向拼接：\n',np.c_[a1,a3]) 横向拼接： [ 8.6 8.66 9.03 8.98 8.78 70.82 70.62 73.54 73.37 72.08] 纵向拼接： [[ 8.6 70.82] [ 8.66 70.62] [ 9.03 73.54] [ 8.98 73.37] [ 8.78 72.08]] 1.8 数组分解12a4 = np.array(w.wsd("000858.SZ", "close,open,low", "2018-07-05", "2018-07-11", "").Data)a4 array([[70.82, 70.62, 73.54, 73.37, 72.08], [71.69, 69.9 , 71.58, 73.57, 71.2 ], [69.7 , 68.88, 70.4 , 72.08, 70.93]]) 123print('横向分解为5个数组：\n',np.hsplit(a4,5))print()print('纵向分解为3个数组：\n',np.vsplit(a4,3)) 横向分解为5个数组： [array([[70.82], [71.69], [69.7 ]]), array([[70.62], [69.9 ], [68.88]]), array([[73.54], [71.58], [70.4 ]]), array([[73.37], [73.57], [72.08]]), array([[72.08], [71.2 ], [70.93]])] 纵向分解为3个数组： [array([[70.82, 70.62, 73.54, 73.37, 72.08]]), array([[71.69, 69.9 , 71.58, 73.57, 71.2 ]]), array([[69.7 , 68.88, 70.4 , 72.08, 70.93]])] 二、常用数组在工作或者学习中，有些数组是我们常用的，利用numpy中的函数可以容易地产生这些数组。 2.1 np.arange(起始数，终止数，间隔)1234# np.arange()函数 终止数并不产生 print(np.arange(1,10,1)) print()print(np.arange(1,10,0.1)) [1 2 3 4 5 6 7 8 9] [1. 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2. 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3. 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4. 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5. 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6. 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7. 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8. 8.1 8.2 8.3 8.4 8.5 8.6 8.7 8.8 8.9 9. 9.1 9.2 9.3 9.4 9.5 9.6 9.7 9.8 9.9] 2.2 np.linspace(起始数,终止数,产生数的个数)12345#在指定区间返回均匀间隔的数字print(np.linspace(1,10,10))print()print(np.linspace(-1,1,20))# np.linspace()函数 终止数是产生的 [ 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.] [-1. -0.89473684 -0.78947368 -0.68421053 -0.57894737 -0.47368421 -0.36842105 -0.26315789 -0.15789474 -0.05263158 0.05263158 0.15789474 0.26315789 0.36842105 0.47368421 0.57894737 0.68421053 0.78947368 0.89473684 1. ] 2.3 常用矩阵12345print('元素都为1的方阵：\n',np.ones((3,3)))print()print('元素都为0的方阵：\n',np.zeros((3,3)))print()print('单位阵：\n',np.eye(3)) 元素都为1的方阵： [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] 元素都为0的方阵： [[0. 0. 0.] [0. 0. 0.] [0. 0. 0.]] 单位阵： [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] 2.4 np.tile()函数该函数的作用是重复某个对象为一定的结构 1234short = np.arange(1,4,1)print(short)long = np.tile(short,3)print(long) [1 2 3] [1 2 3 1 2 3 1 2 3] 1234small = np.eye(2)print(small)big = np.tile(small, (2,2))print(big) [[1. 0.] [0. 1.]] [[1. 0. 1. 0.] [0. 1. 0. 1.] [1. 0. 1. 0.] [0. 1. 0. 1.]] 三、Numpy常用常量1234print('自然底数：',np.e)print('缺失值：',np.NaN)print('无穷大：',np.inf)print('圆周率：',np.pi) 自然底数： 2.718281828459045 缺失值： nan 无穷大： inf 圆周率： 3.141592653589793 四、Numpy随机数产生123456789print('一维正态随机数：\n',np.random.randn(5))print()print('二维正态随机数：\n',np.random.randn(2,2)) print()print('二维0-1均匀分布随机数：\n',np.random.rand(2,2)) print()print('5个10-20的均匀随机整数：',np.random.randint(10,20,5))print()print('二维均匀随机整数：\n',np.random.randint(10,50,(2,2))) 一维正态随机数： [-0.65240628 1.07426523 -0.86730208 -0.91339241 1.20066541] 二维正态随机数： [[ 0.9663628 -1.32891237] [-0.54264665 -1.29035995]] 二维0-1均匀分布随机数： [[0.52527192 0.17719291] [0.10103986 0.41623399]] 5个10-20的均匀随机整数： [17 18 14 12 11] 二维均匀随机整数： [[38 24] [24 38]] numpy.random函数 函数 说明 seed 随机数生成器的种子 permutation 序列的随机排列或者随机排列的范围，不改变原数组 shuffle 序列就地随机排列，改变原数组 rand 均匀分布样本值 randint 给定上下限随机产生整数 randn 正态分布样本值 binomial 二项分布样本值 normal 正态分布样本值 beta beta分布样本值 chisquare 卡方分布样本值 gamma Gamma分布样本值 uniform [0,1)均匀分布样本值 choice 从数组中随机选择若干个元素 12345a = np.arange(1,11,1)print(a)np.random.shuffle(a)print()print('随机打乱a中的元素顺序：\n',a) [ 1 2 3 4 5 6 7 8 9 10] 随机打乱a中的元素顺序： [10 3 2 6 1 9 8 7 4 5] 123print(a)print()print('随机从a中选取5个元素：\n',np.random.choice(a,5)) [10 3 2 6 1 9 8 7 4 5] 随机从a中选取5个元素： [5 3 8 9 4] 五、Numpy矩阵性质123456789101112x = np.random.randint(1,10,(3,3))print('原矩阵：\n',x)print()print('矩阵对角线：\n',np.diag(x)) print()print('矩阵上三角：\n',np.triu(x)) print()print('矩阵下三角：\n',np.tril(x)) print()print('矩阵的迹：\n',np.trace(x)) print()print('矩阵的转置：\n',x.T) 原矩阵： [[4 3 6] [2 7 1] [2 3 8]] 矩阵对角线： [4 7 8] 矩阵上三角： [[4 3 6] [0 7 1] [0 0 8]] 矩阵下三角： [[4 0 0] [2 7 0] [2 3 8]] 矩阵的迹： 19 矩阵的转置： [[4 2 2] [3 7 3] [6 1 8]] 1234x = np.random.randint(1,10,(3,3))print('原矩阵：\n',x)print()print('矩阵元素向右循环移动2位：\n',np.roll(x,2)) 原矩阵： [[1 1 6] [7 3 3] [7 6 5]] 矩阵元素向右循环移动2位： [[6 5 1] [1 6 7] [3 3 7]] 六、Numpy矩阵运算12# numpy下的子模块linalg是一个线性代数运算库，关于矩阵运算主要使用该库来完成import numpy.linalg as la 123# 以000001行情数据为例a5 = np.array(w.wsd("000001.SZ", "close,open,low", "2018-07-08", "2018-07-11", "").Data)a5 array([[9.03, 8.98, 8.78], [8.69, 9.02, 8.76], [8.68, 8.89, 8.68]]) 123456789print('原矩阵：\n',a5)print()print('矩阵的行列式：\n',la.det(a5)) print()print('矩阵的逆：\n',la.inv(a5)) print()print('矩阵的特征值分解：\n',la.eig(a5))print()print('矩阵的奇异值分解：\n',la.svd(a5)) 原矩阵： [[9.03 8.98 8.78] [8.69 9.02 8.76] [8.68 8.89 8.68]] 矩阵的行列式： 0.0967539999999976 矩阵的逆： [[ 4.31196643 1.11416582 -5.48607809] [ 6.27984373 22.42801331 -28.98691527] [-10.74374186 -24.08479236 35.28949708]] 矩阵的特征值分解： (array([2.65036938e+01, 2.08824580e-01, 1.74815890e-02]), array([[-0.58362007, -0.78625042, -0.09211654], [-0.57657184, 0.5886538 , -0.64732544], [-0.57179763, 0.18787491, 0.75662693]])) 矩阵的奇异值分解： (array([[-0.58355077, 0.78217367, -0.21834112], [-0.5766266 , -0.58842029, -0.56680096], [-0.57181314, -0.20485584, 0.79439526]]), array([2.65057682e+01, 2.11310491e-01, 1.72745790e-02]), array([[-0.57510827, -0.58571691, -0.57112711], [ 0.81163598, -0.49595222, -0.30867203], [-0.10245733, -0.64106715, 0.76061515]])) numpy.linalg函数 函数 说明 diag 以一维数组的形式返回方阵的对角线元素或将一维数组转化为方阵 dot、trace、det 矩阵乘法、矩阵的迹运算、矩阵行列式 eig、inv、pinv 方阵的特征值和特征向量、方阵的逆、矩阵的Moore-Penrose伪逆 qr、svd 矩阵的QR分解、奇异值分解 solve 解线性方程组 $𝑋\beta = 𝑦$，其中$𝑋$为方阵 lstsq 计算$𝑋\beta = 𝑦$的最小二乘解 七、多项式曲线拟合12import matplotlib.pyplot as plt # 导入作图库 为了更好展示曲线拟合的结果plt.style.use('ggplot') 例如，对于下面的这些散点进行多项式拟合。观察散点的形态，采用直线取拟合 12345x = np.linspace(-10,10,100)y = 2*x + 1 + np.random.randn(100)*2fig = plt.subplots(figsize=(14,8))plt.plot(x, y, 'rx')plt.show() 1from numpy import polyfit,poly1d 12coef_fit = polyfit(x, y, 1) #进行线性拟合 1代表的是多项式拟合的多项式的阶数 这里指的是线性拟合coef_fit #查看拟合的系数 array([1.96386726, 1.11375232]) 12345fig = plt.subplots(figsize=(14,8))plt.plot(x, y, 'rx',label='真实散点')plt.plot(x, coef_fit[0] * x + coef_fit[1], 'k-',label='拟合直线')plt.legend(loc='best')plt.show() D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 30495 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 23454 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 25955 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 28857 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 25311 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 21512 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 30452 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:211: RuntimeWarning: Glyph 32447 missing from current font. font.set_text(s, 0.0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 30495 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 23454 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 25955 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 28857 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 25311 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 21512 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 30452 missing from current font. font.set_text(s, 0, flags=flags) D:\anaconda\lib\site-packages\matplotlib\backends\backend_agg.py:180: RuntimeWarning: Glyph 32447 missing from current font. font.set_text(s, 0, flags=flags) 从上图可以看到，直线拟合的结果还是比较好的（报错可能是汉字bug 12f = poly1d(coef_fit) #也可以直接产生拟合的函数解析式print('拟合函数：',f) 拟合函数： 1.964 x + 1.114 氦核：numpy库很有“大计算器”的味道了，其实不会用的时候再去查也可以，主要是应该了解有什么基础功能，否则就会出现“自己实现某些基础算法”的乌龙（某种意义上也是好事，笑）。一起加油吧。 （完）]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python联萌|初探WindAPI]]></title>
    <url>%2F2020%2F02%2F01%2F20200201WindAPI%2F</url>
    <content type="text"><![CDATA[最后编辑于：2020.02.05 12:30 Wind API使用说明windAPI是一个很好的工具，可以不通过客户端获取数据（不过前提是要有土豪的账号加持，笑）本文大部分介绍性文字转载自万旷网。本文的分析全部通过python完成。配置安装略过不表，请致电客服经理小哥哥（声音奶声奶气有点温柔！ 0. 综述目前万矿网支持的API函数有： 1、WSD日期序列函数：支持股票、债券、基金、期货、指数等多种证券的基本资料、股东信息、市场行情、证券分析、预测评级、财务数据等各种数据。WSD可以支持取 多品种单指标 或者 单品种多指标 的时间序列数据。（氦核：可以说是最重要的函数） 2、WSS多维函数：同样支持股票、债券、基金、期货、指数等多种证券的基本资料、股东信息、市场行情、证券分析、预测评级、财务数据等各种数据。但是WSS支持取多品种多指标某个时间点的截面数据。 3、WSQ行情数据函数：支持股票、债券、基金、期货、指数等多种证券品种的实时行情数据，既可以选择获取一次性的快照数据，也可以选择订阅数据（即交易所有新的行情就推送）。 4、WSET数据集：支持股票、债券、基金、期货、指数等多种证券品种板块成分、指数历史成分股以及权重，以及各种市场常用报表的获取。 5、TDays 日期函数：日期函数包含日期序列函数（TDays)、日期偏移函数(TDaysOffset) 以及日期区间统计函数（TDaysCount）。 下面为大家介绍各个函数的详细用法。 注： 建议用户在使用取数函数时直接借助API函数生成相应的取数代码，然后修改其中的参数使其满足自己的取数需求。(氦核注：这个很好使，点点点就能拿到代码。) 1. WSD日期序列函数定义该命令用来获取选定证券品种的历史序列数据，包括日间的行情数据、基本面数据以及技术数据指标。（氦核注：有些数据被限制获取的数量了，有些是最多五十个，这倒是很不方便。如果看实盘还好，但是做分析就不舒服了。） 函数输入WSD函数结构 w.wsd（security,fields,startdate,enddate,option） Element Type Description 证券（必选） Security String 获取数据的证券列表 范例1：’600030.SH’说明：证券列表支持Wind代码及证券转换类工具函数输出的Wind代码结果 指标（必选） Fields String 获取数据的指标列表 范例1：’CLOSE,HIGH,LOW,OPEN’ 起始日期（必选） StartDate 时间序列的起始日期 范例1：’2017-01-01’,’-5w’说明：支持日期类工具函数输出的标准日期结果，支持相对日期宏表达方式，日期宏具体使用方式参考’日期宏’部分内容 截止日期（必选） EndDate 时间序列的截止日期，若为空默认为系统当前日期 范例1：’2017-05-30’，Sys.Date()，支持相对日期，比如’0w’; 不输入的话为当前时间说明：支持日期类工具函数输出的标准日期结果，支持相对日期宏表达方式 指标参数（可选） Parameter/Value String 提取指标时使用的参数名/指定参数的值 范例：’TRADE_DATE=20110301;FUND_DATE=20101231’说明：多指标参数支持在不同引号内分开取值 变频参数（可选） Period String 每天一值:D/每周一值:W/每月一值:M/每季度一值:Q/每半年一值:S/每年一值:Y 范例：’Period=D’ ，默认Period=D 输出日期（可选） Days String 所有工作日:Weekdays/所有日历日:Alldays/排除所有非交易日:Trading 范例：’Days=Trading’，默认Days=Trading 填充方式（可选） Fill String 沿用之前数据:Previous/返回空值:Blank 范例：’Fill=Previous’，默认Fill=Blank 日期排序（可选） Order String 升序:A/ 降序:D，最近日期在先 交易日历（可选） TradingCalendar String 选择不同交易所所在国家地区日历 范例1：’ TradingCalendar =SSE’，默认TradingCalendar =SSE;SSE表示上交所，SZSE表示深圳证券交易所，CFFE表示中金所…… 输出币种（可选） Currency String 使用货币设置： ORIGINAL:原始货币/HKD：港币/USD：美元/CNY：人民币 范例1：’Currency =Original’，默认Currency =Original 关于指标参数的详细说明见 7.指标常见参数说明 函数输出 输出内容 说明 错误ID ErrorCode 返回值为0 ，则表示代码运行正常。若为其他则需查找原因 数据列表 Data 函数读取的数据存到此列表中，比如：读取000592.SZ 的close,open指标从’2017-05-08’到’2017-05-18’区间的数据.Data=[[5.12,5.16,5.02,4.9,4.91,5.13,5.35,5.42,5.32],[5.3,5.12,5.17,4.98,4.94,4.93,5.1,5.4,5.4]] 证券代码列表 Codes 输入的证券代码列表 .Codes=[000592.SZ] 字段列表 Field 函数输入中请求的字段列表 .Fields=[CLOSE,OPEN] 时间列表 Times 输出时间序列.Times=[20170508,20170509,20170510,20170511,20170512,20170515,20170516,20170517,20170518] 示例12345# 加载相关的包from WindPy import *import timeimport pandas as pdw.start() w.start是启动函数 12345# 例1、 取富士康概念股近七个交易日的每天机构资金流入额date=time.strftime("%Y-%m-%d", time.localtime()) stock=w.wset("sectorconstituent","date="+date+";sectorid=1000011346000000").Data[1] #富士康概念股最新板块成分buyamt=w.wsd(stock, "mfd_buyamt_d", "ED-7TD", date, "unit=1;traderType=1") #traderType表示类型，如机构、大户、中户、散户，具体参数设置可以借助API函数了解buyamt 1pd.DataFrame(buyamt.Data,index=stock,columns=buyamt.Times).T 1234# 例2、 任取一只国债010107.SH六月份以来的净值历史行情数据history_data=w.wsd("010107.SH", "sec_name,ytm_b,volume,duration,convexity,open,high,low,close,vwap", "2018-06-01", "2018-06-11", "returnType=1;PriceAdj=CP") # returnType表示到期收益率计算方法，PriceAdj表示债券价格类型‘#pd.DataFrame(history_data.Data,index=history_data.Fields,columns=history_data.Times).Tpd.DataFrame(history_data.Data,index=["中文简称","YTM","成交量","久期","凸性","开盘价","最高价","最低价","收盘价","均价"],columns=history_data.Times).T 1234# 例3、 取国内所有农产品主力合约不同日期所对应的具体合约future=w.wset("sectorconstituent","date="+date+";sectorid=1000009337000000") #农产品主力合约板块tradecode=w.wsd(future.Data[1], "trade_hiscode", "2018-01-01", "2018-06-11", "")pd.DataFrame(tradecode.Data,index=future.Data[2],columns=tradecode.Times).T 2. WSS多维数据函数定义命令用来获取选定证券品种的历史截面数据 命令原型为：data= w.wss(品种代码,指标,可选参数) 函数输入WSS函数输入 w.wss（security,fields, option） Element Type Description 证券（必选） Security String 获取数据的证券列表 范例：’600030.SH,600031.SH 说明：证券列表支持Wind代码及证券转换类工具函数输出的Wind代码结果 指标（必选） Fields String 获取数据的指标列表 范例1：’CLOSE,HIGH,LOW,OPEN’ 范例2：[‘CLOSE’,’HIGH’,’LOW’,’OPEN’] 指标参数（可选） Parameter/Value String 提取指标时使用的参数名/指定参数的值 范例：’TRADE_DATE=20170601;FUND_DATE=’20161231’ 说明：多指标参数支持在不同引号内分开取值 输出币种（可选） Currency String 使用什么货币 ORIGINAL/HKD/USD/CNY 范例：’Currency =Original’，默认Currency =Original 关于指标参数的详细说明见 7.指标常见参数说明 示例1234# 例4、 取A股纳入MSCI各成分股的基本资料信息MSCI_stock=w.wset("sectorconstituent","date="+date+";sectorid=1000027970000000")infor=w.wss(MSCI_stock.Data[1] , "sec_name,ipo_date,mkt,stockclass,industry_sw,indexcode_sw,SHSC,SHSC2","tradeDate="+date+";industryType=1")pd.DataFrame(infor.Data,index=infor.Fields,columns=MSCI_stock.Data[1]).T 1234# 例5、 取截止日期 上海证券交易所 发行的国债 基本资料bond=w.wset("sectorconstituent","date=2018-06-11;sectorid=a101010201000000").Data[1]error_code,bond_data=w.wss(bond, "sec_name,issueamount,term,issue_issueprice,couponrate,coupon,interesttype,interestfrequency,carrydate,maturitydate,ptmyear,trade_status","unit=1;tradeDate=20180611",usedf=True)bond_data.head(10) 1234# 例6、 取被动指数型基金最新业绩排名fund=w.wset("sectorconstituent","date=2018-06-11;sectorid=2001010102000000").Data[1]error_code,returns=w.wss(fund, "sec_name,return_1w,return_1m,return_3m,return_6m,return_1y,return_ytd,fund_fundmanager","annualized=0;tradeDate=20180611",usedf=True)returns.head(10) 123#按今年以来总回报排序returns_sort=returns.sort_values(by = 'RETURN_YTD',ascending=False) returns_sort.head(10) 12first_fund=list(returns_sort.index.values)first_fund[0] 1234from WindCharts import *error_code,nav=w.wsd(first_fund[0], "NAV_adj", '2017-01-01', "2018-06-11", usedf=True)chart=WLine(title="复权单位净值走势图",subtitle=first_fund[0],data=nav)chart.plot() 3. WSQ行情数据函数3.1 实时行情取数函数说明定义命令用来获取选定证券品种的当天实时指标数据，数据可以一次性请求，也可以通过订阅的方式获取 命令原型为： data=w.wsq(品种代码,指标,可选参数,回调函数) 函数输入函数名: w.wsq（security,fields,func = None) Element Type Description 证券（必选） Security String 获取数据的证券列表 范例：’600030.SH’说明：实时行情所支持品种较多，基本上终端中有的行情接口中皆可取得 指标（必选） Fields String 获取数据的指标列表 范例：’rt_open,rt_high,rt_last’ 回调函数（可选） Func 指定回测函数 范例：’ func=w.demoCallback’ 返回选定品种的实时数据，支持一次请求和订阅两种方式。 示例12345#例7. 获取沪股通最新一笔的行情数据hksh=w.wset("sectorconstituent","date=2018-06-12;sectorid=1000014938000000").Data[1]mk_data=w.wsq(hksh,"rt_last,rt_vol,rt_amt,rt_chg,rt_pct_chg,rt_swing,rt_vwap,rt_upward_vol,rt_downward_vol,rt_ask1,rt_ask2,rt_ask3,rt_ask4,rt_ask5,rt_bid1,rt_bid2,rt_bid3,rt_bid4,rt_bid5")#pd.DataFrame(tradecode.Data,index=future.Data[2],columns=tradecode.Times).Tpd.DataFrame(data.Data,index=data.Fields,columns=data.Codes).T 12#任意订阅一只股票的最新行情w.wsq("000001.SZ", "rt_last", func=DemoWSQCallback) 1234#订阅行情后当交易所发送新的行情数据时，这里就会推送# 上面订阅后会返回一个RequestID，此ID作为后面取消订阅函数的传入参数#当传入ID为0时表示取消所有订阅 w.cancelRequest(3) 4. WSET数据集函数定义命令用来获取数据集信息，包括板块成分、指数成分、ETF申赎成分信息、分级基金明细、融资标的、融券标的、融资融券担保品、回购担保品、停牌股票、复牌股票、分红送转等 参数设置为起止日期、板块名称等，不同的报表有不同的参数设置 命令原型为： data=w.wset(数据集名称,可选参数) 函数输入函数名: w.wset（view，options），返回股票，基金，债券，商品等专题统计报表的数据。 Element Type Description 数据集（必选） view String 提取数据集的VIEW名 范例1：’SectorConstituent’ View参数（可选） Parameter/Value String 提取指标时使用的参数名/指定参数的值 范例1：’date=20130531’; 板块列表（可选） sectorid String 获取数据的板块ID 范例1： ‘sector=全部A股’ 范例2：’sectorid=a001010100000000’ 字段列表（可选） Field String 获取字段列表的数据 范例1：’field=wind_code,i_weight’ 基本获取某些板块的数据方法在上面的函数介绍中已经涉及了，这里就不再赘述 示例123#例8、 获取申万一级行业的成分股sw_index=w.wset("sectorconstituent","date=2018-06-12;sectorid=a39901011g000000") #申万一级行业指数代码sw_index 12345# 下面分别取各行业指数的成分股result=pd.DataFrame()for i in range(len(sw_index.Data[0])): x=pd.DataFrame(w.wset("sectorconstituent","date=2018-06-12;windcode="+sw_index.Data[1][i]+"").Data[1],columns=[sw_index.Data[1][i]]) result=pd.concat([result,x], axis=1) 1234#例9、 获取A股纳入MSCI成分股的2017年报的股票分红实施情况# MSCI_stock=w.wset("sectorconstituent","date="+date+";sectorid=1000027970000000") MSCI股票代码上文已经取出error_code,bonus=w.wset("bonus","orderby=报告期;year=2017;period=y1;sectorid=1000027970000000",usedf=True)bonus.head(10) 123#例10、 沪深交易所期权列表error_code,option=w.wset("optioncontractbasicinfo","exchange=sse;windcode=510050.SH;status=trading",usedf=True)option.head(10) 5. TDays 日期函数5.1 返回区间内的日期序列w.tdays定义命令用来获取一个时间区间内的某种规则下的日期序列。 函数输入函数名：TDays(startDate,endDate,[Optional argument]) Element Type Description 起始日期（必选） StartDate String 时间序列的起始日期 范例1：”2015-01-01”，支持日期宏 截止日期（必选） EndDate String 时间序列的截止日期，置空取当前最新日期 范例1：”2015-06-30”，支持日期宏 日期类型（可选） Days String 所有工作日：Weekdays，所有日历日：Alldays，排除所有非交易日：Trading 范例：’Days=Trading’，默认Days=Trading 变频参数（可选） Period String 每天一值：D， 每周一值：W，每月一值M：，每季度一值：Q ，每半年一值：S ，每年一值：Y 范例：’Period=D’ 交易日历（可选） TradingCalendar String TradingCalendar默认为上海证券交易所，当DAYS为日历日的时候，这个参数不起作用,只有当DAYS为交易日的时候，这个参数才起作用,默认“TradingCalendar=SSE”(上海证券交易所) 示例123# 例11 取上交所2018年以来的交易日期序列，交易所为空默认为上交所date_list=w.tdays("2018-05-13", "2018-06-13"," ")date_list 5.2 返回某个偏移值对应的日期w.tdaysoffset定义命令用来获取基于某个基准时间前推(0)指定天数的日期。 命令原型为：data=w.tdaysoffset(偏移值，基准时间,可选参数) 函数输入函数名:TDaysOffset(offset, refDate, [Optional argument]) Element Type Description 参考日期 refDate String 参照日期 范例1：”2015-01-01”，支持日期宏 日期类型（可选） Days String 所有工作日：Weekdays，所有日历日：Alldays，排除所有非交易日：Trading 范例：’Days=Trading’，默认Days=Trading 变频参数（可选） Period String 每天一值：D， 每周一值：W，每月一值M：，每季度一值：Q ，每半年一值：S ，每年一值：Y 范例：’Period=D’，默认Period=D 交易日历（可选） TradingCalendar String TradingCalendar默认为上海证券交易所，当DAYS为日历日的时候，这个参数不起作用,只有当DAYS为交易日的时候，这个参数才起作用。默认“TradingCalendar=SSE”(上海证券交易所) 偏移量（可选） Offset 偏移参数 偏移参数，为整数，&gt;0后推，&lt;0前推，默认为0 示例1234# 例12 取从今天往前推10个月的日历日import datetimetoday = datetime.date.today() w.tdaysoffset(-10, today.isoformat(), "Period=M;Days=Alldays") 5.3 返回某个区间内日期数量w.tdayscount定义命令用来获取两个时间区间内的某种规则下的日期序列个数 命令原型为：data= w.tdayscount(开始时间，结束时间,可选参数) 函数输入函数名：TDaysCount(startDate,endDate, [Optional argument]) Element Type Description 起始日期（必选） StartDate String 时间序列的起始日期 范例1：”2017-01-01”，支持日期宏 截止日期 EndDate String 时间序列的截止日期，置空取当前最新日期 范例1：”2017-06-30”，支持日期宏 日期类型（可选） Days String 所有工作日:Weekdays，所有日历日：Alldays，排除所有非交易日：Trading 范例：’Days=Trading’，默认Days=Trading 交易日历（可选） TradingCalendar String TradingCalendar默认为上海证券交易所，当DAYS为日历日的时候，这个参数不起作用,只有当DAYS为交易日的时候，这个参数才起作用，默认“TradingCalendar=SSE”(上海证券交易所) 123# 例13 统计2017年交易日天数days=w.tdayscount("2017-01-01", "2017-12-31", "").Data[0]days 6. 日期宏的说明6.1 通用日期支持相对日期表达方式，相对日期周期包括:交易日TD、日历日：D、日历周：W、日历月：M、日历季：Q、日历半年：S、日历年：Y。 相关说明：1、以’-’代表前推，数字代表N个周期，只支持整数；后推没有负号，比如’-5D’表示从当前最新日期前推5个日历日；2、截止日期若为’’空值，取系统当前日期；3、可对日期宏进行加减运算，比如’ED-10d’。 举例：1、起始日期为1个月前，截至日期为最新 StartDate=’-1M’,EndDate=’’2、起始日期为前推10个交易日，截至日期为前推5个交易日 StartDate=’-10TD’,EndDate=’-5TD’ 6.2 特殊日期宏 宏名称 截止日期 开始日期 去年一季 去年二季 去年三季 去年年报 今年一季 今年二季 今年三季 最新一期 本年初 下半年初 本月初 本周一 上周末 上月末 上半年末 上年末 上市首日 宏助记符 ED SD LQ1 LQ2 LQ3 LYR RQ1 RQ2 RQ3 MRQ RYF RHYF RMF RWF LWE LME LHYE LYE IPO 123#例14 用日期宏IPO的示例error_code,data=w.wsd("000001.SZ", "close", 'IPO', "2018-06-11", usedf=True)data.head() 7. 指标参数的说明关于WSD/WSS中常见指标参数的举例说明 element description value Indicator examples demo tradeDate 交易日期 自填 开盘价 ‘tradeDate=20180618’ startDate 开始日期 自填 区间开盘价 ‘startDate=20180618’ endDate 截止日期 自填 区间开盘价 ‘endDate=20180618’ adjDate 复权基期 自填 收盘价(支持定点复权) ‘adjDate=20180618’ rptDate 报告期 自填 净利润 ‘rptDate=20171231’ priceAdj 复权方式 不复权/前复权/后复权/定点复权 收盘价 ‘priceAdj=U’ cycle 周期 日/月/…… 成交量 ‘cycle=D’ bondPriceType 债券价格类型 全价/净价/收益率/市价 涨跌幅(债券) ‘bondPriceType=2’ currencyType 币种 原始币种/人民币/美元…… 未平仓卖空金额 ‘currencyType=Cur=CNY’ ndays 天数(用负号表示前推) 自填 N日涨跌幅 ‘ndays=-5’ rptType 报表类型 合并报表/母公司报表…… 杠杆率 ‘rptType=1’ dataType 数据类型 本外币/本币/…… 贷款利息收入-短期 ‘dataType=2’ traderType 类型 机构/大户/散户…… 流入单数 ‘traderType=1’ exchangeType 交易所 上海/深圳/银行间…… 跨市场代码 ‘exchangeType=SSE’ index 所属指数 上证50指数/上证180指数/沪深300指数…… 是否属于重要指数成份 ‘index=1’ unit 单位 元、股、份、张、户 总市值、注册资本、持有人持有数量、单一投资者报告期末持有份额合计、股东户数…… ‘unit=1’ industryType/category/industryStandard 行业级别/行业标准 一级行业/二级行业…… 全部明细、中信行业/申万行业…… 所属行业名称(支持历史)、所属恒生行业名称 ‘industryType=1’，’category=1’,’industryStandard=3’ adminType 行政区划级别 省级/地级/县级 所属行政区划 ‘adminType=1’ year 年度 2018/2017/2016…… 管理层年度薪酬总额 ‘year=2017’ month 月份 1月/2月/…… 五星基金占比 ‘month=2’ order 大股东排名、名次 第一名/第二名/…… 持有人持有数量、基金经理年限 ‘order=1’ instituteType 机构类别 基金公司/证券公司/…… 配售对象名称 ‘instituteType=1’ topNum 名次 第1名/第2名…… 离职日期 ‘topNum=1’ shareType 股本类型 流通股本/总股本 户均持股比例 ‘shareType=1’ reportDateType 报告期 第一季度(1-3月)/第二季度(4-6月)…… 单一投资者报告期末持有份额合计 ‘reportDateType=1’ Type 资产池资产级别 次级/优先级/次优级 各级发行总额 ‘reportDateType=1’ type 选择权类型、评级对象类型 赎回/回售/……、主体信用评级/不限…… 行权资金到帐日 ‘type=Pr’、’type=3’ serial 第N次提前偿还 手动输入数字 提前还本比例 ‘serial=1’ ratingAgency 评级机构 标普/穆迪…… 债项评级 ‘ratingAgency=16’ bondTypeIndex 债券类型 短期融资券/中期票据/…… 历史累计注册额度 ‘bondTypeIndex=2’ bondType 债券类型 金融债/同业存单…… 存量债券余额 ‘bondType=1’ termType 期限类型 1年期内/1-3年期内…… 存量债券余额(按期限) termType=2 credibility 估值类型 推荐/不推荐/行权…… 日终估价全价 ‘credibility=2’ fundType 基金分类 投资类型(一级分类)/投资类型(二级分类) 同类基金数量 ‘fundType=1’ fundRatingAgency 评级机构 1/2…… 四星基金占比 ‘fundRatingAgency=2’ chargesType 收费类型 前端/后端 认购费率 ‘chargesType=1’ returnType 收益率计算方法 普通收益率/对数收益率 几何平均年化收益率 ‘returnType=1’ annualized 是否年化 是/否 近1周回报 ‘annualized=0’ netType 报告期净值数据项 过去1个月/过去3个月/…… 报告期净值增长率 ‘netType=1’ 12例16 指标参数说明案例w.wss("510010.SH", "fund_fullname,fund_similarfundno,fund_purchasefee,fund_redemptionfee,fund_dq_status,fund_pchredm_largepchmaxamt,fund_manager_totalnetasset,fund_manager_arithmeticannualizedyield,fund_manager_totalreturnoverbenchmark,fund_corp_fivestarfundsprop,fund_corp_fourstarfundsprop,fund_corp_teamstability,issue_etfdealshareonmarket,fund_etfpr_estcash,fund_etfpr_cashratio", "fundType=1;chargesType=0;tradeDate=20180619;unit=1;order=2;returnType=1;year=2018;month=1;fundRatingAgency=2;startDate=20180520;endDate=20180620") 8.财务数据的说明Python如图，万矿的财务数据都是传入报告期，然后返回相应报告期的财务数据；在参数设置中可以直接设置报表类型为合并报表/母公司报表/合并报表（调整）/母公司报表（调整）。 万矿是取每个季度最后一天作为报告期,如取2017年的四个定期报告数据，那报告期设置分别为 ：一季报：2017-03-31，半年报：2017-06-30，三季报：2017-09-30，年报:2017-12-31 如果要用wsd取多个报告期的公告数据，则需要将周期设置为季，日期类型设置为日历日(因为有的季度最后一天不是交易日) 财务数据回填说明：1、比如说某公司A在2018年3月22日公布2017年年报，那在2018年3月22以前取2017年年报数据则为空，A公司公布年报数据后，我们会将公布的数据回填到2017-12-31，用户传入相应的报告期参数即可取出对应的年报数据；2、如果用户直接取出年报数据做回测则会引入未来数据，所以如果要用财报数据做回测要先按照定期报告披露日期拉平至时间序列再做回测。 下面举例说明如何获取各个报告期的财务数据。 123#例 17 用wsd函数取000001.SZ[平安银行]近十年的利润表(合并报表)数据error_code,finance_data=w.wsd("000001.SZ","tot_oper_rev,oper_rev,int_inc,insur_prem_unearned,handling_chrg_comm_inc,tot_prem_inc,reinsur_inc,prem_ceded,unearned_prem_rsrv_withdraw,net_inc_agencybusiness,net_inc_underwriting-business,net_inc_customerasset-managementbusiness,other_oper_inc,net_int_inc,net_fee_and_commission_inc,net_other_oper_inc,tot_oper_cost,oper_cost,int_exp,handling_chrg_comm_exp,oper_exp,taxes_surcharges_ops,selling_dist_exp,gerl_admin_exp,fin_exp_is,impair_loss_assets,prepay_surr,net_claim_exp,net_insur_cont_rsrv,dvd_exp_insured,reinsurance_exp,claim_exp_recoverable,Insur_rsrv_recoverable,reinsur_exp_recoverable,other_oper_exp,net_inc_other_ops,net_gain_chg_fv,net_invest_inc,inc_invest_assoc_jv_entp,net_gain_fx_trans,opprofit,non_oper_rev,non_oper_exp,net_loss_disp_noncur_asset,tot_profit,tax,unconfirmed_invest_loss_is", "2007-01-01", "2017-12-31", "unit=1;rptType=1;Period=Q;Days=Alldays",usedf=True)finance_data 1234#例18 用wss取上证50的常见财务指标的2017年年报数据sh_50=w.wset("sectorconstituent","date=2018-06-20;sectorid=1000000087000000").Data[1]error_code,Y_finance_data=w.wss(sh_50, "tot_oper_rev,tot_oper_cost,opprofit,tot_profit,net_profit_is,np_belongto_parcomsh,extraordinary,wgsd_deductedprofit,researchanddevelopmentexpenses,ebit,ebitda,tot_cur_assets,fix_assets,long_term_eqy_invest,tot_assets,tot_cur_liab,tot_non_cur_liab,tot_liab,cap_rsrv,surplus_rsrv,undistributed_profit,cash_recp_sg_and_rs,cash_pay_acq_const_fiolta","unit=1;rptDate=20171231;rptType=1;currencyType=",usedf=True)Y_finance_data （完）特别鸣谢：我家松鼠]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遍历山河|南京]]></title>
    <url>%2F2020%2F01%2F22%2F20200122%E5%8D%97%E4%BA%AC%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[旅行结束，慢慢添加我的心绪啦。有人说旅行是痛并快乐着，我觉得没错。偶尔快乐多，偶尔痛苦多。但是南京不太一样，在这里，我几乎没有痛苦，只有一些遗憾和无穷快乐。 14日 旅行日志告别我纯白如仙的松鼠，我独自拎着箱子，踏上了这次期待不算太久，但我绝对足够向往的旅行。 直达车不直达，特快车不特快，临客不会真的零客，只有高铁和动车还没有辱没名号。或许正是因为他们的出现，其他的客车才相形见绌，铁路系统从此认知失调。车上从各处来的人，混着不同的口音乱杂开放。我很少坐长时间的硬座，今晚恐怕会是个令人记忆深刻的不眠夜。 我初次独自远行去陌生的城市旅游，但也不算完全陌生，还好有认识的长辈在那里，还有松鼠一起，热心地帮我打点安排，让我才能安心地踏上旅途。看着满满当当的旅途安排，我心中说不上来的复杂。自己曾经标榜“读万卷书，行万里路”，然而真正做到还得靠自己一时冲动，当然也归功于周围人们的帮助。虽然很想细细品味，可是走得地方实在太多，眼花缭乱免不了落入走马观花之流，不知所向，只能到现场慢慢品味，再做取舍了。 听说哥哥做打算往往提前两个月，安排行程，预订酒店，这应该很清醒的旅行状态。很期待父母能策划一场出行，但是他们也碍于时间空间的距离，没法随心所欲。松鼠也在帝都寻了一份差事做，于是只留我一个孤独上路。我首次尝试独自出行，我期待一场奇妙的旅行，但是也在不断为之准备着。我着实向往着南京，以至于定下了行程才察觉天气不如意的问题，但冷暖或许已经无妨，我偶尔意决，今天算一次。 有一件值得在意的事：在火车上，教室里，坐着的时候，感觉自己的位置很清晰。但一站起来就瞬间失去方向感，不知自己身处何方。究其原因，或许是同一个视角看的太久，在升维瞬间失了格吧。 15日 旅行日志冬雨不寒马蹄疾，一日看尽金陵花。湿身方嫌寡情雨，酩酊难解太浮夸。 突然吟诗，说明开心。作为一个本意独行这个城市的旅人，没想到获得了异常热情的款待，在一个伯伯的帮助下，我今天辗转了无数景点，尝遍了千万小吃，大饱眼福的同时也大快朵颐，精神和肉体皆是满载而归。 白天刚到南京，漆黑混杂着大雾混沌成一片，南京像个封存老窖中的佳酿，像黑暗中待开启的宝箱，像迎进屋子但披着盖头的新娘，待人一探究竟。逐渐习惯了那些随心所欲的道路规则和巨大显示屏展示的宜人的红绿灯倒计时灯，其实这座城市没有我一开始想象的那么井然，但是在我眼中这般混乱之下，竟也有可以寻找的规则和有序。人人在这里都可以大摇大摆地做天王老子，享受每一刻在路上的时光。我也很享受在这里大街小巷行走的快乐。 我行程飞快。从长江大桥江底隧道，路过狮子山阅江楼，草草翻过毗芦寺，梅园新村，六朝博物馆，再到小九华山，鸡鸣寺和玄武湖，雨中闯入大钟亭，鼓楼和南京大学，夜里的狮子市，夫子庙，穿街走巷雨中狂奔才赶到的1912，没有哪一处突兀多余。这些或自然或历史或现代的风光景色，都已和南京融为一体，俨然成了南京现代生活的一部分。在意想不到的地方，总会突如其来杀出一座山，一座楼，一道水，一院佛。这里充满了人文和自然气息，物华天宝、人杰地灵。这里据说寸土寸金，跨过三五步一条马路突然就是一个崭新的迥然不同的世界，每一步都可能产生新的邂逅，这种未知的刺激令我着迷。南京也多美女，平日里原子弹炸不到一个，南京一颗手榴弹能炸一片，松鼠还嫉妒。 城市的诗情画意，还有它不为人知的小心思，我都有所理解。不仅仅是小面包车上与伯伯一字一句的对谈和学习，更多的是一种气质：城市的气质、人的气质、全部自然总体总和的气质。我听到了地道的南京话，插科打诨，无比惬意，虽然不习惯其中个中洒脱粗鄙，但是想到儒雅如我这个三秦猛男也常常口吐芬芳当街骂娘的事实，我也笑着理解了这种调侃性质的口癖。南京话总有种火药味，还是不要太较真的好。 明天还要早起，今天走着玩几万步，像是玩着走。白天五点到站还没睡够觉，实在困累之至，加上本应该成为东道主的松鼠也沉迷游戏，不搭理我，夜里雨中骑车狂奔尽兴而归才发觉丢了伞，浑身湿透地回了宾馆。大欢喜背后总是一个个小失落。松鼠也质疑我、嗔责我，虽然看起来像是口是心非，有了些南京人互怼的味道，但依然令我十分难受，可是又不想反驳什么，坏了我的期待和焦渴就算了，让松鼠批评一下舒服舒服也无妨。且熄一熄我这团让我膨胀到无所适从的火焰吧。 明天还有明天的旅途，眼皮打架，不说了。 16日 旅行日志早起我就感受到了南京别样的一面，至少在没有暖气这一点上，就宣告了它是一座冰冷的城市。融入和接受恐怕很难，我想只有熬得住寒冷、耐得住性子，才能和这座城市打成一片，才更加接近这座城市的灵魂所在。我还是在寻找神奇的路上，从未停止。我也住过很久没有暖气的房子，全靠电暖气续命，每天起床前一定要把冰凉的衣服在暖气上烤一下，起床才不至于那么煎熬。 今天的目标颇多，本来只与昨天持平，结果今天伯伯突然有事，无力再送我。我已经心怀感激，哪敢多做要求，于是今天的路成全了昨日口中奢望的“独行”。就结果来看，我还是奢望有车接车送只管参观的旅途。不过今天去的景点都是需要用双脚踏踏实实丈量的，倒也完全没有投机取巧的空间，要车也无用。于是就诞生了这近乎4万的超越极限的旅程，从表面看来有车的旅行效率可以提高一倍。 气度雍容的美龄宫开启了这段钟山之旅，皇气浩瀚的明孝陵，排衙端庄的颜真卿碑林，肃穆清白中山陵，绝景入云的灵谷塔，还有一些情趣无限的小地方：三绝碑、音乐台、无梁殿、四方城。虽然走马观花，对历史由来也一知半解，但我总算是将那奇人奇景刻进脑海，待之后再品味。没顾上吃饭，斩了根烤肠，又转头向环环相扣的瞻园，途经深街老巷的老门东，又仰止中华门，行止雨花台。这么多还不够我体验，晚上又补足了第一天遗憾的玄武湖，还去了夜幕下的三山街熙南里，新街口商圈，论充实，这一天的容量比起考研的长途作战有过之无不及，我也好久没有来过这样的高强度旅行了，导致我写文章这会儿只能像一张煎饼一样在旅店中摊开来任人宰割，甚至因为太累了，害怕泡澡堂子散了紧绷的神，最后没去。 一路上都是人文景观，而绝美的风景并非很多，触动内心的更少。南京还在雨中，紫金山烟雾蒸腾。雨让天地模糊了界限，这混沌让我有些烦躁，可是微雨再不增添更多烦恼之上，把凝重和游人从景区一同抹去，留给我肆意想象和发挥的空间。在过年前的下雨的工作日游览南京，或许这体验只有现在才可能得到。这个季节的钟山树林红褐色与深绿色交融，错综的枝桠遮天蔽日，只有登高临台才有如此疏朗的开阔视野。但真正居高的时刻，我又没了高屋建瓴的气势，变得不知所向，无所适从。年轻时候，爱登高楼，待我再成熟一些，怕是就失去登楼心劲了。现在总觉得高楼如家乡这般，珍惜之处想去可是又不敢去，顺心和忧愁总是相伴而生，这就是爱吧。 颜真卿的碑林和刘勰文心雕龙纪念馆在钟山一角，本就与帝王伟人陵寝格格不入，但更出格的是一座无人的小园子还夹在两个建筑物之间，毫无征兆地撞出来，园虽小，但阁廊水木应有尽有，明明修在一个平面上，竟也有小曲折和错落的层次感，可能南京的园子有难名且优雅的修葺之道吧。我很喜欢这个没有名字的园子，更多是情景之喜，先前的凝重的地气在此行不通，进去出来竟然变成一园的清淡芳香，有如解油化腻的饭后小汤，沁人心脾。 其实我对园林毫无体验，来之前甚至只有一些北方园子的印象。观赏中若觉得某处入眼甚是契合我美感，凭空而爱，毫无理由，因此不好意思多说。看了瞻园，层层叠叠，钦佩和沉浸感无以复加，惬意凉亭。只是零落季节的园子，竟都有此等魅力，引得我反复穿行，寻找角度，左看右看，去留频繁。园子的窗门给出无限空间、亭廊自如穿梭带给人想置身其中的欲望、山水花草有情有据铺陈摆放十分讲究，惊为天作。雕梁画栋、星星灯火，与这季节的残花败柳，相映成趣，爱至深，盖我已成园内人。但这种“同化”有例外，我在园中发现了一只猫，在雨中漫步，愁眉苦脸，仿佛并不开心。我笨拙地学了几声猫叫，企图诱骗，反倒逼它使出浑身解数，飞檐走壁消失在楼台间。或许我不会骗人，更不会骗猫。我的体会就连我自己都不能完全把握，异想天开地企图把喜悦传递给猫猫，简直是乐昏了头。不过有趣的是，察觉到自己似乎“很快乐”的我自己，就接着认真地快乐起来。 猫猫是有灵气的，每一只猫都有，于是我喜欢看猫。人不一定，但也有灵气。昨天在小九华山的一条幽径中，一直黑猫欣然路过，丝毫不在意一旁赶路的我，瞬间隐匿，留我略略遗憾。今天晚上玄武湖的湖中小洲上也遇见了小黑猫，自顾自躲雨，看我几眼，又故作高冷，转身翻窗离去，似乎还被窗子卡了几秒钟。我本来想帮它，但觉得猫猫灵性至此，我一介庸人又能改变什么呢，或许不论它能不能穿过那条窗缝，我都不该出手才是。 可是，猫猫不改本性，我的认知上限却是容易被改变的。神道漫长，上山路上遇见一位高人，内力颇深，一曲《青藏高原》响彻整条路，我们身形交错瞬间，竟有种被浩然之气浇透打穿的感觉。我很少为了看人而回头，但这次如同一定要确认什么似的回了头。我也趁没人唱了两句，除了遭了报应绊了一跤差点滑倒，好像没有什么场地的加成。我猜想可能有两个原因：一是我不该放浪形骸，陕西人有时邪，说曹操曹操就到，陵寝高歌还是不好；二是我的唱歌内力太稀薄，遇见高手甚至会反噬自己，平时多是献丑。 其实，这一趟转悠下来，我也不知自己真正喜欢上了南京的什么，也不知道自己不喜欢什么。心中也像下了一场雨，界线模糊了。但明显的一点是，原本我不喜欢或者丝毫不感兴趣的东西，我开始尝试了，甚至会用心找它的特点。就比如我把主打甜味的梅花糕，形容为嬉笑怒骂，我自以为完全合理。粘和脆，苦和甜，硬和软，彩和白，一对对矛盾交织，共同构成了这一口梅花糕。或许真应该用“嬉笑怒骂”来形容，着实难忘。松鼠觉得不该把外壳做糊掉，糊掉生出多余苦味，可是那样的甜可能也单调，而且恐怕也只有高明的店家才能做到，并非是人们常常吃到的那种，少了一种地气吧。 吃的事之后专门写一篇。很晚了，休息。明天还要拼一拼，不过早上可以慢悠悠吃个早茶。人总是忙碌的，就拿我来说，担心的事有很多，我还悬着一颗梦想抢改签票的心，现在还担心松鼠的身体，坏的发展都是受罪。我不想松鼠受罪，我也不想受罪，可是有时候确实得为了所爱的什么拼一把。人是成长的，有些人走不了那么多路，可是热情驱使着，就会毫无征兆地焕发出超越身体机能的力量。 17日 旅行日志今天去总统府，天下文枢坊，莫愁湖，奥体公园，还有大屠杀纪念馆，为南京之游收了官。每一处都有独特的感官体验。吃得好，玩得好，虽然走马观花，但也算肤浅到极致，极尽三天之能事，算是不虚此行。每每看到它新的面貌，有时是市井平常，有时是青春自由，有时是工作辛劳，不管什么样子，当我更加了解南京时，就又多几分喜爱。 从来没有高强度这么拼命地走过路，我很少如此用痛感苦感来强记某些旅程，但若旅途中带着一些苦痛和不顺利，那此行也会愈加刻骨。整条路也多亏只有我一人，我可以随心所欲走南闯北，当然也因为计划不周耽误了很多时间。所幸，南京是个交通便利的城市，用不着我在路上费劲，就能轻松地兜转访寻。限制自己的只有昨天透支的腿，让今天确实无力实现出格的想法，脑子都不灵光了许多。若说前两天在旅行，今天却像是一种修炼和补救错误。 这次出游，没做什么攻略，没写什么太长的安排，每次游览景点，也只是按照一个深度随心逛下去。也不知道有什么美食和根源，只好误打误撞，有时现场搜索，或蹭蹭别团的导游。狼狈不堪。今天也萌发了一种无力感，起因于我偶然听到总统府中的一位导游讲起府门前八字台阶和某“森”楼奥秘，我才察觉自己的充实只是肤浅的充实，有些事有些来由根本不是我一个人能查到或者体会到的。除了对先前的自满自惭形秽，我也更加喜爱南京这座城。 其实和松鼠聊过，但是免不了再提一嘴。在我眼里，南京无疑是一座“城”，有声有色，有古有今。但我不曾了解南京的那部分，却在我心底呼之欲出，而那一部分更像是一座“市”。人们日出而作日落而息，每个人都寻着一份事儿做。我独潇洒，逆流而动，这些天还真无人与我争，可能是因为天寒又下雨，还赶上年前大家都忙碌，基本都是独享美景，宛如包场。吃东西也一样，我作为半个吃货，常苦于选择困难，很少能搏得最好吃的头彩。但尽管这样，我还是被南京本地美食所征服，足见南京美食同文化一样博大精深。 其实，我总在渴望从不同的角度认识南京，渴望从最真实又最深刻的一面认识人事物。这次走马观花，我抱着能触碰这座城市除了历史以外的全部表象的另类的渴望，我想融入这里早上的忙碌生活，想度过不眠的灯火秦淮，体会拥堵和顺畅，看遍美食与美人，我喜欢更平常的南京，而非更“南京”的南京。或许这些微妙的差别只有等我再咀嚼时，才会体会。不过我想，这次快速游览，大概并非毫无意义，对我来说这算是已经铺好纸笔，只待我闲暇时再去看两眼，便能描画挥洒了。 说了许多大话空话客套话，可是没学会最想了解的南京话。当然，就算学会，我也不敢在老南京们面前班门弄斧，最多调侃一下松鼠。但是一点可以肯定，南京人的普通话不怎么好，至少这一路我时刻注意着，和我对话的人，讲普通话的不超过一手之数，大多数普通市民都会带着点南京的口音。通常，本地人对本地方言都有些特别的情节，我听了几天，虽然有些难名的词根和语缀记了几个，南京话虽然听着像北方话，却抛去了很多太刚坚的词，剩下一些令我摸不着头脑的婉转语调，以及一种鞭辟入里的直爽，和一种说完就抛的洒脱。出口成脏是常事，有人确实喜欢拿腔拿调还吹胡子瞪眼，只是听者不必钻牛角尖，应该用脏字还一口回去才是正解。看他们聊得火热，我感叹语言排外的严重，初来乍到完全假装不了本地人，可能也因此没能混进想看的学校。不管如何，南京话稍稍让我有些羡慕。 想到今天又去夫子庙，这次奔着几个文化展馆去的，个中展馆确实值得玩味。再回味前两次来这里，总觉得看到的秦淮灯火只是一种虚假的繁华。有趣的小玩意琳琅满目，可我不屑一顾；当地独特的美食飘香穿巷，可我吃过后便无心驻足。然而我偏爱这里的砖瓷碑匾，飞檐叠壁，亭台廊阁，我深醉这里的兵戈祸中，天下文枢，古都气韵。这一城氤氲于书香食色，融化于绵山长水，烙刻于时代纵横，这样的城，只南京一座。 你们又要说，去过南京就开始吹南京，是不是收了钱奉了旨，屁颠屁颠地傻乐呵。南京是个普通城市，也有着普通的一面，那是每个城市都有的市井市侩的一面。主干道上还在修建便民交通，拥堵是不可避免的，这座城市还在发展之中，还在成长之中，还好相见不晚，未来可期。 我眼中的平凡事也多，有取纸巾铲屎的路人老奶奶，有骂骂咧咧却又热心的公交司机，有高唱分手快乐的醉酒小情侣当街大戏，有书包加身的名校学子匆忙赶路尽显惫色，有早点摊、总统府、瞻园、玄武湖和小九华数不清的悠闲猫猫，有不喜欢戴手表手机的晨练大爷和急着下班地铁引导员，有追出五十米把落下东西送还的餐厅服务员，有无心逛街还等在商场门口的寂寞小男友，有快下班了还特意为我重启设施的景点管理员，有好心过问冷暖的酒店前台，有不给好脸色的公事公办学校门卫，有飞行在街巷的电动车手，有遛狗带娃不栓绳的小阿姨，有酒吧街用生命力高谈阔论的落魄青年，有串摊指点附近美食攻略的温柔小哥……明明只有三天，我遇见的人们比我想象中的要更多更丰富。不管事由如何，这一切的人一起组成了我眼中的南京市民，正是他们生活在这片城市中，带给这座城市无限生命力。刷新了我对南方人和南京人的认识。 你若问他们怎么看我，我觉得像是在看一只外地来的金丝猴：操着听不出籍贯的口音和生得一副独特的面貌身材，带着目中无人怼天怼地的气魄，三步并一步飞快穿行的孤独旅人。外地人来南京，南京人都无比自豪，开始话六朝古都，十洲云水。作为半个西安人，我对历史有莫名的感情，可惜这次没去成最为推崇的南京博物院，是最大的遗憾了，但也不算遗憾，反正走马观花看博物馆，那才是真的浪费时间。 之后再专门写点美食吧，在此不做赘述。我喜欢南京。 （未完待续，还有春运和美食没写qwq） 附录：美食记录15晨桂花糖芋苗赤豆元宵鸡汁汤包鸡汁回卤干 15午慈悲素鹅面沉鱼落雁（松鼠素鱼）煮干丝 15晚清炖狮子头江南水乡一桶仙蟹黄汤包香干马兰头清炒芦蒿肥肠臭豆腐砂锅千层油糕桂花拉糕糯米糖藕 16日晨油条烧麦南瓜红枣粥 16晚张三生煎+垃圾锅贴梅花糕 17早酥饼鸭血粉丝汤 17午喜妞炸串 鸭肠晨祥记牛肉锅贴桂花双皮奶酱汁鸭肉卷奇异果青麦汁芋圆bobo鲜奶茶 17晚香辣牛肉锅盔酱汁烤鸭]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件初探|matlab之空间计量]]></title>
    <url>%2F2019%2F12%2F30%2F191230%E7%A9%BA%E9%97%B4%E8%AE%A1%E9%87%8F%2F</url>
    <content type="text"><![CDATA[最后修改日期：191231，18:31 事由松鼠有难，正派学长自然要拿出一点正派学长的样子。从零开始自学matlab，写写作业，不过如此。好啦，话不多说爱不多示，进入正题吧。 一、介绍说明数据为1978年-2011年华东七城市（上海、山东、安徽、江苏、浙江、江西、福建）各省区的GDP（GDP）、人口（RENKOU）、财政收入（GSH）、财政支出（GZH）、投资（TZ）、消费（XF）、进口（JK）。（氦核：别问我为什么起这些low爆的变量名，某松鼠的老师起的 以GDP为因变量，其他六个变量为解释变量，详细情况见表1。 表1 变量介绍表 变量名 含义 变量水平 单位 GDP 地区生产总值 连续型 亿元 RENKOU 人口 连续型 万人 GSH 财政收入 连续型 亿元 GZH 财政支出 连续型 亿元 TZ 投资 连续型 亿元 XF 消费 连续型 亿元 JK 进出口 连续型 亿元 利用matlab软件构造空间自回归模型、空间误差模型、空间杜宾模型和空间面板数据模型。其中空间自回归模型、空间误差模型、空间杜宾模型均使用2017年数据，空间面板数据模型使用40年全部数据构造。（氦核：切莫手滑一上来就panel函数。。。空间面板数据模型和空间xx模型有微妙的区别。血的教训 基于邻域关系的空间权重矩阵weight1由“华东省区公路邻域.xlsx”得出，基于距离关系的空间权重矩阵weight1由“华东各省会城市的距离”得出，基于GDP差距的经济距离关系的空间权重矩阵由“华东2014年GDP计算的经济距离”得出。综合权重矩阵取a=b=c=1/3（毫无梦想和灵魂的取值）。最后利用matlab构造出模型并进行检验。（氦核：文件在文末附件，你们看看我多好 注1：（1）空间自回归模型：y=ρWy+βX+μ（2）空间误差模型：y=βX+μ μ=ρWμ+ε（3）空间杜宾模型：y=ρW_1 y+Xβ_1+W_2 Xβ_2+μ（4）空间面板数据模型：y_it=ρWy_it+βX_it+μ_it 注2：空间权重矩阵的选择，要有基于邻域关系的空间权重矩阵，基于距离关系的空间权重矩阵，基于某个经济指标的经济距离关系的空间权重矩阵。提倡构建组合空间权重矩阵W=aW_1+(1-a)W_2（0＜a＜1）；或 W=aW_1+bW_2+ cW_3（a+b+c=1）。 二、代码贴代码之前，先说几个坑。 sar函数，sem函数，sdm函数，一定不能做多年的数据，只能使用一年数据，否则报错。（氦核：不知道谁没搞清楚原理就开始写代码了呢。。。 松鼠的老师还是有意思，自己派人写了个有趣的包：fanzhuan（将上/下三角矩阵变成对称矩阵），里面还有几个对空间权重矩阵做标准化的函数，不想找的话normw()也可以，具体内容实在不想看了。 这次用到的方法，是空间计量，对于我来说很新奇，同时觉得很有趣。matlab空间计量需要最关键的jplv7工具包，下载安装这里也略过。 准备结束，下来就要开工了。这次代码注释写的很详细，原因不表，读者心领神会即可。 工作开始12345678910111213141516171819202122clear all;% 工作路径userpath(&apos;C:\Users\Surface\Desktop&apos;); % 读取数据[data,name] = xlsread(&apos;data&apos;,1); % 原数据为原始面板数据，需要转化为空间面板数据% 读取几个权重矩阵weight1 = xlsread(&apos;华东各省会城市距离&apos;,1); % 基于距离关系的空间权重矩阵weight2 = xlsread(&apos;华东省区公路邻域&apos;,1); % 基于邻域关系的空间权重矩阵weight3t = xlsread(&apos;华东2014GDP计算的经济距离&apos;,1); % 基于GDP的经济距离关系的空间权重矩阵weight3 = fanzhuan(weight3t); % 将上三角矩阵反转为实对称矩阵，需要fanzhuan包% 空间权重标准化,函数需要fanzhuan包W1 = hbzh(weight1);W2 = lyhbzh(weight2);W3 = hbzh(weight3);W = 1/3*W1+1/3*W2+1/3*W3 % 加权构建复合矩阵% 问题的维度T = 40; % 时期数量 N = 7; % 地区数量 最最重要的一个地方来了，要使用空间计量分析函数，就要明白其数据结构。 调整数据结构空间面板数据结构时间1 个体1 x1 x2 x3时间1 个体2 x1 x2 x3时间1 个体3 x1 x2 x3……时间2 个体1 x1 x2 x3时间2 个体2 x1 x2 x3时间2 个体3 x1 x2 x3……时间T 个体1 x1 x2 x3时间T 个体2 x1 x2 x3时间T 个体3 x1 x2 x3 原始的面板结构如下，需要变换调整： 个体1 个体2 个体3 个体4 时间1 x_{11}^{(1)},…… x_{1k}^{(1)} x_{11}^{(2)},…… x_{1k}^{(2)} x_{11}^{(3)},…… x_{1k}^{(3)} x_{11}^{(4)},…… x_{1k}^{(4)} 时间2 x_{21}^{(1)},…… x_{2k}^{(1)} x_{21}^{(2)},…… x_{2k}^{(2)} x_{21}^{(3)},…… x_{2k}^{(3)} x_{21}^{(4)},…… x_{2k}^{(4)} 时间3 x_{31}^{(1)},…… x_{3k}^{(1)} x_{31}^{(2)},…… x_{3k}^{(2)} x_{31}^{(3)},…… x_{3k}^{(3)} x_{31}^{(4)},…… x_{3k}^{(4)} 时间4 x_{41}^{(1)},…… x_{4k}^{(1)} x_{41}^{(2)},…… x_{4k}^{(2)} x_{41}^{(3)},…… x_{4k}^{(3)} x_{41}^{(4)},…… x_{4k}^{(4)} …… …… …… …… …… 1234567891011121314151617% 调整数据结构,从原始数据转化为空间面板数据temp = zeros(T,7); % 构建存储面板数据的0矩阵for i = 1:T % 第i年至第40年 shh = data(i,[2:8]); % SHH上海当年的全部数据 shd = data(i,[9:15]); % SHD山东当年的全部数据 ah = data(i,[16:22]); % AH 安徽当年的全部数据 js = data(i,[23:29]); % JS 江苏当年的全部数据 zhj = data(i,[30:36]); % ZHJ浙江当年的全部数据 jx = data(i,[37:43]); % JX 江西当年的全部数据 fj = data(i,[44:50]); % FJ 福建当年的全部数据 temp&#123;(7*i-6):(7*i),:) = vertcat(shh,shd,ah,js,zhj,jx,fj); % 将每一年的数据贴进准备好的0矩阵 if i&gt;40 break endend 完事了，建模。先做个简单最小二乘法的回归对比一哈。 OLS回归12345678910111213141516% 0 OLS模型% 提取变量y = temp(:,[1]); % 华东各地区GDPx_ols = temp(:,[2:7]); % 变量名vnames=strvcat(&apos;gdp&apos;,&apos;renkou&apos;,&apos;gsh&apos;,&apos;gzh&apos;,&apos;tz&apos;,&apos;xf&apos;,&apos;jk&apos;); % 因变量：GDP（亿元），自变量：人口(万人)，财政收入（亿元），财政支出（亿元），投资（亿元），消费（亿元），进出口（亿元）% 建模results_ols = ols(y,x_ols); % 进行最小二乘法回归prt_reg(results_ols,vnames,1); I = eye(T);W1 = kron(I,W);res = moran(y,x_ols,W1); % moran 检验prt(res); %moran值较大，在0.01显著性水平下，依据空间分布的特征显著 做完空间相关性检验后，一切才刚刚开始。空间相关性检验一般就moran和LR、LM检验。都有现成的函数，适合新手。 空间计量模型SAR,SEM,SDM1234567891011121314151617181920212223242526272829303132333435363738394041424344454647% ---------------------------------------------------------------------% 1 SAR模型% 提取变量：2017年数据y = temp(274:280,[1]); x_sar = temp(274:280,[2,3,4,5,6,7]); % 因变量：GDP（亿元），自变量：人口(万人)，财政收入（亿元），财政支出（亿元），投资（亿元），消费（亿元），进出口（亿元）% 建模results_sar = sar(y,x_sar,W);prt(results_sar,vnames);results_sar.tstat % t检验统计量的值results_sar.rho % 空间自回归系数rhoresults_sar.beta % 模型估计的beta值% ---------------------------------------------------------------------% 2 SEM模型% y = beta*x + mu ; mu = rho*W*mu + e % 选择变量：2017年数据y = temp(274:280,[1]); x_sem = temp(274:280,[2:7]); results_sem = sem(y,x_sem,W);prt(results_sem,vnames);results_sem.tstat % 渐进t检验统计量的值(最后输入为rho空间自回归系数)results_sem.rho % (p above)results_sem.beta % 模型估计的beta值% ---------------------------------------------------------------------% 3 SDM空间杜宾模型% y = XB + u (optional) + v (optional) + s, s = p*W*s + e% 选择变量:2017年数据y = temp(274:280,[1]); x_sdm = temp(274:280,[2:4]); % 空间相关性影响变量vnames1=strvcat(&apos;gdp&apos;,&apos;renkou&apos;,&apos;gsh&apos;,&apos;gzh&apos;); % 建模results_sdm = sdm(y,x_sdm,W); prt(results_sdm,vnames1);results_sdm.tstat % t检验统计量的值results_sdm.rho % 最后输入为rho空间自回归系数results_sdm.beta % 模型估计的beta值 以上是三个一年数据的结果，下面是面板数据模型，平时网上搜到的基本都是面板数据模型。 工具箱目录jplv7→spatial→panel下有一个演示程序：demopanelscompare好像是随机效应与混合模型的选择，使用的方法是LR检验。 空间面板数据模型真没什么好说的。要说的话只能怪罪LMsarsem_panel函数太猛了。 1234567891011121314151617181920212223% 4 空间面板数据模型% 4.1 空间相关性检验%LM检验，大于6.635拒绝原假设（不存在空间自相关），即存在误差项空间自相关LMsarsem_panel(results_sar,W,y,x_sar) % 结果表明0.1显著性水平下，空间固定效应LR检验通过，空间固定效应的空间滞后的LM检验通过% 时间固定效应的的空间误差的LM检验不通过,时间固定效应的LR检验不通过% 故选择时间效应固定模型info.model=2info.model=2;% 4.2 提取变量：30年数据y = temp(:,[1]); x_sar = temp(:,[2,3,4,5,6,7]); % 因变量：GDP（亿元），自变量：人口(万人)，财政收入（亿元），财政支出（亿元），投资（亿元），消费（亿元），进出口（亿元）% 4.3 建模results_general = sar_panel_FE(y,x_sar,W,T,info); prt_spnew(results_general,vnames);results_general.corr2 % 模型的拟合优度调整的r方results_general.tstat % t检验统计量的值results_general.rho % 最后输入为rho空间自回归系数results_general.beta % 模型估计的beta值 info.model参数是设定模型是某种固定效应：info.model=0：表示此模型为混合模型，即没有固定效应；info.model=1：表示此模型为地区固定效应模型；info.model=2：表示时间固定效应模型；info.model=3：表示双向固定效应模型。 经验小总结help+函数名这个指令太有用了，任何软件的帮助文档都是学软件的最好老师。 matlab有个现象，你不加句末的分号，就会输出在下面。我一开始循环没有加分号，每次运行都很慢。 不知道为什么，他们读“马特拉布”，难道是北美口音？不得而知。 虽然没有系统学习过matlab，但是这次借着任务也算好好认识了这款传说中的软件，点一下，玩一年，help不花一分钱。真心感觉全部的语言都有相通之处。不过matlab在做统计的问题时，我总是觉得用着不趁手，不如真正的统计软件，输出结果也乱糟糟的。或许这就是没有把好钢用在刀刃上吧。 结果不贴了，有兴趣的人们可以跑跑看。数据传送门 松鼠作业一定很优秀了（老父亲笑容） （后补，松鼠观罢：烦死了！） （完）]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|名字]]></title>
    <url>%2F2019%2F09%2F18%2F190918%E5%90%8D%E5%AD%97%2F</url>
    <content type="text"><![CDATA[名字氦核 190918名字对于人来说，是十分珍贵的东西。当我们喜欢上某人某事某物时，往往就会给他起上一个爱称。当然笔名也是如此。我也有过很多称呼，有时爱了叫禾禾，有时不爱了叫呵呵，以外观特点取我名的倒没多少，可能是名字比起外貌更加容易记，谁的名字还没有个谐音之梗呢？我倒是不在意，好记就行了。 我曾经称呼自己单行道子，乍看不是人名，但事出有因。当年看了一部对我废话能力影响无穷的动漫，深爱其中某位一流恶党的“彻底”作风，于是在注册贴吧之时就起下此名，希望自己在单行道上一路前行。那时愿望单纯，希望自己在人生路上要永远能够前进，不要后退。变强，那时单纯而由衷的心愿，藏在这个名字中伴随我走了三年。倒是现在，提起这个名字就会掩面而笑，好似回忆起一段不愿意认真审视的二货黑历史。 后来上高中，视野开阔，脑洞清奇，偏爱理科。听说冥冥之中有一种名为“氦核”的物质，在粒子中力大无比，电子板上轨迹粗壮，单纯而专一，会影响周围而不受电子太多干扰，能够冲击未知带来新发现，种种特性，颇合我心。加上谐音近似我的昵称，不假思索就把它当作了名字。每每有人问起笔名何意，我便莞尔。多年来也无人猜透其中深意，只是认为不过非主流个性，正反约等于氢弹或者核弹。现在也有以我祖母为首直呼我为王源的，我有时会觉得什么真实的东西被掩盖了，但是名字本就是他人用来称呼自己的，对大家来说好记就行。 说到“氦核”，就不得不提封笔之事，前些日子写歌词的时候又犯了曾经纠正过的毛病：笔下虚浮，空无一物。自觉填词生命耗尽，于是歇了笔耕，改完最后之作后不再写词。虽然看见好词仍会心头悸动，可我知道，不够努力是无法超越某些界限的，不够安静也看不见那些词。明明情感丰富，却摘不出一句能说出口，这也是由于我读书太少、想的太多、说的太多。 近日我突然想，有没有什么新名字更适合现在得我，毕竟成长了一些，不会把“我要变强”的字眼放在嘴边，待世界也更加温柔，原来那希望发光发热的名字貌似不太适合我现在佛系的心。便寻机求问《易经》，占得一卦泽天夬，夬者，决也。懂的朋友应该已经懂了，不懂的朋友现在肯定是不懂。其实改不改名字，答案我心里早都明白，只不过又让我坚定了一个目标，厚积薄发，该发就发。天不生氦核，屁话界万古如长夜。 关于名字还有一件有趣的事。起一个好记的名字真的挺重要，但碰巧会被大家都记住的那种，偶尔会产生奇妙的化学反应。母亲的朋友是个老师，在我小学时候经常去他的办公室玩耍。有一天去了，他正在批评一个大哥哥，应该是他的学生，但口中全都是“胡锦涛，你怎么没写作业……胡锦涛怎么又旷课……”我觉着疑惑，觉得像是模仿新闻联播。后来听他讲，原来受批评的学生名叫胡锦涛，那人自号“西北猛汉”，据说某次受罚还一口气做了两百个俯卧撑，属实猛男。他还说，每次教育那个学生时，都有一种奇妙的感觉，仿佛自己是为了某种巨大存在的命运而从事了教育。我当时听了觉得可笑，只是名字相仿罢了。后来我听说了一个词，叫人类命运共同体，我觉得来形容这个应该差不多，反正名字看去相似，而且好记，就行了。 （完，纯属虚构）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|我的一个朋友]]></title>
    <url>%2F2019%2F08%2F03%2F190803%E6%88%91%E7%9A%84%E4%B8%80%E4%B8%AA%E6%9C%8B%E5%8F%8B%2F</url>
    <content type="text"><![CDATA[我的一个朋友氦核 190803写下这个标题，突然有些后悔，后悔自己似乎不好意思对大家讲这样的事。不过低头沉吟一刻又振作起来，反正要说的不是我的事，是我万能的朋友的故事，这样想来便又能好好说话了。 我这个朋友是个温柔的人，与其说温柔，不如说是闷骚，因为至少他看起来一脸凶相。他还算个性鲜明，名字平平凡凡却有浩瀚之格、显赫之气，念出来让你觉得西安城一半都是他家的；最显著的特点可能是他不太标准的普通话，经常是人未至而声可先辨之。他虽没做过惊天地泣鬼神的壮举，更谈不上有什么豪言壮语，不过活得潇洒。他偶尔当当墙头草二五仔，还算是有底线，否则我也不会成为他的朋友。总而言之我这位朋友是一位普通市民，包藏莫须有的祸心。 我们本来是同班同学，井水不犯河水，几乎没有交集。因为貌似憨厚可爱，他常被老师当作活跃气氛的调料，本人虽抱怨每每躺枪，却也习以为常，偶尔还引以为荣。我们结识于一场不怎么愉快的言论对撞，他的一段毫无立场可言的发言一度让我信服，可是后来渐渐对个中细节心生疑惑，便询问一二，结果年轻气盛擦枪走火，两人约好某年某月某日某时，李家村广场麦当劳后排雅座，当面辩论。我这位朋友虽说看似憨厚，但也是两百斤的猛男，胳膊弯起来比我大腿还粗，冲动答应了面谈之后，我只好叫上好友在旁掩护，情况不对便出手相救。结果到店虚惊一场，才知道这人深谙网战的无用，缺少形成交流所必要的要素，作为获得信息之处到也无妨，可显然无法加深感情。他并不讨厌我的刻薄（当时的确有些无理），有了倒戈卸甲以礼来降之意，虽然嘴上没有明说，但我们的关系从那时就建立起来。 当了朋友不要紧，这位朋友的世界虽然和我格格不入，不过也稍稍有些重合——都喜欢某一类作品。我们在那方面聊得甚多，不过他的爆炸发言我一般听之任之，不做太多理会。除过重合的世界，我们生活中剩下的都是难以相互理解的部分。我知道他对科幻文学如痴如醉，他当然也了解我的死宅面目。我请教他的少，但是他私下里常常会来讨教令他不解的死宅心性，在我看来简直不可理喻无法言明的，只能深夜躲在被窝里悄悄百度得那些羞耻问题，他也能面不改色地问出口来。我在掩面解释后，往往痛恨自己了解过多的如此没用死宅世界的知识。他了解了新的知识总会向我挑眉竖大拇指，然后痛骂“死宅真恶心”。我能猜到他的心情，但我也宽恕自己：被骂是被他逼的。 “放鸽子”指约好了却失约，而我这个朋友最喜欢鸽人。往往是约好的时间又过了一半个小时仍不见人影，久了也不想和他约什么局，但考虑到他又会大方请客请求我的原谅，只得耐心下来。这位朋友是个土豪，主动约我就大概率要请客。我们还实行过一段时间的“部分AA制”，即他请一部分，我们再平摊一部分，这样好像能减轻我白嫖的罪恶感。在他面前我会摇身一变成为一个小女人，只需要跟着他就有吃的，结果现在我却也有了他这样招摇撞骗的伎俩，约人百试不爽。不过我却不喜欢放鸽子，失约总是不好的，他每每提起这点就说我“只学到了功夫，没有内力”，我不以为然。 我们联系不多，既然是纯洁的君子之交，当然免不了慢慢淡去。虽说偶尔一起打个游戏，不过没了见面的机会，语音聊天的背后总是免不了产生距离感。他网上的言论越来越少，用他的话说叫做“闲人自作自受”，曾经网络键盘阵地的一把手，如今也高高挂起免战牌，一副“我不争，故天下莫能与我争”的阿Q心情，虽然可笑但也潇洒很多。我却依然喜欢争论，多少年本性难移。今天偶然想起他，却也是念着有人请客，但是其他人却怎么也想不起来，某种意义上他还是很成功的。 （完，本文纯属虚构） 后注：假期忙碌，内心空虚，学习之余，以文自娱。]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不务正业|近期收集的有趣小链接]]></title>
    <url>%2F2019%2F06%2F04%2F190604%E6%9C%89%E8%B6%A3%E5%B0%8F%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[不务正业|近期收集的有趣小链接神经病人思路广，沙雕网友欢乐多。 下面是氦核收集的有趣api，希望大家喜欢 NO.1 明日方舟公开招募计算器传送门 其实不仅仅是计算器，还包括升级花费计算，材料计算，干员表之类。热衷于收集图鉴的我自然不会放过这个有趣的小东西。希望再出几个高级资深tag啊！ NO.2 王斌对联AI传送门 这个ai曾经因为太 暴 力而遭到了封杀，今天，这个ai又回来了！支持逗号分句，建议少输入一些特有名词，毕竟会对出你想不到的爆句。 NO.3 瞎子也要搞人工智能——deepmind的星际争霸2项目传送门 使用接口是python（无力咆哮），有能力的猛男可以尝试引进训练一波，感受ai的游戏世界，内存消耗不大，主要是费电。 NO.4 steamspy游戏数据传送门 我觉得如果要研究价格变化，这个地方你不得不去看看。 NO.5 mikutap小游戏传送门 相信我，你会停不下来的，点一下，玩一年。]]></content>
      <tags>
        <tag>娱乐time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|某不科学的笔记总结（持续更新）]]></title>
    <url>%2F2019%2F05%2F17%2F190508%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9F%90%E4%B8%8D%E7%A7%91%E5%AD%A6%E7%9A%84%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[数据挖掘|某不科学的笔记总结（持续更新）记录：氦核 最后编辑时间：20190514 文章主要参考马景义老师的数据挖掘教学，与刘苗老师相应课件。笔记中间夹杂很多个人思考与经验，如有错误，请在下方评论区指出，欢迎讨论。 主要算法一览基于乔利斯基分解的逐步回归 传送门偏差方差分解&amp;五个模型评价相关指标 传送门最优子集回归&amp;最小角度回归 传送门基于lasso的LARSN 传送门 决策树 传送门 adaboost算法介绍（含详细权重解释） 传送门 聚类分析、EM算法、数据爬取（py）暂缺 最近要写数据分析报告，笔记补充较晚，见谅（根本就没人看吧喂！）]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|adaboost原理]]></title>
    <url>%2F2019%2F05%2F14%2F190514adaboost%2F</url>
    <content type="text"><![CDATA[adaboost原理（包含权重详细解释）参考网页 现阶段流行的boosting算法有adaboost，XGBboost，不要求对数据有什么假定，通过迭代不断完善对模型的建设，是非参数方向的升华，一定程度上解决了高维灾难。 最后更新时间：190514/23:31 1.1 Adaboost是什么AdaBoost，是英文”Adaptive Boosting”（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 具体说来，整个Adaboost 迭代算法就3步： 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器（也叫做基分类器）。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 1.2 Adaboost算法流程给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，其中实例$x\in X$，而实例空间$X\subset \R^n​$ ，yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。 Adaboost的算法流程如下： 步骤1. 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。 D_1 = (w_{11},w_{12},w_{13},...,w_{1N}), w_{1i} = \frac{1}{N}, i = 1,2,...,N 步骤2. 进行多轮迭代，用m = 1,2, …, M表示迭代的第多少轮 a. 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）： G_m(x): \chi -> {-1,+1}会得到原始的和预测的y，+1，-1。 b. 计算Gm(x)在训练数据集上的分类误差率 e_m = P(G_m(x_i)≠y_i) = \sum_{i=1}^{N}I(G_m(x_i) ≠ y_i) \tag{误差率}这是一个错分情况。 由上述式子可知，Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和 c. 计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）： \alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}​注：$\alpha_m​$是一棵树的权重，直接根据每棵树的错分情况来的。 由上述式子可知，$e_m \leq 1/2​$时，am &gt;= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 d. 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代 D_1 = (w_{m+1,1},w_{m+1,2},w_{m+1,3},...,w_{m+1,N}),​w_{m+1,i} = \frac{w_{m,i}}{Z_m}exp(-\alpha_m\gamma_iG_m(x_i)), i = 1,2,...,N \tag{权值更新公式}这是一个指数损失$w_{1i}$,$Z_m$是在做规范化。 使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。 其中，$Z_m$是规范化因子，使得$D_{m+1}$成为一个概率分布： Z_m = \sum_{i=1}^{N}exp(-\alpha_my_iG_m(x_i)) \tag{规范化因子} 步骤3. 组合各个弱分类器 f(x) = \sum_{m=1}^{M}\alpha_m G_m(x)​注：分类对应投票，组合对应回归。 从而得到最终分类器，如下： G(x) = sign(f(x)) = sign(\sum_{m=1}^{M}\alpha_mG_m(x)) ​如果概念模型很抽象，那么来看一个例子吧。 1.3 Adaboost的一个例子下面，给定下列训练样本，请用AdaBoost算法学习一个强分类器。(二分类问题) 序号 1 2 3 4 5 6 7 8 9 X X 0 1 2 3 4 5 6 7 8 9 Y 1 1 1 -1 -1 -1 1 1 1 -1 求解过程：初始化训练数据的权值分布，令每个权值$W_{1i} = \frac{1}{N} = 0.1​$，其中，N = 10，i = 1,2, …, 10，然后分别对于m = 1,2,3, …等值进行迭代。 拿到这10个数据的训练样本后，根据 X 和 Y 的对应关系，要把这10个数据分为两类，一类是“1”，一类是“-1”，根据数据的特点发现：“0 1 2”这3个数据对应的类是“1”，“3 4 5”这3个数据对应的类是“-1”，“6 7 8”这3个数据对应的类是“1”，9是比较孤独的，对应类“-1”。抛开孤独的9不讲，“0 1 2”、“3 4 5”、“6 7 8”这是3类不同的数据，分别对应的类是1、-1、1，直观上推测可知，可以找到对应的数据分界点，比如2.5、5.5、8.5 将那几类数据分成两类。当然，这只是主观臆测，下面实际计算下这个具体过程。 迭代过程1 对于m=1，在权值分布为D1（10个数据，每个数据的权值皆初始化为0.1）的训练数据上，经过计算可得： 阈值v取2.5时误差率为0.3（x &lt; 2.5时取1，x &gt; 2.5时取-1，则6 7 8分错，误差率为0.3）， 阈值v取5.5时误差率最低为0.4（x &lt; 5.5时取1，x &gt; 5.5时取-1，则3 4 5 6 7 8皆分错，误差率0.6大于0.5，不可取。故令x &gt; 5.5时取1，x &lt; 5.5时取-1，则0 1 2 9分错，误差率为0.4），注：判错概率较高 阈值v取8.5时误差率为0.3（x &lt; 8.5时取1，x &gt; 8.5时取-1，则3 4 5分错，误差率为0.3）。 可以看到，无论阈值v取2.5，还是8.5，总得分错3个样本，故可任取其中任意一个如2.5，弄成第一个基本分类器为： G_1(x) = \begin{cases} 1, & x < 2.5 \\ -1& x>2.5 \end{cases}​上面说阈值v取2.5时则6 7 8分错，所以误差率为0.3，更加详细的解释是：因为样本集中 0 1 2对应的类（Y）是1，因它们本身都小于2.5，所以被G1(x)分在了相应的类“1”中，分对了。 3 4 5本身对应的类（Y）是-1，因它们本身都大于2.5，所以被G1(x)分在了相应的类“-1”中，分对了。 但6 7 8本身对应类（Y）是1，却因它们本身大于2.5而被G1(x)分在了类”-1”中，所以这3个样本被分错了。 9本身对应的类（Y）是-1，因它本身大于2.5，所以被G1(x)分在了相应的类“-1”中，分对了。 从而得到G1(x)在训练数据集上的误差率（被G1(x)误分类样本“6 7 8”的权值之和）e1=P(G1(xi)≠yi) = 3*0.1 = 0.3。 然后根据误差率e1计算G1的系数： \alpha_1 = \frac{1}{2} log\frac{1-e_1}{e_1} = 0.4236​这个a1代表G1(x)在最终的分类函数中所占的权重（这颗树的权重），为0.4236。接着更新训练数据的权值分布，用于下一轮迭代： D_1 = (w_{m+1,1},w_{m+1,2},w_{m+1,3},...,w_{m+1,N}),​w_{m+1,i} = \frac{w_{m,i}}{Z_m}exp(-\alpha_m\gamma_iG_m(x_i)), i = 1,2,...,N​（注：原文上一个公式开头是$w_{m+i}​$，疑似写错） 值得一提的是，由权值更新的公式可知，每个样本的新权值是变大还是变小，取决于它是被分错还是被分正确。 即如果某个样本被分错了，则yi Gm(xi)为负，负负得正，结果使得整个式子变大（样本权值变大），否则变小。注：简单地说，上一轮判错，权重则增大* 第一轮迭代后，最后得到各个数据新的权值分布D2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715,0.1666, 0.1666, 0.1666, 0.0715)。由此可以看出，因为样本中是数据“6 7 8”被G1(x)分错了，所以它们的权值由之前的0.1增大到0.1666，反之，其它数据皆被分正确，所以它们的权值皆由之前的0.1减小到0.0715。 分类函数f1(x)= a1*G1(x) = 0.4236G1(x)​. 此时，得到的第一个基本分类器sign(f1(x))在训练数据集上有3个误分类点（即6 7 8）。 从上述第一轮的整个迭代过程可以看出：被误分类样本的权值之和影响误差率，误差率影响基本分类器在最终分类器中所占的权重。 迭代过程2 对于m=2，在权值分布为D2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)的训练数据上，经过计算可得： 阈值v取2.5时误差率为0.16663（x &lt; 2.5时取1，x &gt; 2.5时取-1，则6 7 8分错，误差率为0.16663）， 阈值v取5.5时误差率最低为0.07154（x &gt; 5.5时取1，x &lt; 5.5时取-1，则0 1 2 9分错，误差率为0.07153 + 0.0715）， 阈值v取8.5时误差率为0.07153（x &lt; 8.5时取1，x &gt; 8.5时取-1，则3 4 5分错，误差率为0.07153）。 所以，阈值v取8.5时误差率最低，故第二个基本分类器为： G_2(x) = \begin{cases} 1, & x < 8.5 \\ -1& x>8.5 \end{cases}​面对的还是下述样本： 序号 1 2 3 4 5 6 7 8 9 X X 0 1 2 3 4 5 6 7 8 9 Y 1 1 1 -1 -1 -1 1 1 1 -1 很明显，G2(x)把样本“3 4 5”分错了，根据D2可知它们的权值为0.0715, 0.0715, 0.0715，所以G2(x)在训练数据集上的误差率e2=P(G2(xi)≠yi) = 0.0715 * 3 = 0.2143。 计算G2的系数： \alpha_2 = \frac{1}{2} log\frac{1-e_2}{e_2} = 0.6496​更新训练数据的权值分布： D_{m+1} = (w_{m+1,1},w_{m+1,2},w_{m+1,3},...,w_{m+1,N}),​w_{m+i} = \frac{w_{mi}}{Z_m}exp(-\alpha_m\gamma_iG_m(x_i)), i = 1,2,...,ND3 = (0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.01667, 0.1060, 0.1060, 0.1060, 0.0455)。被分错的样本“3 4 5”的权值变大，其它被分对的样本的权值变小。f2(x)=0.4236G1(x) + 0.6496G2(x) 此时，得到的第二个基本分类器sign(f2(x))在训练数据集上有3个误分类点（即3 4 5）。 迭代过程3 对于m=3，在权值分布为D3 = (0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.01667, 0.1060, 0.1060, 0.1060, 0.0455)的训练数据上，经过计算可得： 阈值v取2.5时误差率为0.10603（x &lt; 2.5时取1，x &gt; 2.5时取-1，则6 7 8分错，误差率为0.10603）， 阈值v取5.5时误差率最低为0.04554（x &gt; 5.5时取1，x &lt; 5.5时取-1，则0 1 2 9分错，误差率为0.04553 + 0.0715）， 阈值v取8.5时误差率为0.16673（x &lt; 8.5时取1，x &gt; 8.5时取-1，则3 4 5分错，误差率为0.16673）。 所以阈值v取5.5时误差率最低，故第三个基本分类器为： G_3x) = \begin{cases} 1, & x < 5.5 \\ -1& x>5.5 \end{cases}面对的还是下述样本 依然还是原样本： 序号 1 2 3 4 5 6 7 8 9 X X 0 1 2 3 4 5 6 7 8 9 Y 1 1 1 -1 -1 -1 1 1 1 -1 此时，被误分类的样本是：0 1 2 9，这4个样本所对应的权值皆为0.0455， 所以G3(x)在训练数据集上的误差率e3 = P(G3(xi)≠yi) = 0.0455*4 = 0.1820。 计算G3的系数： \alpha_3 = \frac{1}{2} log\frac{1-e_3}{e_3} = 0.7514更新训练数据的权值分布： D_{m+1} = (w_{m+1,1},w_{m+1,2},w_{m+1,3},...,w_{m+1,N}),w_{m+i} = \frac{w_{mi}}{Z_m}exp(-\alpha_m\gamma_iG_m(x_i)), i = 1,2,...,ND4 = (0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)。被分错的样本“0 1 2 9”的权值变大，其它被分对的样本的权值变小。 f3(x)=0.4236G1(x) + 0.6496G2(x)+0.7514G3(x) 此时，得到的第三个基本分类器sign(f3(x))在训练数据集上有0个误分类点。至此，整个训练过程结束。 现在，咱们来总结下3轮迭代下来，各个样本权值和误差率的变化，如下所示（其中，样本权值D中加了下划线的表示在上一轮中被分错的样本的新权值）： 训练之前，各个样本的权值被初始化为D1 = (0.1, 0.1,0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)； 第一轮迭代中，样本“6 7 8”被分错，对应的误差率为e_1=P(G_1(x_i)≠y_i) = 3*0.1 = 0.3，此第一个基本分类器在最终的分类器中所占的权重为a_1 = 0.4236。第一轮迭代过后，样本新的权值为D_2 = (0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)；​ 第二轮迭代中，样本“3 4 5”被分错，对应的误差率为e_2=P(G_2(x_i)≠y_i) = 0.0715 * 3 = 0.2143，此第二个基本分类器在最终的分类器中所占的权重为a_2 = 0.6496。第二轮迭代过后，样本新的权值为D3 = (0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.01667, 0.1060, 0.1060, 0.1060, 0.0455)； 第三轮迭代中，样本“0 1 2 9”被分错，对应的误差率为e_3 = P(G_3(x_i)≠y_i) = 0.0455*4 = 0.1820，此第三个基本分类器在最终的分类器中所占的权重为a_3 = 0.7514。第三轮迭代过后，样本新的权值为D_4 = (0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)。 从上述过程中可以发现，如果某些个样本被分错，它们在下一轮迭代中的权值将被增大，反之，其它被分对的样本在下一轮迭代中的权值将被减小。就这样，分错样本权值增大，分对样本权值变小，而在下一轮迭代中，总是选取让误差率最低的阈值来设计基本分类器，所以误差率e（所有被Gm(x)误分类样本的权值之和）不断降低。 综上，将上面计算得到的a1、a2、a3各值代入G(x)中，G(x) = sign[f3(x)] = sign[ a1 * G1(x) + a2 * G2(x) + a3 * G3(x) ]​，得到最终的分类器为： G(x) = sign[f_3(x)] = sign[ 0.4236G_1(x) + 0.6496G_2(x)+0.7514G_3(x) ]。2 Adaboost的误差界(建议先学习第三部分)通过上面的例子可知，Adaboost在学习的过程中不断减少训练误差e，直到各个弱分类器组合成最终分类器，那这个最终分类器的误差界到底是多少呢？ 事实上，Adaboost 最终分类器的训练误差的上界为： \frac{1}{N} \sum_{i=1}^{N}I(G(x_i)≠y_i)≤\frac{1}{N} \sum_{i=1}^{N}exp(-y_if(x_i)) = \prod_{m-1}^{M}Z_m​注：$Z_m​$是将所有概率做归一化的那个因子 下面，咱们来通过推导来证明下上述式子。 当G(xi)≠yi时，yif(xi)&lt;0，因而exp(-yif(xi))≥1，因此前半部分得证。 关于后半部分，别忘了：（为下面的推导铺垫） w_{m+1,i} = \frac{w_{m,i}}{Z_m}exp(-\alpha_m\gamma_iG_m(x_i)), i = 1,2,...,N​Z_mw_{m+1,i} = w_{m,i}exp(-\alpha_m\gamma_iG_m(x_i))​整个的推导过程如下： \frac{1}{N}\sum_{i}exp(-\sum_{m=1}^{M}\alpha_m\gamma_iG_m(x_i))= w_{1i}\sum_{i}exp(-\sum_{m=1}^{M}\alpha_m\gamma_iG_m(x_i))$$ 注意：$\frac{1}{N}$第一次迭代的权重 $$ = w_{1i}\prod_{m=1}^{M}exp(-\alpha_m\gamma_iG_m(x_i))​= Z_1\sum_{i}w_{2i}\prod_{m=2}^{M}exp(-\alpha_m\gamma_iG_m(x_i))$$ 此步需要依靠上面的提到过的式子 $$ = Z_1Z_2\sum_{i}w_{3i}\prod_{m=3}^{M}exp(-\alpha_m\gamma_iG_m(x_i))​= Z_1Z_2...Z_{M-1}\sum_{i}w_{Mi}exp(-\alpha_m\gamma_iG_m(x_i))= \prod_{m-1}^{M}Z_m结论：这个结果说明，可以在每一轮选取适当的Gm使得Zm最小，从而使训练误差下降最快。 接着，咱们来继续求上述结果的上界。 对于二分类而言，有如下结果： 其中，。 继续证明下这个结论。 由之前Zm的定义式跟本节最开始得到的结论可知： 而这个不等式可先由e^x和1-x的开根号，在点x的泰勒展开式推出。 值得一提的是，如果取γ1, γ2… 的最小值，记做γ（显然，γ≥γi&gt;0，i=1,2,…m），则对于所有m，有： 这个结论表明，AdaBoost的训练误差是以指数速率下降的。另外，AdaBoost算法不需要事先知道下界γ，AdaBoost具有自适应性，它能适应弱分类器各自的训练误差率 。 最后，Adaboost 还有另外一种理解，即可以认为其模型是加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法，下个月即12月份会再推导下，然后更新此文。而在此之前，有兴趣的可以参看《统计学习方法》第8.3节或其它相关资料。 3 Adaboost 指数损失函数推导事实上，在上文1.2节Adaboost的算法流程的步骤3中，我们构造的各个基本分类器的线性组合 f(x) = \sum_{m=1}^{M}\alpha_mG_m(x)是一个加法模型，而Adaboost算法其实是前向分步算法的特例。那么问题来了，什么是加法模型，什么又是前向分步算法呢？ 注意，adaboost算法理论性质并非提出伊始就全部得知，后来在公认的好的解释中逐渐完善。了解：可加模型，指数损失，二分类算法 3.1 加法模型和前向分步算法如下图所示的便是一个加法模型 f(x) = \sum_{m=1}^{M}\beta_mb(x;\gamma_m)​其中，$b(x;\gamma_m)​$称为基函数，$\gamma_m​$称为基函数的参数，$\beta_m​$称为基函数的系数。 在给定训练数据及损失函数$L(y,f( x))$的条件下，学习加法模型$f(x)$成为经验风险极小化问题，即损失函数极小化问题： \underset{\beta_m,\gamma_m}{min} \underset{m=1} {\overset{M} \sum}\beta_m b(x_i;\gamma_m)注：boosting中可以有各种各样的损失，这只是两种损失而已（指数损失，经验风险损失）。同时注意，adaboost并未对总体做假定，使用的更倾向于非参数的方法，在较低维空间有好效果，高维会出现维数灾难（详情见LASSO算法的介绍章节） 随后，该问题可以作如此简化：从前向后，每一步只学习一个基函数及其系数，逐步逼近上式，即：每步只优化如下损失函数： \underset{\beta,\gamma}\min\sum_{i=1}^{N},L(y_i,\beta b(x_i;\gamma))这个优化方法便就是所谓的前向分步算法。 下面，咱们来具体看下前向分步算法的算法流程： 输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}​$ 损失函数：$L(y,f(x))​$ 基函数集：${b(x;\gamma)}$ 输出：加法模型$f(x )$ 算法步骤： 1.初始化$f_0(x) = 0$ 2.对于m=1,2,..M a)极小化损失函数 (\beta_m,\gamma_m) = arg \underset{\beta,\gamma}{min} \sum_{i=1}^{N},L(y_i,f(_{m-1}(x_i) + \beta b(x_i;\gamma))​得到参数\beta_m,\gamma_m​ b)更新 f_m(x) = f_{m-1}(x) + \beta_mb(x;\gamma_m)​ 3.最终得到加法模型 f(x) = f_{M}(x) = \underset{m=1} {\overset{M}\sum} \beta_mb(x;\gamma_m)​就这样，前向分步算法将同时求解从m=1到M的所有参数（、）的优化问题简化为逐次求解各个、（1≤m≤M）的优化问题。 3.2 前向分步算法与Adaboost的关系在上文第2节最后，我们说Adaboost 还有另外一种理解，即可以认为其模型是加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。其实，Adaboost算法就是前向分步算法的一个特例，Adaboost 中，各个基本分类器就相当于加法模型中的基函数，且其损失函数为指数函数。 换句话说，当前向分步算法中的基函数为Adaboost中的基本分类器时，加法模型等价于Adaboost的最终分类器 f( x) = \underset{m=1} {\overset{M}\sum} \alpha_mG_m(x )你甚至可以说，这个最终分类器其实就是一个加法模型。只是这个加法模型由基本分类器及其系数组成，m = 1, 2, …, M。前向分步算法逐一学习基函数的过程，与Adaboost算法逐一学习各个基本分类器的过程一致。 下面，咱们便来证明：当前向分步算法的损失函数是指数损失函数 L(y,f(x)) = exp(-yf(x))​时，其学习的具体操作等价于Adaboost算法的学习过程。 假设经过m-1轮迭代，前向分步算法已经得到$f_{m-1}(x)$： f_{m-1}(x) = f_{m-2}(x) + \alpha_{m-1}G_{m-1}(x) = \alpha_{1}G_{1}(x) + ... + \alpha_{m-1}G_{m-1}(x)​而后在第m轮迭代得到$\alpha_m$、$G_m(x)$、$f_m(x)$，其中$f_m(x)$为： f_m(x) = f_{m-1}(x) + \alpha_mG_m(x) \tag{模型 }而和未知。所以，现在咱们的目标便是根据前向分步算法训练和，使得最终在训练数据集T上的指数损失最小，即 (\alpha_m,G_m(x)) = arg \underset{\alpha,G}{min} \sum_{i=1}^{N}exp(-y_i(f_{m-1}(x_i) + \alpha G(x_i) ))​针对这种需要求解多个参数的情况，可以先固定其它参数，求解其中一两个参数，然后逐一求解剩下的参数。例如我们可以固定G_1(x),...,G_{m-1}(x)​和\alpha_1,...,\alpha_{m-1}​，只针对$G_m(x)​$,$\alpha_m ​$做优化。 换言之，在面对G_1(x),...,G_{m-1}(x),G_m(x)​和\alpha_1,...,\alpha_{m-1},\alpha_m ​ 这2m个参数都未知的情况下，可以： 先假定G_1(x),...,G_{m-1}(x)和\alpha_1,...,\alpha_{m-1}已知，求解出$G_m(x)$和$\alpha_m $； 然后再逐一求解其它未知参数。 且考虑到上式中的 exp(-y_if_{m-1}(x_i))既不依赖 $\alpha$ 也不依赖G，所以是个与最小化无关的固定值，记为\bar{w}_{mi }，即\bar{w}_{mi } = exp(-y_if_{m-1}(x_i))，则上式可以表示为（后面要多次用到这个式子，简记为(\alpha_m, G_m(x ))： (\alpha_m,G_m(x)) = arg \underset{\alpha,G}{min} \sum_{i=1}^{N} \bar{w}_{mi } exp(-y_i \alpha G(x_i))只需要找到(\alpha_m,G_m(x)) ​使得式子最小就行了。 值得一提的是，$\bar{w}_{mi}​$虽然与最小化无关，但$\bar{w}_{mi}​$依赖于$f_{m-1}(x)​$，随着每一轮迭代而发生变化。 接下来，便是要证使得上式达到最小的\alpha_m^* 和 G^*_m(x)​就是Adaboost算法所求解得到的\alpha_m 和 G_m(x)​。 为求解上式，咱们先求G^*_m(x)​再求\alpha_m^* ​。 首先求G^*_m(x)。对于任意\alpha >0，使上式(\alpha_m,G_m(x))最小的G(x)由下式得到： G_m^*(x) = arg \underset{G}{min}\sum_{i=1}^N\bar{w}_{mi}I(y_i ≠ G(x_i))​注意：$y_i ≠G(x_i)​$的时候示性函数取值为1。 别忘了，\bar{w}_{mi} = exp(-y_i,f_{m-1}(x_i))​。 跟1.2节所述的误差率的计算公式对比下： e_m = P(G_m(x_i) ≠ y_i) = \sum _{i=1}^{N} w_{mi}I(y_i ≠ G_m(x_i))​可知，上面得到的便是Adaboost算法的基本分类器，因为它是在第m轮加权训练数据时，使分类误差率最小的基本分类器。换言之，这个便是Adaboost算法所要求的，别忘了，在Adaboost算法的每一轮迭代中，都是选取让误差率最低的阈值来设计基本分类器。 然后求。还是回到之前的这个式子上： (\alpha_m,G_m(x))= arg \underset{\alpha, G}{min} \sum _{i=1}^{N} \bar{w}_{mi}exp(-y_i \alpha G(x_i))​这个式子的后半部分可以进一步化简，得：（这一部分是求解目标） \sum_{i=1}^{N}\bar{w}_{mi}exp(-y_i\alpha G(x_i))= \sum_{y_i = G_m(x_i)}\bar{w}_{mii}e^{-\alpha} + \sum_{y_i ≠ G_m(x_i)}\bar{w}_{mii}e^{\alpha}= (e^\alpha - e^{-\alpha})\sum_{i=1}^{N}\bar{w}_{mi}I(y_i ≠ G(x_i)) + e^{-\alpha}\sum_{i=1}^{N}\bar{w}_{mi} 疑问：第二行拆开之后如何理解呢？这两项求和是什么东西呢？ 前一个看成一个1，后一个看成错误率，再求导就好算了 接着将上面求得的$G_m^*(x)$ G_m^*(x) = arg \underset{G}{min} \sum _{i=1}^{N} \bar{w}_{mi}I(y_i ≠ G_m(x_i))​代入上式中，且对求导，令其求导结果为0，即得到使得一式最小的，即为： 这里的跟上文1.2节中的计算公式完全一致。 此外，毫无疑问，上式中的便是误差率： e_m = \frac{\sum_{i=1}^{N} \bar{w}_{mi } I(y_i ≠G(x_i))} {\sum_{i=1}^{N} \bar{w}_{mi } } = \sum_{i=1}^{N} \bar{w}_{mi } I(y_i ≠G(x_i)) ​即$e_m $就是被$G_m(x) $误分类样本的权值之和。 就这样，结合模型f_m(x) = f_{m-1}(x) + \alpha_mG_m(x)，跟\bar{w}_{mi } = exp[-y_if_{m-1}(x_i)]，可以推出 \bar{w}_{m+1,i} = exp[-y_if_m(x_i)]​= exp[-y_i(f_{m-1}(x_i)+ \alpha_mG_m(x))]​= exp[-y_if_{m-1}(x_i)]+ exp[-y_i\alpha_mG_m(x))]​从而有： \bar{w}_{m+1,i} = \bar{w}_{m,i} exp(-y_i \alpha_mG_m(x))​与上文1.2节介绍的权值更新公式 \bar{w}_{m+1,i} = \frac{\bar{w}_{m,i}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), i = 1,2,...,N相比，只相差一个规范化因子，即后者多了一个 Z_m = \sum_{i=1}^{N}exp(- \alpha_m y_iG_m(x_i))​所以，整个过程下来，我们可以看到，前向分步算法逐一学习基函数的过程，确实是与Adaboost算法逐一学习各个基本分类器的过程一致，两者完全等价。 综上，本节不但提供了Adaboost的另一种理解：加法模型，损失函数为指数函数，学习算法为前向分步算法，而且也解释了最开始1.2节中基本分类器及其系数的由来，以及对权值更新公式的解释，你甚至可以认为本节就是对上文整个1.2节的解释]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|决策树]]></title>
    <url>%2F2019%2F05%2F07%2F190507decisiontree%2F</url>
    <content type="text"><![CDATA[数据挖掘|决策树和相关算法数据挖掘笔记 记录：氦核 最后编辑时间：20190508 前排简介，本文不涉及实战，仅仅举例。文章中间夹杂很多个人思考与经验，如有错误，请在下方评论区指出，欢迎讨论。 预测数据集年龄、性别、家庭所得、是否购买 目标：用前面三列的数据预测是否购买 &gt;root node根节点non-leaf node：非叶结点branches：分支leaf node：叶子节点 注：树不一定对称。很多时候会有一个偏态树分为两种，一种是分类树（离散变量），一种是回归树（连续变量）。 对每一个变量进行大致分类： 1.年龄-小于35、大于等于352.家庭所得：低、高、小康3.性别：男、女 此时，只有年龄是我们希望看到的根节点分类。 如何分节点呢？最常见的方式是计算信息增益 信息增益的计算方法1：ID3I = -(P_1*log_2(p_1) + ... + P_k*log_2(p_k)) = - \sum_{i = 1}^{k}{P_i*log_2(p_i)}Gain(X) = I(n,n_1) - E(X) ​注： I(n,n_1) = -(n_1/n)*log_2(n_1/n) + (n-n_1)/n *log_2((n-n_1)/n)E(X) = m_1 /n * I(m_1,m_{11}) + m_2 /n * I(m_2,m_{21}) +...+ m_k /n * I(m_k,m_{k1})节点分开后，对另外的变量进行信息增益的计算 一棵树，三个节点 总结一下：信息增益——基于熵的概念（搞信息的人常用） 做分支前后熵的差值 方法2：GINI INDEXi(t) = \sum_{i≠j}p(i/t)p(j/t)​gini_{split}(T) = \frac{N_1}{N} gini(T_1) + \frac{N_2}{N} gini(T_2)​将基尼系数最小的作为划分属性 做统计的常用，比较简便 注：ID3（信息增益） 当ID3确定根节点以及后续节点后，当满足一下条件该分支可以结束： 1.该群数据的每一个数据都已经归类到同一类别中 2.该群数据已经没办法找到新的属性进行节点分割 3.该群数据已经没有尚未处理的数据 过度拟合问题并不一定是好事，有可能发生过度拟合，在推广模型时产生较大误差。 两种过度拟合： 1.噪声导致的过度拟合，如错误的分类，或者属性值 2.缺乏代表性的样本导致，如数据有偏 出现过度拟合时处理方式：剪枝 剪枝预剪枝：提前停止树的构建1.定义一个高度，到达时停止生长 2.达到某个节点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这个方法对于处理数据的数据冲突问题比较有效 3.定义一个阈值，当达到某个节点的实例个数小于阈值时，停止生长（常用） 4.定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值大小来决定是否停止生长 思考：有没有什么不太好的地方 第二个比较特别，特殊情况特殊考虑。第四个更容易接受，理由是比较客观。因为阈值不好设置，需要经验。 因此预剪枝的方式好理解，但是多采用后剪枝的方式。但是要求计算量和计算速度。 后剪枝首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。 相比于预剪枝，这种方法更常用。 其他剪枝方法Minimal cost-complexity pruningCCP代价复杂度剪枝 R_\alpha(T) = R(T) + \alpha |T| ​R_\alpha(T(\alpha)) = min_{T]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不务正业|崩坏3战舰甲板清洗姿势指北（指空中劈叉）]]></title>
    <url>%2F2019%2F04%2F20%2F0420houkai3%2F</url>
    <content type="text"><![CDATA[无图，崩坏3rd入坑向教程 崩坏3战舰甲板清洗姿势指北（指空中劈叉）文|氦核 前言 致谢首先感谢做攻略的各路神仙，在新人入坑之际仿佛救命稻草的存在。 崩坏三月光社攻略中心： 手机版 网页版 b站up：赫萝的苹果，大房子，想早睡的熬夜君…… 如果一个人一直玩崩坏三，那他不一定能成为崩坏三大神，但是一定会秃。——氦核 众所周知，崩坏三是一款动作类3D通关养成手游。低级区(&lt;lv55)玩家新奇无限、每每升级总能享受新玩法，解锁新装甲时也会大饱眼福；中级区(lv56&lt;x&lt;lv70)玩家渐入佳境，开始琢磨如何穿搭装备，开始研究如何在现有的等级中提高战斗力，让自己获得更佳的游戏体验。 据说高级区玩家都有一颗看破红尘的心。他们自称萌新，无欲无求，用肝或钞能力追寻着他们在崩坏3rd世界中最后的梦想。（编不下去…… 由于氦核还是中级区lv67咸鱼，对高级区不怎么了解，因此也不想做强度上的攻略了。既然是一款养成游戏，那么氦核就来说说我们新手入坑该如何找到适合自己的老婆吧！（你老婆真棒） 入坑伊始多少人是因为崩三良心的画面入坑的呢（bug先不论）。可能是氦核接触的游戏少，但是说句公道话，崩坏三的画面可以说是动作游戏里比较出众的了。有逼真物理效果的3D模型，有各种让人欲罢不能的不俗立绘，华丽的战斗画面和可爱的互动也都一级棒（可惜刚入坑时的摸妹系统惨遭河蟹），最开心的是，在大部分高配机型上崩坏3都能用60fps运行，丝滑如水的女武神们简直让人把持不住…… 好了，有了想一探究竟的念头，你就可以对这个游戏深入了解了。 先解释一些名词吧。看这篇文章的舰长对游戏应该已经有一定了解了，还有云玩家可比我厉害（笑）。 游戏通常会让各位舰长用三位女武神作为一个小队进行探索。排在第一个位置的叫做队长，不同的女武神有不同的队长技能对全队进行加成（如领域装·白练使全队生命上限增加14%，真不愧是白练！）。 在任务中，通常由一名女武神站场，其他两名在后台待机（ob位，可以回复技能冷却cd）。在场的某种状态（时空断裂，点燃，眩晕，麻痹……）可能触发后台女武神的连击技能（QTE技能）。女武神在敌人攻击的瞬间闪避，能触发闪避技能（大家都有），而某些女武神在做出对敌人攻击的极限闪避后，可能触发时空断裂（又叫时停），顺便一提时停也可能通过其他方式触发（如次元边界突破的出场）。 女武神和敌人的属性分三种：机械，生物，异能。机械克制生物，生物克制异能，异能克制机械。（CG机甲手撕崩坏兽这些桥段都是骗人的，呜呜呜……） 女武神的面板值，攻击，能量，会心（暴击率吧……对物理伤害有用），生命，防御。属性能通过升级（改变自身面板），装备武器、圣痕（装备凑），基地升级、神之键（解锁比较晚，是全局加成）来改变。攻击方式有主要依靠物理、主要依靠元素、元素与物理混合三种。物理攻击加成比较多，伤害可观，依靠“暴击率，暴击伤害，普攻伤害”等词条进行加成；元素攻击具有很高的面板，能穿透护盾，依靠“全伤害，某状态伤害，某状态易伤”等词条进行加成。 敌人除了有属性和攻击力以外，还有攻击速度，移动速度，防御力，某种特殊伤害抗性这些属性。（容易被新手忽略。） 女武神的攻击方式：普通攻击，蓄力攻击，QTE攻击，出场技攻击，分支攻击，必杀攻击，爆发模式，过载模式，召唤物攻击…… 接下来要从这几个方面介绍：&gt;1.女武神特色2.剧情&amp;玩法3.武器、圣痕、抽抽抽系统4.部分崩坏3术语 女武神特色首先喜欢的人物自然要好好培养，崩坏三里面角色虽然多，但是每个角色都有很多种装甲、灵魂觉醒，每个形态都有不同的战斗方式和强度。 首先是被称为草履虫的琪亚娜·卡斯兰娜（Kiana Kaslanna），和某知名作品《魔法禁书目录》的主角命运相似，作为主角的琪亚娜同志在很长一段时间内都只能作为辅助（如圣女祈祷），有时能兼顾输出这样的角色。她作为副c（如白骑士·月光），以分支作为主要输出手段，分支为aa+长按a。不过自从出了空之律者后，琪亚娜从草履虫进化成了律化纳，成为了现阶段的物理输出天花板。如果你是朝着输出强度来玩这个游戏的，那女王一定是你的首选。 除了女王空之律者和白骑士·月光，琪亚娜的装甲都会随着游戏进行慢慢解锁出来，并不需要氪金或者爆肝，这也是好事。其中，初始s白骑士·月光可以从祈愿石（保底6000水，新手福利三选一）直接解锁。我个人最喜欢月光的“光翼展开”，除了造成伤害很高之外，一个有字幕的QTE技能要多帅就有多帅（看到我的翅膀了嘛？你盾没了！） 作为游戏女二，贤惠担当（逐渐工具人化）的雷电·芽衣（Raiden Mei）可以说是早期各位舰长的主要输出，做主c。无论是脉冲装·绯红、女武神·强袭（能够增幅）、影舞冲击还是雷电女王的鬼铠，都透露着窝就是c位的气息。她以分支和大招作为主要输出手段 (据说太刀角色是攻击成长最高的角色) ，连续打出多次分支 ，四个芽衣都是如此。分支为a+长按a(几个芽衣分别有不同方式触发分支)。总的来说，还是很好培养的（秃头暴论）。 初始S雷电女王的鬼凯可以从祈愿石（保底6000水，新手福利三选一）直接解锁，不过因为她是雷元素输出的辅助（简称“鬼”），如果考虑输出的话可以缓缓。女武神·强袭在成为破晓强袭之前伤害都有些乏力，不过爆发出来四十米大刀还是很霸气的。除了鬼凯，其他角色包括强袭的增幅核心都能在游戏中慢慢获得。 我比较喜欢的角色布洛妮娅·扎伊切克（Bronya Zaychik），又叫板鸭。在游戏之中使用远距离火炮或激光进行远程蓄力输出。不过崩三说到底还是个动作游戏，全程远程放风筝确实不好（空之律者：放肆，人类！）。因此板鸭在游戏中一般充当控制，兼顾一定伤害。能打伤害的板鸭不多，有女武神·战车（假冒的初始B，其实是初始S！）、异度黑核侵蚀，次元边界突破……都要求超高的装备配置和队友组合套路，新手还是先不考虑输出为妙。所有板鸭都有一个控制效果的大招以及巨额伤害的qte ，借用大佬的话，或许设计初衷就是让板鸭qte打出伤害开大控制然后下场的吧。 女武神·战车有十秒之久的流血黑洞，大招还能换人，普攻能点火，真是强大如斯。驱动装·山吹是物理近战输出的最好辅助之一，增幅核心启动后加成简直强到让人怀疑人生，大招还有聚怪的引力球，还是个慢慢玩就能有的角色，很好用！（我的第一个ss，升ss也没大变化就是了hhh，主要是为了升级核心）战车的碎片平时在商店用金币可以换。雪地狙击、银狼的黎明，异度黑核侵蚀这些我没怎么用过（其实就是没有吧！银狼被我雪藏了），次元边界突破（简称“次”）是标配保底出的，对于特定阵容有奇效（如聚怪扩冰流，具体请查攻略网）。 无量塔·姬子（Murata Himeko）是我最喜欢的女武神。有故事，有颜值（除了头发确实有些奇怪），同时有一定伤害。虽然在版本更迭中几乎沦落为挖矿角色，也基本上看不到她活跃的身影，但不得不说她在剧情中是一个有血有肉的鲜活无比的战士形象，尤其是剧情“最后一课”太震撼，看官基本都会鼻头一酸（不再剧透），我也是在推完剧情后从板鸭粉转过去的（对不起！布洛妮娅！）。姬子是近战蓄力伤害角色 ，一般作为破盾、给敌方加负面buff的存在在队伍中做辅助。具有高压制力，高抗打断能力和减伤。 融核装·深红，战场疾风，女武神·凯旋，极地战刃（冰姬）四个装甲（灵魂觉醒）都能通过剧情慢慢解锁。融核装·深红是雷属性输出，破盾很快，不过缺点是非常吃技能点，不过完全体的压制力非常之高（一直平a），喜欢这种慢节奏的主c可以考虑，不过还是有些弱势就是啦。女武神·凯旋大招能聚怪，可以作为辅助登场（主要时间还是挖矿TAT）。战场疾风强度比较低，真实初始B。冰姬可以很好地辅助一些需要冰冻环境的主c（如誓约、月魂），自己打伤害就有点头疼了。血色玫瑰和真红（xiang）骑士·月蚀作为高效火元素输出也有一定使用率（老深渊）。强度算是T1级别，不是很出众，虽说毕业后针对特定敌人也不俗，但是确实劳命伤肝啊哈哈哈。（真红玩家泪流了下来） 八重樱（Yae Sakura）是开放世界（一种收益不小的玩法）故事中的人物，使用太刀（和芽衣一样），粉色头发外加蜜汁兔儿圈粉无数，是一个温柔而坚强的巫女，不过嘴确实有些狠毒（“舰长补给全保底，舰长副本零掉落……”）。可以作为主c出场，具有高额分支伤害（追击、刃反）和辅助性质的大招（辅助自己，自己的buff自己加伤）。是个需要把握输出节奏的角色，有时不能贪刀（冲动，就会白给；犹豫，就会败北！）只用过真炎幸魂（炎八），火伤不算低，不过新手打起来很奇怪就是了。御神装·勿忘（冰八）论人气还是很高的，只是无奈游戏更新太快，冰八现在输出比较低。是一个可以肝的初始s。逆神巫女输出是真的不行了，有难以逾越的天花板。为了强度玩的新手多多注意。 德莉莎·阿波卡利斯（Theresa Apocalypse）在游戏中充当吉祥物和可爱担当（新版本双子出场可能世界第一可爱的地位会有些动摇），身材娇小，但是年龄（拖走）……是以十字架为主要输出手段的角色，因为武器“犹大的誓约”伤害太过显眼，有了”女武神犹大“专属装备的戏称。德莉莎（傻）都高频率的攻击同时具有蓄力模式。德莉莎的大部分装甲都能直接获得，除了神恩颂歌和处刑装·紫苑两个初始S需要抽，其他都能慢慢获得。其中女武神·誓约可以做雷元素主c，樱火轮舞（火傻）能补连击和点燃，月下破盾和大招爆发还可以，神恩T0辅助开大招聚怪脆弱一气呵成。至于紫姓女子，新手一定要远离，千万不要被迫害。 卡莲·卡斯兰娜（Kallen Kaslana）是最为无（kuai）脑（le）的角色，满嘴骚话，是各位舰长的真实情人，被称为舞王的存在。攻击频率高，只需要aaaaaa就能打出伤害，是游戏里第一个aaaaaaa为主要输出模式的角色。没用过，不知道强度怎么样。初始s第六夜想曲可以肝，初始A圣仪装·今样签到就能解锁。在剧情中的形象还是挺丰满的（物理），与八重樱在八重村相遇，成就一段佳话，也造就了被众人讨厌的主教绿托。 符华（Fu Hua）是神州的女武神，身份成谜（剧情会慢慢解释），仙人一般的存在，知性而温柔，神州平板代言人（寸劲警告）。当之无愧的平民战神，强度党必备。第一个将b引入连招的角色 第一个能让你体验连招的快感的角色。白夜执事（白夜）、影骑士·月轮（黑丝阿符）都是超强输出，除了女王之外少有的天花板级别角色。赤翎既能当输出，也能当辅助，是上仙化身。女武神·迅羽没用过，不过听说大波（幻觉）破盾也是一流的。甚至在圣痕方面，和上仙有关的东西都是好东西……还基本都是肝物，这实在是太感人了！ 丽塔·洛丝维瑟（Rita Rossweisse）是游戏的色气担当，和希尔有不共戴天之仇（抢版本，抢武器），是飞船AI爱酱的化身，剧情中的立场和身份都很神奇（不影响我们冲冲冲）。丽塔是平砍流角色，不过蓄力攻击和武器主动也能产生一定伤害，纯主c和辅助都可以。黯蔷薇是少有的能和白夜执事相媲美的初始A，不过伤害需要SS才差不多，而且必须要有毕业武器（其他武器差的不是一点半点），搭配山吹也可以A出一片天，是开局可选的新手任务福利（和祈愿不同）；缺点是容易被打断，很脆，蓄力虽然加伤害但要损血（深受其苦）。猎袭装·影铁（绿塔/狗）是很强力的黑魂流元素辅助，我打你疼，你打我也疼。苍骑士·月魂是强力的冰元素输出（靠大招），我没有（非洲微笑）。 女王（空之律者）/极地战刃/影铁引入爆发模式的概念：通过一次蓄力 ，改变以后的攻击模式。深红/真红/影铁还有过载模式，即在爆发结束之后会进入变弱或限制状态。 以上基本就是女武神的特色介绍了。其中混杂了不少个人观点，如有雷同，纯属英雄所见略同，老婆不分你我。希望能对想入坑的你提供帮助。 剧情&amp;玩法剧情真的好，建议去官方IP站补漫画和视觉小说。 玩法：1.主线（务必稳扎稳打，普通全三星不是梦，困难到前三章完可以歇歇）2.编年史（lv24解锁，算是剧情的补充，慢慢打着就行）3.开放世界（lv18解锁，剧情跟着做，星石稍微留一留，买材料亏）4.联机玩法（lv17解锁，前期打通一遍就行，除非有需求，否则没什么好刷的）5.舰团（lv17解锁，早点加一个，人多的最好）6.基地（lv23解锁，注意升级指挥中心，升的越高探险获得的材料就越好，别忘了崩坏炉）7.女武神训练（跟着做）8.深渊&amp;狄拉克之海&amp;记忆战场（一定要打，据说是后期的核心玩法，有很多好材料）9.挑战之路&amp;时序漩涡（受苦，对新手来说不友好）10.曜日活动（第三层解锁以后可以每天打一打）11.其他当期活动（能做就做，换的不亏） 新手福利玩法：1.命运女武神（影舞，黯蔷薇，白夜执事三选一）2.祈愿之地（60水1抽，100抽保底三S角色选一）3.女武神训练&amp;剧情任务&amp;两个挑战任务（跟着做就行）4.导师（这个系统对导师帮助大一些……）5.右下角手机里，有一个资料站，里面有水晶。 注意的事：1.20级之后才能解锁活动系统，很多材料高级才有，可以适当升级。但是建议别升级太快，每天150使命之后就不要领任务奖励了，加速卡少用。2.不要尝试同时练好几个角色，尽管你的金币和技能点看起来很多，他们真的只是看起来很多而已。3.有活动圣痕和皮肤时，优先换皮肤。强度是几个版本的事，但是帅和可爱是一辈子的事~4.每天逛商店，用金币换点好东西，养成类游戏就要有养成的感觉233。40级之后充值界面有礼包可以领。 武器、圣痕和抽卡系统肝物：能够依靠体力换取材料去购买的方式获得的装备，不需要水晶。屯水：攒水晶，通常以2800为单位，低于1万的水晶都属于未攒好的状态。 武器共有七种：双枪、太刀、重炮、大剑、十字架、拳套、镰刀。同时还分蛋池武器（只能从补给中抽取获得）和肝物武器。萌新接触蛋池的机会不多，抽武器前期基本抽一次赚一次（雾），歪出来的装备基本都能用，算是高级过渡品。 歪出来，歪：指没抽到想要的，保底或者随机出了别的高级物品。 肝物总的来说，分为联机材料肝物，逆熵黑核/天命白核肝物，崩坏碎片肝物。 毕业装备暂且不提，蛋池也不提了，萌新抽了就抽了，建议等全池子都很好时再去抽，这样歪了也不亏。那氦核就随便说几句前期可用性比较高的肝物圣痕和武器（50级左右），反正萌新的东西也没几件，想肝出来玩玩成本也不高。 圣痕物理输出：阿提拉下（连击加物理伤害，其实是五星强度吧）芥川龙之介套组（连击加近战物理伤害、会心、攻速）伊丽莎白中（80%hp以上时，加物理伤20%） 物理辅助：德莉莎起源套组（加普攻全队伤害，德莉莎额外加伤） buff输出：特斯拉上中下（无条件高额麻痹伤害，冰冻伤害，眩晕伤害）里纳尔多中（补麻痹） 元素输出：火：浅井茶茶中冰：阿蒙森套组（含全队元素加成）时雨绮罗套（加元素伤害，闪避加元素伤害，三件套加全伤15%）符华·乐师套组（初始四星） sp后台补充：布洛妮娅·懒惰（如战车） 单体输出：赤鸢上（初始四星） QTE、分支输出：坂本龙马两件 大招支援类：柴可夫斯基套（用完大招后，两件换人加伤，三件换人回血） 莫挨老子类：符华·傲慢（初始四星） 以上均为萌新可肝的圣痕，基本都有上位替代圣痕。有时抽蛋池就能齐（汉娜爱你哟~非洲微笑）。。。 武器首先，神之键全都是玩具，不过神之键还是很关键的（全局加成）。 下面列一些值得收藏的萌新玩具。 双枪：火妖精1型（被动概率点燃），暗耀者手炮（被动伤害距离相关，如月光），月神之守护（技能浮空+时锁） 太刀：苗刀·电魂（附加雷伤），结晶逆刃刀（技能冰冻），热能切割刃（技能点燃），苗刀·雷妖（低伤害，技能脆弱） 重炮：马尔可夫A（蓄力炮，被动点燃），阴极子炮07式（激光，技能减攻速），X-01青眼巨蟒（多重蓄力炮，技能减攻速减移速） 大剑：黎明审判（技能加暴击率攻速，被动暴击时概率脆弱），黑轩辕剑（击中时停，秃子必备），超重剑·冲锋（技能浮空） 十字架：游骑兵（火伤，不用结晶能肝到满级50），银色切割者（被动虚弱加伤，技能上虚弱），雷天使（麻痹），火天使（点燃） 拳套：太虚之握（神之键），星环漩涡（AOE，技能减移速拉怪），CAS-II 浪子（直线爆发） 镰刀：群青风暴（加攻速，收益明显） 抽抽抽系统游戏的核心玩法就是抽抽抽啊！但是，抽什么不亏，抽什么亏还是需要事先了解一下的。抽卡用的是水晶，这个是玩家在签到、任务和活动均能获得的一种货币，充钱自然也可以啦。下面解释几个名词。 池子：指一次抽取时可能抽出的全部东西的集合。毕业装备：最适合该角色的装备，武器和圣痕。up：在同等星级装备中的概率提升，让舰长们更容易获得。 好了，我们就分池子看一看吧： 1.标配角色池子（10连保底A级以上女武神）：除了灵魂觉醒的初始s以外的全部角色都可以抽出。一般只白嫖，用水晶抽就有点上头了。2.标配装备池子（10连保底4星装备）：没什么特色的池子，中规中矩，建议不抽。3.精准（装备池）（10连保底4星）：分两个池子，时间稍微错开一点，各自计算保底，过期保底计算清零。两个同期的池子一般仅有up不同，各自有各自的装备。一般攒水晶（屯水）都是为了抽精准和扩充（后面提），是毕业装备的主要来源。建议全池子都挺好的时候再抽。歪出来也不亏。4.扩充（角色池）（100连保底本期S角色，过期保底不清零）：通常角色从这里抽。保底28000水，倾家荡产的量。5.皮肤活动（10发）：通常保底需要9800水，通常会保底……6.武器与其他活动（50连保底武器）：有特定需求抽，每十连送碎片，用的也是精准卡。7.进阶补给（100连保底S角色）：特别活动的角色池子，送碎片。8.友情补给：绿一绿。点屏幕点得开心就好。 以上就是抽卡系统的大概介绍了。每个月舰团有两张标配补给卡可以领，签到也能领一张（稍难），主要还是看精准和扩充啦。请舰长们努力洗甲板吧~~ 补充：氪金的话，月卡强度足够。灰色领域不推荐。 附录：网页上随处可见的崩坏3术语原网页 基本用语1、机八、s8、冰樱—八重樱·御神装勿忘(来源于其机械属性及初始s) 2、冰刀—仿灵刀冰昙天(勿忘的毕业武器) 3、米忽悠、你忽悠、米和油、小作坊、mhy—米哈游，英文名mihoyo(本游戏制作公司) 怒艹 大伟哥—刘伟(公司创始人) 蹦蹦蹦、伟游、 你游 —崩坏3rd 一单—在本游戏为人民币648元 另有一冬/一恶魔/一盒蛋等，表示获得冬之公主皮肤(小恶魔)和盒蛋所需充值的2000rmb和5000rmb 海豹—特性是平时潜水，上岸就晒(在论坛里炫耀自己抽卡的那些人) 女武神角色4、草履虫、稽亚娜、笨蛋—琪亚娜(来源于其傻白甜的性格)游日天—游侠 月光—白骑士月光圣女—圣女祈祷 5、鸡子、大奶—无量塔姬子 血鸡、火鸡、 过气 玫瑰 0.98零点九吧 —姬子血色玫瑰(来源于其名字及火元素攻击方式，后者为mhy黑历史 看知乎或24楼 ) 矿场疾风—战场疾风 深红—融核装深红(注意不要跟芽衣的绯红搞混) 6、板鸭—布洛尼亚(谐音 胸围 ) 7、A8、生8、逆神—八重樱逆神巫女(来源于其生物属性及初始A属性) 8、学院长、大姨妈、德丽傻 合法萝莉 —徳丽莎 s傻、基(机)德—徳丽莎处刑装紫苑 雷傻—徳丽莎誓约 樱傻、火傻、狐傻、狐德—徳丽莎樱火轮舞 9、牙医—雷电芽衣 强无敌—雷电芽衣强袭鹦鹉—影舞冲击 矿铠、充电宝—雷电女王的鬼铠(来源其队长技) 怪物相关10、蚊子—突进级崩坏兽 大盾—圣殿级崩坏兽(外形系列怪) 11、白双刀—忠忍武死士(外形的系列怪) 黑双刀—顺忍武死士(外形的系列怪) 红双刀—隐流上忍(上述双刀结合体) 召唤表—亡灵死士(外形的系列怪) 火弓表—神侠上尉 圣痕相关12、乐队—爱因斯坦乐队圣痕(杂技圣痕) 13、岳父岳母—齐格飞和塞西莉亚 14、沙比(傻逼)套—莎士比亚圣痕(优秀的元素增伤圣痕)强烈建议使用沙比这一称呼! 15、傻白—伊丽莎白圣痕(实用的新手圣痕尤其是中位) 16、虐猫、艹猫 fps —薛定谔圣痕(来源请自行搜索这个人) 17、艳后和沙皇—克列欧博克拉和叶卡捷琳娜圣痕(优秀的通用圣痕) 日常和游戏相关18、猪、白猪、劳模—boss加尼萨 冰猪、黑猪—boss帕凡提 19、皮皮马—boss黑轩辕 20、挖矿—将当前比较弱上不了战场的女武神派去家园里的远征中心做远征拿材料 矿工则多指当前版本弱势的女武神 放屁除了勿忘都是矿工（氦核注：现版本你懂的 紫菜—紫色材料即四星材料 金刀、银刀—金银刀币(外传和驱魔获得，可换取各类材料武器圣痕) 门票—清神御守(打轮回狐狸用，驱魔获得) 21、sl—save/load(游戏类通用术语，反复存读档，一般指强行关闭游戏再进入游戏使游戏保持某个进度，手游一般通过在手机后台直接关闭进程实现) 22、60水晶无事发生—崩3出现各种差错bug时经常是对全服玩家发放60水晶来换取玩家的原谅 介个是设定呢—客服娘无法解释策划的一些zz行为时的说法 985、ch—崩3策划 这锅我们985不背 武器相关23、晕枪、核弹按钮—游戏中会送的低星神器双枪科尔特 晕炮—降临活动中可获得的火炮潜伏者加农(两者都有眩晕效果，多用于触发战车的强力qte) 24、御三家—游戏初始赠送的三个女武神琪亚娜领域装白练、芽衣脉冲装绯红、布洛尼亚战车(碎片获取方式最多最容易升阶) 25、香蕉刀—苗刀电魂(优秀的平民武器，属性好获取简单) 甜甜圈—能量跃迁者 电锯—尼德霍格翼爪 新电锯—黎明审判 26、老子—李耳圣痕 耳红—李耳红圣痕(mhy不知道出于什么心理，中国的人物圣痕一般都比较弱鸡，典型代表李耳神农，所谓无上李耳王和农的传人)李耳目前只有上位给山吹比较好用，所以无上… 27、保时捷刀—降临系列武器的暴食节刀 顺便一提降临武器中的合金系列都是腊鸡 水一—水妖精1型双枪(圣女可用的优秀平民武器) 水二—水妖精2型(游侠可用的优秀扩冰流武器) 马A—马尔科夫A型火炮(战车可用的优秀平民武器)马C—马尔科夫C型 结与晶—双枪光与影(来源目前其弱鸡的功能，一般分解作结晶) 精卫、鸟枪—精卫之翼(优秀的平民武器) 蛋池毁灭者—烈焰毁灭者(弱鸡双枪) 苗枪、圣遗物之耻—2nd圣遗物(来源于其弱鸡的武器技能，与苗刀电魂差不多，不过其实还是把不错的圣女武器) 三阿弟—3rd圣遗物(强袭和A8适用) 电磁炮—超电磁手炮(优秀的通用双枪) 索尔—索尔之锤(优秀的通用双枪) 月神、浮空枪—月神守护(优秀的平民双枪) 原谅刀—雷切(来源其颜色，影舞毕业适用)少数时候指疾风太刀 打刀—反立场打刀11式(某些套路可用) 灵刀—灵刀樱吹雪(逆神巫女专属武器) 磁暴铲—磁暴斩(鬼铠毕业武器) 周立波—高周波切割刀 血舞—鲜血之舞(优秀的通用大剑) 跑酷刀、飙车剑—牛鬼切·长光(疾风适用) 掉帧炮—5th圣遗物(优秀的通用火炮) 28、村夫—诸葛孔明圣痕(来源于三国演义电视剧 b站鬼畜 ) 村姑—琪亚娜跟某法国村姑 贞德 的发型蜜汁相似，通称村姑发型 29、深渊用语：漏气—深渊掉血 你老婆才是充气的 滑稽—隐身药水 丢垃圾，上炸弹—丢对他人深渊道具(理性爬塔不要搞事)磕伟哥—使用雄伟药水 蹲比—改版前深渊趁别人刚上33层或隐身药水失效时马上上炸弹的人 作死三件套：李耳红上普朗克中符华/牛顿下 30、遇事不决阿提拉—阿提拉作为最强最通用三星圣痕获得无数好评，几乎所有的输出角色在得到毕业装前均可用作过渡装。 31、第一绿者—奥托 大人加油啊 (来源于八重樱跟卡莲复制人徳丽莎 的姬情) （完·真结局）]]></content>
      <tags>
        <tag>娱乐time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|LARSN]]></title>
    <url>%2F2019%2F04%2F16%2F190416LARSN%2F</url>
    <content type="text"><![CDATA[最后编辑于190514 本文需要一定的Lasso基础，可以推荐的文章学习。 参考文章Lasso回归 维数灾难高维数据何谓高维数据？高维数据指数据的维度很高，甚至远大于样本量的个数。高维数据的明显的表现是：在空间中数据是非常稀疏的，与空间的维数相比样本量总是显得非常少。在分析高维数据过程中碰到最大的问题就是维数的膨胀，也就是通常所说的“维数灾难”问题。研究表明，随着维数的增长，分析所需的空间样本数会呈指数增长。如下所示，当数据空间维度由1增加为3，最明显的变化是其所需样本增加；换言之，当样本量确定时，样本密度将会降低，从而样本呈稀疏状态。假设样本量n=12,单个维度宽度为3，那在一维空间下，样本密度为12/3=4，在二维空间下，样本分布空间大小为3×3，则样本密度为12/9=1.33，在三维空间下样本密度为12/27=0.44。 设想一下,当数据空间为更高维时，$X=[x_1x_1,x_2x_2,….,x_nx_n]$会怎么样？ 需要更多的样本，样本随着数据维度的增加呈指数型增长； 数据变得更稀疏，导致数据灾难； 在高维数据空间，预测将变得不再容易； 导致模型过拟合。 具体例子可以参考 机器学习:分类问题中的“维数灾难” 高维数据对非参数方法带来了致命打击。 理论基础：数据降维对于高维数据，维数灾难所带来的过拟合问题，其解决思路是：1）增加样本量；2）减少样本特征。 而对于现实情况，会存在所能获取到的样本数据量有限的情况，甚至远小于数据维度，即：d&gt;&gt;n。如证券市场交易数据、多媒体图形图像视频数据、航天航空采集数据、生物特征数据等。 主成分分析作为一种数据降维方法，其出发点是通过整合原本的单一变量来得到一组新的综合变量，综合变量所代表的意义丰富且变量间互不相关，综合变量包含了原变量大部分的信息，这些综合变量称为主成分。主成分分析是在保留所有原变量的基础上，通过原变量的线性组合得到主成分，选取少数主成分就可保留原变量的绝大部分信息，这样就可用这几个主成分来代替原变量，从而达到降维的目的。 但是，主成分分析法只适用于数据空间维度小于样本量的情况，当数据空间维度很高时，将不再适用。 Lasso是另一种数据降维方法，该方法不仅适用于线性情况，也适用于非线性情况。Lasso是基于惩罚方法对样本数据进行变量选择，通过对原本的系数进行压缩，将原本很小的系数直接压缩至0，从而将这部分系数所对应的变量视为非显著性变量，将不显著的变量直接舍弃。 lasso回归普通线性模型普通线性模型 Y = X\beta + \epsilon响应变量$Y = (y_1 + y_2 + … + y_n)^T$,协变量$X = (x^{(1)} + x^{(2)} + … + x^{(n)})$，对于每一个$X^{(j)}$有$X^{(j)} = (x^{(j)}_1 + x^{j}_2 + … + x^{(j)}_n)^T$. 假设每个$X^{(j)} (i = 1,2,…,n, j = 1,2,…,d)$均中心化和标准化（实际操作中这一步很关键），随机误差项$\epsilon_i … N(0,\sigma^2),(i = 1,2,…,n),\epsilon = (\epsilon_1,\epsilon_2,…,\epsilon_n)^T$ 回归系数$\beta = (\beta_1,\beta_2,…,\beta_d)^T$ 当X为列满秩设计矩阵时，回归系数$\beta$可由普通最小二乘估计方法求得， \hat{\beta} = (X^TX)^{-1}X^TY当X不为列满秩设计矩阵时，普通最小二乘法将不再适用于求解回归系数$\beta$，惩罚方法应用较广。 惩罚方法当X不为列满秩设计矩阵时，普通最小二乘法将不再适用于求解回归系数，此时引入惩罚方法。该方法可以同时实现变量选择和参数估计，在参数估计时，通过将部分参数压缩为0来达到变量选择的效果。惩罚方法时取惩罚似然函数最小时的值作为回归系数的估计值，即\hat{\beta} = arg min_{\beta∈R^d}(||Y - X\beta||^2 + P_{\lambda}(|\beta|)) 其中惩罚项$P_{\lambda}(|\beta|) = \lambda \sum_{j = 1}^{d}|\beta_j|^m,m&gt;=0$,$\lambda$为调节参数（也可以为向量）。m = 1时，得到$L_1$惩罚项（Lasso惩罚）；当m = 2时，得到$L_2$惩罚项（Ridge惩罚）。 Lasso方法Lasso方法是在普通线性模型中增加$L_1$惩罚项，对于普通线性模型的Lasso估计为\hat{\beta_{Lasso}} = arg min_{\beta∈R^d}||Y - X\beta||^2 s.t. \sum_{j = 1}^{d}|\beta_j|=0 等价于\hat{\beta_{Lasso}} = arg min_{\beta∈R^d}||Y - X\beta||^2 +\lambda \sum_{j = 1}^{d}|\beta_j| 其中，t与$\lambda$一一对应，为调节系数。 令t_0 = \sum_{j = 1}^{d}|\hat{\beta_j}(OLS)|，当t&lt;t0时，一部分系数就会被压缩至0，从而降低X的维度，达到减小模型复杂度的目的。 参考文章Lasso回归 LARS一般性质lasso的两个等价形式: \hat{\beta}(t) = arg\ min \frac 12\|Y-X\beta||_2^2\ \ \ \ s.t \ \ ||\beta||_1\leq t \tag{1}\hat{\beta}(\lambda) = arg\ min \frac 12 \ ||Y-X\beta||_2^2+\lambda\ ||\beta||_1 \tag{2} $\forall t \geq 0,\exists \lambda \geq 0,$ 使得$ \hat{\beta}(\lambda)=\hat{\beta}(t).$ 记 \beta_j^+=\left\{\begin{array}{ll}\beta_j,& \beta_j\geq0\\ 0,& \beta_j< 0\end{array}\right., \quad\quad \beta_j^-=\left\{\begin{array}{ll}-\beta_j,& \beta_j\leq0\\ 0,& \beta_j>0\end{array}\right.则有 \beta_j=\beta_j^+-\beta_j^- ,\quad |\beta_j|=\beta_j^++\beta_j^- 在考虑引入正部和负部后，增加了两个约束条件，故拉格朗日函数为： \Gamma(\beta_j^+,\beta_j^-;\lambda)=\frac 12||Y-\sum(X_j(\beta_j^+-\beta_j^-))||_2^2+\lambda1_p^T(\beta_j^++\beta_j^-)-\sum{\lambda_j^+\beta_j^+}-\sum{\lambda_j^-\beta_j^-}\tag{3} 求解要求:\begin{cases}\lambda_j^+\beta_j^+=0\\\lambda_j^-\beta_j^-=0\\ \beta_j^+\geq0 \\ \beta_j^-\geq0 \end{cases} 求导有：\begin{cases}\frac{\partial \Gamma(\beta_j^+,\beta_j^-;\lambda)}{\partial \beta_j^+}=-X_j^T(Y-X\beta)+\lambda-\lambda_j^+=0 \\ \frac{\partial \Gamma(\beta_j^+,\beta_j^-;\lambda)}{\partial \beta_j^-}=X_j^T(Y-X\beta)+\lambda-\lambda_j^-=0\end{cases} \tag{4} 由互补松弛定理有: \begin{cases}\lambda_j^+= 0,\quad \beta_j^+>0\\ \lambda_j^+\geq0,\quad \beta_j^+=0 \end{cases} 同理\begin{cases}\lambda_j^-= 0,\quad \beta_j^->0\\ \lambda_j^-\geq0,\quad \beta_j^-=0 \end{cases} 令导数为0，于是式(4)退化为： \begin{cases}若\beta_j^+(\lambda)\neq 0 ,\ \ \ X_j^T(Y-X\beta(\lambda))=\lambda \\ 若\beta_j^-(\lambda)\neq 0 ,\ \ \ X_j^T(Y-X\beta(\lambda))=-\lambda \\ 若\beta_j(\lambda)=0,\ \ \ |X_j^T(Y-X\beta(\lambda))|\leq\lambda\end{cases}\tag{5}注： $\lambda$和t的变化趋势相反，当t逐渐变大的时候，$\lambda$逐渐变小。 记 \lambda_0= \max\{|X_1^TY|,\cdots,|X_p^TY|\}, \quad j_0 = \arg\max\{|X_1^TY|,\cdots,|X_p^TY|\},当$\lambda\geq\lambda_0$时,$\hat{\beta}(\lambda)=0.$ 记活跃集为$A(\lambda)=\{j\ |\hat{\beta}_j(\lambda)\neq0\}$ 存在$\lambda_k$, $k=0,\dots,$ 当$\lambda\in(\lambda_{k+1},\lambda_{k}),$$A(\lambda)$保持不变 当$\lambda =0$，该模型的解为最小二乘解 初始化 $\exists\lambda_0$，当$\lambda\geq\lambda_0$时,$\hat{\beta}(\lambda)=0,$解得\lambda_0= \max\{|X_1^TY|,\cdots,|X_p^TY|\}, \quad j_0 = \arg\max\{|X_1^TY|,\cdots,|X_p^TY|\}. 记活跃集为$A(\lambda)=\{j\ |\hat{\beta}_j(\lambda)\neq0\}$ 存在$\lambda_k$, $k=0,\dots,$ 当$\lambda\in(\lambda_{k+1},\lambda_{k}),$$A(\lambda)$保持不变 在$\lambda=\lambda_k$时，活跃集发生变化，或者有变量进入，或者有变量退出，但可以证明以下事实 $\hat{\beta}(\lambda)$是连续的 当$\lambda\in(\lambda_{k+1},\lambda_{k}),$ $\hat{\beta}(\lambda)$是分段线性的 所以，最小角度回归算法有两个关键点 当$\lambda\in(\lambda_{k+1},\lambda_{k})$时，其线性形式是什么 确定$\lambda_{k+1},$ 也就是分段线性形式改变的位置 一般化、第k步 当$\lambda\in(\lambda_{k+1},\lambda_{k}),$ $A(\lambda)$保持不变, 记做$A_k$ 记$\beta_{A_k}=\beta_{A_k}(\lambda_k),$ $c_{k}=X_{A_K}^T(Y-X_{A_K}\beta_{A_K}), $$s_{A_K}=sign(c_{k})$ 由式(5) X_{A_K}^T(Y-X_{A_K}\hat{\beta}_{A_K}(\lambda))=s_{A_K}\lambda, \quad \quad\lambda\in(\lambda_{k+1},\lambda_k) \tag{6} 可以验证， X_{A_K}^T(Y-X_{A_K}\hat\beta_{A_K})=s_{A_K}\lambda_k\tag{7} 式(7)减式(6)， \hat\beta_{A_k}(\lambda)=\hat\beta_{A_k}+(\lambda_k-\lambda)(X_{A_k}^TX_{A_k})^{-1}S_{AK} 记d_{A_k}=(X_{A_k}^TX_{A_k})^{-1}s_{A_k}\quad\quad \gamma=\lambda_k-\lambda则 \hat\beta_{A_k}(\lambda)=\hat\beta_{A_k}+\gamma d_{A_k} 活跃集，考虑变量进入 首先定义以下变量 记$A^C_k$为$A_k$的补集 记a_k=(X_{A^C_k}^TX_{A_k})d_{A_k}, $c_k$为处于非活跃集自变量与残差的相关系数，c_k=X_{A^C_k}^T(Y-X_{A_k}\hat{\beta}_{A_k}) 记$a_{kj}$是$a_k$的第$j$项, $c_j$是$c$的第j项 那么， $a_{kj}\lambda_k\leq c_{kj}$时，$\gamma_j=\frac{\lambda_k-c_{kj}}{1-a_{kj}}$ $a_{kj}\lambda_k&gt; c_{kj}$时，$\gamma_j=\frac{\lambda_k+c_{kj}}{1+a_{kj}}$ 具体推导如下： 根据KKT条件，如果$A_k$保持不变，那么， \left|X_{A^C_k}^T(Y-X_{A_k}\hat\beta_{A_k}({\gamma}))\right|\leq \lambda_k-\gamma, 式(3)中，$\hat\beta_{A_k}({\gamma})=\hat\beta_{A_k}+\gamma d_{A_k}，$ 所以 |c_k - \gamma a_k|\leq \lambda_k-\gamma 如果$A^C_k$中第$j$个元素率先进入活跃集，那么，$c_{kj}-\gamma a_{kj}=\pm(\lambda_k-\gamma)$ 画图找交点 若$c_{kj}&gt;0$ \gamma_j=\begin{cases}\frac{\lambda_k-c_{kj}}{1-a_{kj}},&-a_{kj}\geq -\frac{c_{kj}}{\lambda_k}\\ \frac{\lambda_k+c_{kj}}{1+a_{kj}},&-a_{kj}\leq -\frac{c_{kj}}{\lambda_k}\end{cases} 若$c_{kj}&lt;0$ \gamma_j=\begin{cases}\frac{\lambda_k-c_{kj}}{1-a_{kj}},&-a_{kj}\geq -\frac{c_{kj}}{\lambda_k}\\ \frac{\lambda_k+c_{kj}}{1+a_{kj}},&-a_{kj}\leq -\frac{c_{kj}}{\lambda_k}\end{cases} 综上，\gamma_j=\begin{cases}\frac{\lambda_k-c_{kj}}{1-a_{kj}},&a_{kj}\lambda_k \leq c_{kj}\\ \frac{\lambda_k+c_{kj}}{1+a_{kj}},&a_{kj}\lambda_k> c_{kj} \end{cases} 考虑变量退出 $\hat\beta_{A_k}({\gamma})=\hat\beta_{A_k}+\gamma d_{A_k}$，记$w_j$为$\beta_{A_k}$的第$j$个元素与$d_{A_k}$的第$j$个元素之比的相反数 活跃集中第$j$个元素率先退出，那么$\gamma_j= w_j$ 活跃集个数小于$p$时，变量的进入和退出 定义$\hat{\gamma}=\min^{+} \{\gamma_j,j=1,2,\cdots,p\}.$ 活跃集=p时 $\exists \gamma_j&gt;0$ 记\hat{\gamma}=min{\{\gamma_j,j=1,2,\cdots,p\},其中\gamma_j>0}则\hat{\gamma}=\begin{cases}\hat{\gamma},&\lambda_k-\hat{\gamma}>0\\ \lambda_k,&\lambda_k-\hat{\gamma}\leq0 \end{cases} $\forall \gamma_j\leq0$则\hat{\gamma}=\lambda_k 更新公式 \beta_{new}=\beta_{old}+\hat{\gamma}\cdot d_A\lambda_{new}=\lambda_{old}-\hat{\gamma} RSS的更新\begin{eqnarray} RSS&=&(Y-X_A(\beta_A+\gamma\cdot d_A))^T(Y-X_A(\beta_A+\gamma\cdot d_A)\\ &=& (Y-X_A\beta_A)^T(Y-X_A\beta_A)-2\gamma(Y-X_A\beta_A)^TX_Ad_A+\gamma^2d_A^TX_A^TX_Ad_A\\ &=& (Y-X_A\beta_A)^T(Y-X_A\beta_A)-2\gamma Y^TX_Ad_A+2\gamma\beta_A^TX_A^TX_Ad_A+\gamma^2d_A^TX_A^TX_Ad_A \end{eqnarray}代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141l2norm &lt;- function(x)&#123; return(sqrt(sum(x*x)))&#125;l1norm &lt;- function(x)&#123; return(sum(abs(x)))&#125;l0norm &lt;- function(x)&#123; return( sum(abs(x)&gt;1e-13))&#125; library(ElemStatLearn)data(prostate)data &lt;- prostate[,-10]head(data)np &lt;- dim(data)n &lt;- np[1]p &lt;- np[2]-1xn &lt;- names(data)[1:p]x &lt;- as.matrix(data[,1:p])xm &lt;- apply(x,2,mean)X &lt;- sweep(x,2,xm,&quot;-&quot;)Xl2norm &lt;- apply(X,2,l2norm)X &lt;- sweep(X,2,Xl2norm,&quot;/&quot;)#apply(X,2,sd)#x &lt;- as.matrix( cbind(1,x) )y &lt;- data[,p+1]ym &lt;- mean(y)Y &lt;- (y-ym)Yl2norm &lt;- l2norm(Y)Y &lt;- Y/Yl2norm##############################XTX &lt;- t(X)%*%XXTY &lt;- drop(t(X)%*% Y) YY &lt;- sum(Y*Y)reA &lt;- NULLrelamb &lt;- NULLreRSS &lt;- NULLreb &lt;- NULL########################A &lt;- rep(F,8)reA &lt;- rbind(reA, A)#df &lt;- sum(A)j &lt;- which.max( abs(XTY))#print(j)A[j] &lt;- !A[j]lamb &lt;- abs(XTY[j])b &lt;- rep(0,8)RSS &lt;- YYrelamb &lt;- c(relamb, lamb)reb &lt;- rbind(reb ,b)reRSS &lt;- c(reRSS,RSS)#####################################################################while(TRUE)&#123; CC &lt;- XTY - XTX %*% b SCC = sign(CC) SCCA &lt;- SCC[A] XTXA &lt;- XTX[A,A,drop=F] d = as.matrix(solve(XTXA, SCCA),ncol =1) a = XTX[!A,A] %*% d gam=rep(0,8) gam[A] = -b[A]/d if(sum(A)&lt;=0 | sum(A)&gt;8 )&#123; print(&quot;Something is wrong!&quot;) break &#125; else if(sum(A)&lt;8)&#123; gam[!A] =ifelse(a*lamb&lt;=CC[!A], (lamb-CC[!A])/(1-a),(lamb+CC[!A])/(1+a)) gamm= max(gam) +1 gam[gam&lt;=0]=gamm j = which.min(gam) gammin = gam[j] RSS &lt;- RSS - 2*gammin*sum(XTY[A]*d) + 2*gammin*sum(b[A]*SCCA)+ gammin^2*sum(d*SCCA) b[A] =b[A] + gammin * d lamb = lamb - gammin reA = rbind(reA, A) A[j] = !A[j] reRSS &lt;- c(reRSS, RSS) relamb = c(relamb, lamb) reb =rbind(reb ,b) &#125; else if (sum(A)==8) &#123; if(sum(gam &gt; 0) &gt; 1) &#123; gamm= max(gam) +1 gam[gam&lt;=0]=gamm j = which.min(gam) gammin = gam[j] if(lamb-gammin &gt;0)&#123; RSS &lt;- RSS - 2*gammin*sum(XTY[A]*d) + 2*gammin*sum(b[A]*SCCA)+ gammin^2*sum(d*SCCA) b[A] =b[A] + gammin * d lamb = lamb - gammin reA = rbind(reA, A) A[j] = !A[j] reRSS &lt;- c(reRSS, RSS) reA = rbind(reA, A) relamb = c(relamb, lamb) reb =rbind(reb ,b) &#125;else&#123; gammin = lamb RSS &lt;- RSS - 2*gammin*sum(XTY[A]*d) + 2*gammin*sum(b[A]*SCCA)+ gammin^2*sum(d*SCCA) b[A] =b[A] + gammin * d lamb = lamb - gammin reRSS &lt;- c(reRSS, RSS) reA = rbind(reA, A) relamb = c(relamb, lamb) reb =rbind(reb ,b) break &#125; &#125; else &#123; gammin = lamb RSS &lt;- RSS - 2*gammin*sum(XTY[A]*d) + 2*gammin*sum(b[A]*SCCA)+ gammin^2*sum(d*SCCA) b[A] =b[A] + gammin * d lamb = lamb - gammin reRSS &lt;- c(reRSS, RSS) reA = rbind(reA, A) relamb = c(relamb, lamb) reb =rbind(reb ,b) break &#125; &#125;&#125;Cp &lt;- reRSS/RSS*(n-p) - n + 2*apply(reA,1,sum)re &lt;- list(b=reb,lambda=relamb,A=reA,RSS=reRSS,Cp=Cp)re 结果不贴了，大家跑一下就行啦。]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|最优子集回归&最小角回归]]></title>
    <url>%2F2019%2F03%2F26%2F190326%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%9C%80%E4%BC%98%E5%AD%90%E9%9B%86%E5%9B%9E%E5%BD%92%26%E6%9C%80%E5%B0%8F%E8%A7%92%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[最优子集回归是从一个较多自变量的模型中，选出优秀的自变量集合做模型的一个方法。 什么是最优子集回归? 当我们进行回归分析时，我们可能会碰到自变量过多, 且存在多重共线性, 或者全模型可能是过度拟合的. 针对这种情况，可以人为根据经验判断筛选对因变量有影响的自变量，比如距离周边学校的距离去预测房价. 但通常我们并不是相关领域的专家, 对可能影响因变量的自变量并不了解，于是我们需要运用算法获得预测效果最优的模型, 进而接近真实模型，比如最优子集回归. 最优子集回归, 即对$p$个预测变量的所有可能组合分别使用最小二乘回归进行拟合 对含1变量的模型，拟合$p$个模型; 对含两个变量的模型，拟合$p(p-1)/2$个模型，以此类推，总共拟合$2^p$个模型. 按照一定的比较准则（如AIC）从中选择一个最优模型. 如何获得全部模型? 最优子集回归需要建立大量的模型，手动输入显然不可行，可以使用二进制向量来表示一个模型，值为1表示选择该变量，值为0表示不选择该变量. 比如总共有3个自变量时，$(0,1,1)$表示使用第2和第3个自变量建立模型. 将所有可能的向量按行排列，得到$2^p\times p$维的矩阵$Z$. 矩阵$Z$的行转化为十进制即为$[0,2^p-1]$的所有整数，所以，$[0,2^p-1]$中的所有整数转化为二进制向量并按行排列即可表示所有可能的模型. 循环法获得矩阵$Z$ 解方程$(1)$，得到$(k_0,k_1,…,k_{p-1})$, 即可将十进制的$n$转化为二进制$p$维向量。n=k_02^0+k_12^1+k_22^2+...+k_{p-1}2^{p-1}\tag{1} turnbits_cir 函数即可实现此功能。 123456789101112131415161718192021222324##turnbits_cir函数用于将十进制转化为p维二进制向量##参数n为十进制数，p为二进制向量维数##输出p维逻辑行向量turnbits_cir = function(n,p)&#123; z = rep(0,p) #预留空间 tn = n #tn为十进制数值 z[1] = tn%%2 #tn/2的余数为z的第一个值 for(j in 2:p)&#123; tn = (tn-z[j-1])/2 #更新tn的值 if(tn == 0) break #tn=0时跳出循环 z[j] = tn%%2 #z的第j个值为tn/2的余数 &#125; as.logical(z) #将向量z转化为逻辑值&#125;##以p=3为例p = 3Z = matrix(unlist(lapply(0:(2^p-1) , turnbits_cir , p)) , #将0：2^p-1转化为p维二进制向量，并按行排列 ncol = p ,byrow = T)Z 这个矩阵的作用是穷举全部自变量的子集。 递归法获得矩阵$Z$ 除了解方程外，我们还可以使用递推公式获得矩阵$Z$。 \begin{array}{lc} \mbox{}& \begin{array}{cc}p=1 \end{array}\\ \begin{array}{c}0\\1\end{array}& \left[\begin{array}{cc} 0\\ 1 \end{array}\right] \end{array} \Rightarrow \begin{array}{lc} \mbox{}& \begin{array}{cc}p=2 \end{array}\\ \begin{array}{c}0\\2\\1\\3\end{array}& \left[\begin{array}{cc} 0&0\\ 0&1\\ 1&0\\ 1&1\\ \end{array}\right] \end{array}\Rightarrow \begin{array}{lc} \mbox{}& \begin{array}{cc}p=3 \end{array}\\ \begin{array}{c}0\\4\\2\\6\\1\\5\\3\\7\end{array}& \left[\begin{array}{cc} 0&0&0\\ 0&0&1\\ 0&1&0\\ 0&1&1\\ 1&0&0\\ 1&0&1\\ 1&1&0\\ 1&1&1\\ \end{array}\right] \end{array}\quad...\tag{2} 上面$(2)$式中矩阵左侧的数字代表矩阵行向量的十进制数值. 从$(2)$式中可以看出，给矩阵左侧添加一列$0$得到的向量，其十进制值为原向量的二倍，给矩阵的左侧添加一列$1​$得到的向量，其十进制值为原向量的二倍加一. 给$2^pp$维$Z$矩阵左侧分别添加一列$0$和$1$，并按行拼接，即可得到$2^{(p+1)}(p+1)$维$Z$矩阵. turnbits_rec 函数即可实现此功能。 1234567891011121314##turnbits_rec函数用于将[0,2^p-1]中的整数转化为(2^p)*p维逻辑矩阵##输出(2^p)*p维逻辑矩阵turnbits_rec=function(p)&#123; if(p==1) return (matrix(c(FALSE,TRUE),ncol=1)) #p=1时返回初始值 else return (cbind(rbind(turnbits_rec(p-1),turnbits_rec(p-1)), #p≠1时给第p个矩阵左侧分别添加一列0和1 #再按行拼接起来得到第p+1个矩阵 rep(c(FALSE,TRUE),rep(2^(p-1),2))))&#125;Z=turnbits_rec(3) Z R语言实现最优子集回归以AIC为比较准测使用 bestlm 函数实现最优子集回归。 123456789101112131415161718192021222324252627282930##bestlm函数用于进行最优子集回归##参数z为二进制行向量，代表不同的模型，参数dataTR为训练集，dataTE为测试集##dataTR和dataTE应有相同的结构，且因变量在最后一列##输出2*2^p维矩阵，第一行为测试误差，第二行为AIC值，不同列代表不同模型bestlm = function(z, dataTR,dataTE)&#123; p = dim(dataTR)[2] #求数据集共有多少列 yxn = names(dataTR) #向量yxn为所有变量的名称 yn = yxn[p] #定义yn为因变量名称 xn = yxn[1:p-1] #定义向量xn为所有自变量名称 tm1 = paste(yn,&quot;~&quot;,sep=&quot;&quot;) #定义tm1为yn~ &#123;if(sum(z) == 0) tm2 = &quot;1&quot; #如果自变量个数为0，tm2为1 else tm2 = paste(xn[z] , collapse = &quot;+&quot;)&#125; #自变量个数不为0，tm2为各自变量名称相加 fam = formula(paste(tm1, tm2, sep = &quot;&quot;)) #将tm1和tm2拼接并转化为公式 lm1 = lm(fam , dataTR) #建立回归模型 TE = mean((dataTE[,p] - predict(lm1,dataTE))^2) #计算测试误差 AIC = extractAIC(lm(fam,dataTR))[2] #计算模型AIC值 c(TE,AIC) #拼接测试误差和AIC值&#125; 例子 接下来，我们用 ElemStatLearn 包中的 prostate 数据集进行最优子集回归 prostate 是关于前列腺切除手术的数据集，共包含十个变量，其中前八列为自变量，第九列为因变量，第十列是逻辑变量，用于区分训练集与测试集。 123456789101112131415161718192021library(ElemStatLearn) #加载ElemStatLearn包data(prostate) #加载prostate数据集data = prostate p = dim(data)[2]-2 #计算自变量个数datatr = data[data[,p+2],1:(p+1)] #获得训练集 datate = data[!data[,p+2],1:(p+1)] #获得测试集 Z = turnbits_rec(p) #计算二进制矩阵mAIC = apply(Z,1,bestlm,datatr,datate) #进行最优子集回归names(data)[1:p][Z[which.min(mAIC[1,]),]] #按测试误差最小选出最优模型names(data)[1:p][Z[which.min(mAIC[2,]),]] #按AIC最小选择最优模型#min(mAIC[2,])plot(mAIC[1,], mAIC[2,]) 1234head(data)lm1 &lt;- lm(lpsa~.,data[,-10])extractAIC(lm1)AIC(lm1) 和以前一样，结果省略不贴了，大家自己跑一下。 最小角回归这里过程比较抽象。 最小角回归是一种变量选择方法，它大大减少了逐步回归过程中的迭代次数，整个求解过程可以被控制在m步内完成. 在传统的向前逐步回归中，我们将每一个估计系数初始值设为0，找到和响应变量相关性最大的自变量，然后尽可能地在这个方向上走最远的距离，直到和当前残差值有很大相关性的另一个自变量的出现。最小角回归基于这种思想，它每一次沿着当前所有模型中变量的角平分线走，直到另一个变量的相关性大到足够进入模型. 现在具体阐述最小角回归的几何原理和迭代步骤： 我们的目标是找一个$\hat{\beta}$用于拟合,拟合值$\hat{\mu}=X\hat{\beta},$使得平方误差最小且满足条件$\sum_{j=1}^{m} \hat{\beta_j}&lt;t.$ 首先,引入一些符号表示,设$X$中的变量都是独立的,$X = (x_1,x_2,\cdots,x_m),A$是${1,2,\cdots,m}$的子集,用于记录模型中已有的变量,一开始是空的,$X_A=(\cdots s_jx_j \cdots)_{j\in A},s_j=\pm1,$其中$s_j$由该变量与残差的相关性决定,$G_A =X_A^TX_A,A_A=({1^T}G_A^{-1}{1})^{-1/2},W_A=A_AG_A^{-1}1,$单位角平分线向量$U_A=X_AW_A.$ ​ 简单给出角平分线向量的求解过程,由单位角平分线的性质$X_A^TU_A=A_A1,||U_A||^2=1,$求解$W_A$和$A_A,X_A^TX_AW_A=A_A1,W_A=(X_A^TX_A)^{-1}A_A1,$代入$U_A^TU_A=1$得$A_A^21^T(X_A^TX_A)^{-1}(X_A^TX_A)(X_A^TX_A)^{-1}1=1,$故$A_A=({1^T}G_A^{-1}{1})^{-1/2}.$ ​ 算法完整迭代步骤,在计算之前，对$X$进行中心化标准化,对$y$进行中心化操作,拟合值$\mu_A$初始值为$0,$估计系数$\hat{\beta}$初始值为$0,$计算所有变量与当前残差的相关性,即$\hat C=X^Ty,$每一个分量分别代表每一个变量与残差的相关性,找出其中最大的$C_j,$记为$C_{max},$将所有相关性等于$C_{max}$的变量编号放进集合A中，其对应的符号记录在集合$S$中,然后计算当前所有集合A中变量的角平分线,即$X_A$列向量的角平分线,这里需要注意的是之所以引入符号$s_j$是为了是列向量经过符号调整后与残差方向的夹角小于$90^。,$这样才能保证我们前进的方向是正确的,才能使拟合值不断靠近y而不是偏离y,计算角平分线到每一个变量的投影,由于数据经过标准化,也即夹角余弦值,当前角平分线的方向决定我们即将要前进的方向,而要走多远由$\hat\gamma$决定,$\hat\gamma$是使得走完这一步以后更新的残差能够与加入系新的变量以后重新计算出的角平分线平行的数值,即现有A集合中的变量以及加入的新变量与残差的相关性相同,$\hat\gamma=min_{j\in A^c}^+(\frac{C_max-C_j}{A_A-a_j},\frac{C_max+C_j}{A_A+a_j}),$可以看出$\hat\gamma$不仅告诉了我们这一步要走多远,还告诉了我们下一步加入A集的变量有哪些.依次更新$\mu_A,\hat{\beta},\hat C$,其中$\mu_A=\mu_A+\hat\gamma U_A,\hat{\beta} = \hat{\beta}+\hat\gamma \beta^+,\beta^+$为$W_A$填充$0$使维度与$\hat{\beta}$相同,$\hat C = \hat C - \hat\gamma a,​$如此迭代下去,m步之内完成求解. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134mchol &lt;- function(x)&#123; mn &lt;- dim(x) m &lt;- mn[1] n &lt;- mn[2] #检验传入参数是否符合要求 if(m != n) &#123; return (&quot;Wrong dimensions of matrix!&quot;) &#125; if(sum(t(x) != x) &gt; 0) &#123; return (&quot;Input matrix is not symmetrical!&quot;) &#125; L &lt;- matrix(0, m, m) for(i in 1:m) #以列为单位求解L &#123; L[i,i] &lt;- sqrt(x[i,i]) if(i &lt; m) &#123; L[(i+1):m,i] &lt;- x[(i+1):m,i]/L[i,i] #更新矩阵，便于下一次同样方法计算 TLV &lt;- L[(i+1):m,i] TLM &lt;- matrix(TLV, m-i, m-i) TLM &lt;- sweep(TLM, 2, TLV, &quot;*&quot;) x[(i+1):m,(i+1):m] &lt;- x[(i+1):m,(i+1):m] - TLM &#125; &#125; L &#125;mforwardsolve=function(L,b)&#123; mn=dim(L); m=mn[1]; n=mn[2] if(m!=n) return (&quot;Wrong dimensions of matrix L!&quot;) if(m!=length(b)) return (&quot;Wrong dimensions of matrix L or vector b!&quot;) #检查输入参数是否符合要求 x=rep(0,m) for(i in 1:m)&#123; x[i]=b[i]/L[i,i] if(i&lt;m) b[(i+1):m]=b[(i+1):m]-x[i]*L[(i+1):m,i] #更新右端项，逐步减去已求出的未知量对右端项的贡献 &#125; x &#125;mbacksolve=function(L,b)&#123; mn=dim(L); m=mn[1]; n=mn[2] if(m!=n) return (&quot;Wrong dimensions of matrix L!&quot;) if(m!=length(b)) return (&quot;Wrong dimensions of matrix L or vector b!&quot;) #检查输入参数是否符合要求 x=rep(0,m) for(i in m:1)&#123; x[i]=b[i]/L[i,i] if(i&gt;1) b[(i-1):1]=b[(i-1):1]-x[i]*L[(i-1):1,i] #更新右端项 &#125; x &#125;######################################################以上为一些需要用到的函数mylars = function(x,y,t)&#123; eps = 1e-8 xnames = names(x) x = as.matrix(cbind(x)) nm = dim(x) n = nm[1] m = nm[2] one = rep(1, n)#中心化 meanx = drop(one %*% x)/n x = scale(x, meanx, FALSE) mu = mean(y) y = drop(y - mu) normx = sqrt(drop(one %*% (x^2)))#标准化 x = scale(x, FALSE, normx) xtx = t(x)%*%x muA = rep(0,n)#初始化拟合值 betahat = rep(0,m)#初始化估计系数 Chat = t(x)%*%y#初始化变量和残差的相关性,因为当前拟合值为0,所以残差就是y A = NULL S = NULL k = 1 while(sum(abs(betahat))&lt; t&amp;&amp;k &lt;= m)&#123; Cmax = max(Chat) SS = sign(Chat) if(is.null(A))&#123; joinA = Chat &gt;= Cmax - eps A = (1:m)[joinA] S = SS[joinA] LA = mchol(t(scale(t(scale(xtx[A,A], FALSE, S)), FALSE, S))) &#125; else&#123; joinA[A] = FALSE joinA[-A] = Chat[-A] &gt;= Cmax - eps#已经在模型里的变量不再重复加入 for(i in (1:m)[joinA])&#123;#利用原有的矩阵更新cholesky分解矩阵 sign_i = SS[i] Lvec = mforwardsolve(LA, xtx[A, i]*sign_i) Lvec_ = sqrt(xtx[i,i] - sum(Lvec*Lvec)) LA = as.matrix(rbind(cbind(LA,0),c(Lvec,Lvec_))) A = c(A, i)#记录加入模型的变量编号 S = c(S, sign_i)#记录对应变量的符号 &#125; GA1 = mbacksolve(t(LA), mforwardsolve(LA, rep(1, length(A))))#求解GA1 AA = 1/sqrt(sum(GA1)) WA = AA*GA1 uA = scale(x[,A], FALSE, S)%*%WA#当前角平分线 a = scale(xtx[,A], FALSE, S)%*%WA#各个分量在角平分线的投影 gammahat = min(ifelse(Chat[-A]&gt;0,(Cmax-Chat[-A])/(AA-a[-A]),(Cmax+Chat[-A])/(AA+a[-A])))#找出最优的步长 muA = muA + gammahat*uA#更新拟合值 betaplus = rep(0, m) betaplus[A] = WA betahat = betahat + gammahat*betaplus#更新系数估计 Chat = Chat - gammahat*a#更新变量与残差的相关性 k = k+1 &#125; &#125; residuals = y - muA names(betahat) = xnames betahat_ = scale(t(betahat), FALSE, normx)#还原估计系数 RSS = apply(residuals^2, 2, sum)#计算残差平方和 cat(&apos;RSS:&apos;, RSS,&apos; &apos;) cat(&apos;mu:&apos;,mu) betahat_&#125;library(ElemStatLearn)data(prostate)x = prostate[,1:8]y = prostate[,9]mylars(x, y, 1000)]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|基于乔利斯基分解运算的逐步回归优化]]></title>
    <url>%2F2019%2F03%2F22%2F190322%E9%80%90%E6%AD%A5%E5%9B%9E%E5%BD%92-Cholesky%E5%88%86%E8%A7%A3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文介绍优化的逐步回归方法：基于Cholesky（乔利斯基）分解实用算法和一定矩阵计算简化方法的逐步回归。原文来自马景义老师的数据挖掘教学。 前排警告：本文变量众多，请耐心按顺序阅读研究。 理论部分：Cholesky分解法Cholesky(乔利斯基)分解法,又称为平方根法，是求解对称正定线性方程组最常用的方法之一。Cholesky分解定理如下： 若$A \in R^ {n \times n}$对称正定，则存在一个对角元均为正数的下三角阵$L \in R^ {n \times n}$，使得$\begin{equation}A=LL^T. \end{equation}$ 因此，我们可按如下步骤求解方程组$Ax=b​$：(1)计算$A​$的Cholesky分解：$A=LL^T​$;(2)求解$Ly=b​$得$y​$;(3)求解$L^Tx=y​$得$x​$. Cholesky分解 实用算法part$\begin{equation}A=LL^T.\end{equation}$$A$中的元素记为$a_{ij}$通常简单而实用的Cholesky分解方法是通过直接比较$A=LL^T$两边的对应元素来计算$L$。设 L= \left[\begin{matrix}l_{11} \\ l_{21}& l_{22}\\ \vdots & \vdots & \ddots\\l_{n1}& l_{n2} & \cdots & l_{nn} \end{matrix}\right].则$LL^T$的元素为 LL^T= \left[\begin{matrix}l_{11} \\ l_{21}& l_{22}\\ \vdots&\vdots&\ddots\\l_{n1}& l_{n2} & \cdots & l_{nn} \end{matrix}\right] \left[\begin{matrix}l_{11}&l_{21}& \cdots&l_{n1} \\ & l_{22} & \cdots & l_{n2}\\ & & \ddots&\vdots\\& & & l_{nn} \end{matrix}\right] =\left[\begin{matrix}a_{11}& a_{12}&\cdots &a_{1n} \\ a_{21}& a_{22} & \cdots& a_{2n}\\ \vdots & \vdots & \ddots\\a_{n1}& a_{n2} & \cdots & a_{nn} \end{matrix}\right].比较两边元素，有关系式： a_{ij}=\sum_{p=1}^j l_{ip}l_{jp}， \ 1 \leq j \leq i \leq n.可直接计算$L$的第一列元素： l_{11}=\sqrt{a_{11}},l_{21}=a_{21}/l_{21},…… l_{i1}=a_{i1}/l_{11}，i=2,…,n.进一步地，假定已算出$L$的前$k-1$列元素。由 a_{kk}=\sum_{p=1}^k l_{kp}^2,则可得 l_{kk}=(a_{kk}-\sum_{p=1}^{k-1} l_{kp}^2)^\frac{1}{2}.再由 a_{ik}=\sum_{p=1}^{k-1} l_{ip}l_{kp} + l_{ik}l_{kk}, \ i = k+1,…,n,得 l_{ik}=(a_{ik}-\sum_{p=1}^{k-1} l_{ip}l_{kp})/l_{kk}, \ i=k+1,…,n.由此便可求出矩阵$L$。 以上介绍可能过于数学化，下面以一个三阶方阵为例进行说明。设 L= \left[\begin{matrix}l_{11}&0&0 \\ l_{21}& l_{22}&0\\ l_{31}&l_{32}&l_{33} \end{matrix}\right].则$M_1=LL^T$的元素为 M_1= \left[\begin{matrix}l_{11}&0&0 \\ l_{21}& l_{22}&0\\ l_{31}&l_{32}&l_{33} \end{matrix}\right] \left[\begin{matrix}l_{11}&l_{21}&l_{31} \\ 0& l_{22}&l_{32}\\ 0&0&l_{33}\end{matrix}\right]= \left[\begin{matrix}l_{11}^2&l_{11}l_{21}&l_{11}l_{31} \\ l_{11}l_{21}& l_{21}^2+l_{22}^2&l_{21}l_{31}+l_{22}l_{32}\\ l_{11}l_{31}&l_{31}l_{21}+l_{32}l_{22}&l_{31}^2+l_{32}^2+l_{33}^2 \end{matrix}\right]可以以列为单位求解$L​$的元素。根据之前的叙述，$L​$的第一列元素可以很容易计算得到。另外，注意到$M_1​$删去第一行和第一列后余下的$2 \times 2​$矩阵（记为$M_{12}​$）的每一个分量的第一项为 \left[\begin{matrix} l_{21}^2 & l_{21}l_{31} \\ l_{31}l_{21} & l_{31}^2 \end{matrix}\right] =\left[\begin{matrix} l_{21}\\ l_{31} \end{matrix}\right] \left[\begin{matrix} l_{21} & l_{31} \end{matrix}\right] ​因此，在计算出$L​$的第一列元素后，从$M_{12}​$中减去上述矩阵，得到$M_2​$： M_{2}= \left[\begin{matrix} l_{22}^2 & l_{22}l_{32} \\ l_{32}l_{22} & l_{32}^2+l_{33}^2 \end{matrix}\right].​注意到这与$M_1​$右下部分的形式完全一致，可以继续计算$L​$的第二列，进而求出$L​$. 前/回代法（Forward/Backward Substitution）对于方程组$Lx=b$和$Ux=b$（其中$L$和$U$分别是下三角矩阵和上三角矩阵），可以分别采用前代法和回代法求解。此处以下三角矩阵的前代法为例。 设方程组$Lx=b$的$L$是已知的非奇异下三角阵（则其主对角线元素非零），则方程组的矩阵形式为： \left[\begin{matrix}l_{11} \\ l_{21}& l_{22}\\ \vdots & \vdots & \ddots\\l_{n1}& l_{n2} & \cdots & l_{nn} \end{matrix}\right] \left[\begin{matrix}x_1 \\ x_2 \\ \vdots\\ x_n \end{matrix}\right] =\left[\begin{matrix}b_1 \\ b_2 \\ \vdots\\ b_n \end{matrix}\right].则由方程组的第一个方程可得： x_1=b_1/l_{11};进一步地，如果已求出$x_1,…,x_{i-1}$，就可以根据方程组的第$i$个方程 l_{i1}x_1+l_{i2}x_2+…+l_{i,i-1}x_{i-1}+l_{ii}x_{i}=b_{i}求出 x_i=(b_i-\sum_{j=1}^{i-1} l_{ij}x_j)/l_{ii}从而可以求出方程组的解。求解上三角方程组的回代法的做法类似。 氦核注：利用回代法求解矩阵的目的是优化r语言内部函数的矩阵计算，提升程序运行速度。其中某些步骤使用反复回代，修改其中部分代码实现，逻辑比较复杂，需要数学验证。 实现代码Cholesky分解mchol函数mchol函数能够实现对对称正定矩阵的Cholesky分解。它以一个对称正定方阵为参数，返回其Cholesky因子。 123456789101112131415161718192021222324252627282930313233343536mchol &lt;- function(x)&#123; #目标是构造L矩阵，X = L*L&apos; mn &lt;- dim(x) m &lt;- mn[1] #行数 n &lt;- mn[2] #列数 #检验传入参数是否符合要求 if(m != n) &#123; return (&quot;Wrong dimensions of matrix!&quot;) #检查是否为方阵 &#125; if(sum(t(x) != x) &gt; 0) &#123; return (&quot;Input matrix is not symmetrical!&quot;) #检查是否对称 &#125; L &lt;- matrix(0, m, m) # 构造一个L矩阵，容纳L_ij for(i in 1:m) #以列为单位求解L &#123; L[i,i] &lt;- sqrt(x[i,i]) if(i &lt; m) &#123; L[(i+1):m,i] &lt;- x[(i+1):m,i]/L[i,i] #更新本次循环计算后的矩阵，便于下一次同样方法计算 TLV &lt;- L[(i+1):m,i] #TLM &lt;- matrix(TLV, m-i, m-i) #TLM &lt;- sweep(TLM, 2, TLV, &quot;*&quot;) TLM &lt;- outer(TLV,TLV) #L的第一列正交 x[(i+1):m,(i+1):m] &lt;- x[(i+1):m,(i+1):m] - TLM #作差消去外层，得到新矩阵，再计算下一列 &#125; &#125; L &#125; 氦核注：这个函数妙处在于，虽然由两层循环构成，但是只需要变动一个参数i就可以完成整个下三角矩阵的计算。下面是该函数的一个使用例子，直接出结果，没什么看的。 1234y=matrix(rnorm(20),5)x=t(y)%*%y #构造对称正定矩阵mchol(x)t(chol(x)) #r语言自带函数，做出来是上三角阵，我们的函数为了方便思考，得到的结果正好对称 2.3637979 0.0000000 0.0000000 0.0000001.2068850 1.7531628 0.0000000 0.000000-0.3977667 -0.2948317 0.9732456 0.0000000.8956632 0.6154149 0.6515290 1.175918 2.3637979 0.0000000 0.0000000 0.0000001.2068850 1.7531628 0.0000000 0.000000-0.3977667 -0.2948317 0.9732456 0.0000000.8956632 0.6154149 0.6515290 1.175918 前/回代法求解前代法 - mforwardsolve函数mforward()以方程组的系数矩阵L（下三角矩阵）和右端常数项b为参数，使用前代法求解方程组$Lx=b$，返回方程组的解。 1234567891011121314mforwardsolve=function(L,b)&#123; mn=dim(L); m=mn[1]; n=mn[2] if(m!=n) return (&quot;Wrong dimensions of matrix L!&quot;) if(m!=length(b)) return (&quot;Wrong dimensions of matrix L or vector b!&quot;) #检查输入参数是否符合要求 x=rep(0,m) for(i in 1:m) &#123; x[i]=b[i]/L[i,i] if(i&lt;m) b[(i+1):m]=b[(i+1):m]-x[i]*L[(i+1):m,i] #更新右端项，逐步减去已求出的未知量对右端项的贡献 &#125; x &#125; 下面是使用mforward()的一个例子。 12345y=matrix(rnorm(20),5)x=t(y)%*%yL=mchol(x); b=1:4mforwardsolve(L,b)forwardsolve(L,b) #R语言中自带函数的求解结果 0.510407168928626 0.954253346647569 1.24831106733918 3.881427677680750.510407168928626 0.954253346647569 1.24831106733918 3.88142767768075 回代法 - mbacksolve函数（与前代法结果相同，形式不同）mbacksolve()以方程组的系数矩阵L（上三角矩阵）和右端常数项b为参数，使用回代法求解方程组，返回方程组的解。 123456789101112mbacksolve=function(L,b)&#123; mn=dim(L); m=mn[1]; n=mn[2] if(m!=n) return (&quot;Wrong dimensions of matrix L!&quot;) if(m!=length(b)) return (&quot;Wrong dimensions of matrix L or vector b!&quot;) #检查输入参数是否符合要求 x=rep(0,m) for(i in m:1)&#123; #这里的循环条件是i从大到小，从下向上运算，因此得到的是上三角矩阵，原理相同 x[i]=b[i]/L[i,i] if(i&gt;1) b[(i-1):1]=b[(i-1):1]-x[i]*L[(i-1):1,i] #更新右端项 &#125; x &#125; 下面是使用mbacksolved()的一个例子12345y=matrix(rnorm(20),5)x=t(y)%*%yL=mchol(x); b=1:4mforwardsolve(L,b)forwardsolve(L,b) #R语言自带函数的求解结果 0.471045013164262 1.75905390934574 1.69960995026223 16.43691809502820.471045013164262 1.75905390934574 1.69960995026223 16.4369180950282 逐步回归减少一个变量: $Lk$ 的计算逐步回归时,在当前模型的基础之上减去一个变量,求解回归系数时,可以不需要重新计算新的$X_{k}^T X_{k}$的分解,而是可以借助上一步的分解$L$,使用Givens变换简化计算.具体的操作方法就是:如果现在要删掉第k个变量，那么对应$X_{k}^T X_{k}$的Cholesky因子$L_{k}$就是上一步的$L$删掉第k行,然后右乘Givens矩阵进行列变换,化为下三角矩阵$L_{k}$.下面举个例子. Givens变换 - gives函数（先决知识）变换的目标是$ \left[\begin{matrix} a &amp; b \end{matrix}\right]\left[\begin{matrix} g_{1} &amp; g_{2}\\ g_{3} &amp; g_{4} \end{matrix}\right]=\left[\begin{matrix} c &amp; 0 \end{matrix}\right] ​$，即通过变换，将最后一列变成0。这个$ G ​$矩阵就是Givens变换要构造的矩阵。 通常使用三角函数矩阵$\left[\begin{matrix} cos(\theta) &amp; -sin(\theta)\\ sin(\theta) &amp; cos(\theta) \end{matrix}\right] ​$。这个矩阵与自己是正交的，不影响向量的长度$ \sqrt{a^2 + b^2}​$，如$ \left[\begin{matrix} a &amp; b \end{matrix}\right] ​$中的a，b分别为3，4时，通过Givens变换的到的矩阵为$ \left[\begin{matrix} 5 &amp; 0 \end{matrix}\right] ​$， 故可以构造Givens矩阵为$ \left[\begin{matrix} a/(\sqrt{a^2 + b^2}) &amp; -b/(\sqrt{a^2 + b^2})\\ b/(\sqrt{a^2 + b^2}) &amp; a/(\sqrt{a^2 + b^2}) \end{matrix}\right] $。 首先给出构造Givens变换矩阵的函数.12345678910##构造Givens矩阵的函数gives &lt;- function(mx, lmx)&#123; mc &lt;- mx[1]/lmx #lmx是mx的长度，mc是cos，ms是sin ms &lt;- mx[2]/lmx matrix(c(mc,ms,-ms,mc),ncol=2)&#125;x1 &lt;- c(3,4)lmx1 &lt;- sqrt(sum(x1*x1))lmx1x1%*%gives(x1,lmx1) 55 -4.440892e-16 12345##原始的X矩阵和其Cholesky分解Lx=matrix(rnorm(60),10)xtx=t(x)%*%xL=mchol(xtx)L 1.8958211 0.00000000 0.00000000 0.00000000 0.000000 0.000000-1.2409637 3.60216335 0.00000000 0.00000000 0.000000 0.000000-0.5486718 -0.44502336 2.87876658 0.00000000 0.000000 0.000000-0.2459321 0.27392813 -0.52737167 2.81602139 0.000000 0.000000-0.9724829 0.52179422 0.03686773 -0.05877291 1.500547 0.0000000.3398855 -0.00289822 -0.23142886 0.80326680 -1.803605 1.795161 下面的例子显示了$L$在删掉第二行后不再是一个下三角矩阵,而是在对角线上多出来一些非零元素.要得到$X_{k}^T X_{k}$的Cholesky因子$L_{k}$,就是要通过Givens变换将这些对角线上的非零元一步步化为0.下面是以第一步化简为例. 12345##删掉了第二个变量后，对应的新的X的分解L只需在原来L的基础之上删去第二行，并使用Givens变换进行列变换化为下三角阵即可##下面是以第一步化简为例L2 =L[-2,] LL2 &lt;- L2L2 #注意！此处L2是一个5行6列的矩阵 1.8958211 0.00000000 0.00000000 0.00000000 0.000000 0.000000-0.5486718 -0.44502336 2.87876658 0.00000000 0.000000 0.000000-0.2459321 0.27392813 -0.52737167 2.81602139 0.000000 0.000000-0.9724829 0.52179422 0.03686773 -0.05877291 1.500547 0.0000000.3398855 -0.00289822 -0.23142886 0.80326680 -1.803605 1.795161 12345678mx &lt;- L2[2,2:3] #需要将L2[2,3]这个对角线上的非零元化为0lmx &lt;- sqrt(sum(mx*mx))L2[2,2:3] &lt;- c(lmx,0) #givens变换将上面加粗的位置转化成了0L2[3:5,2:3] &lt;- L2[3:5,2:3] %*% gives(mx, lmx) #该向量下方的两列元素也被Given矩阵变换了，需要更新L2%*%t(L2)LL2%*%t(LL2) 3.8059636 3.7800191 0.8021618 0.3828310 0.90209233.7800191 9.6332301 -2.7118229 0.5274355 -3.61691420.8021618 -2.7118229 7.5498592 -2.0486273 1.27509790.3828310 0.5274355 -2.0486273 3.3299015 0.63561070.9020923 -3.6169142 1.2750979 0.6356107 21.5283570 3.8059636 3.7800191 0.8021618 0.3828310 0.90209233.7800191 9.6332301 -2.7118229 0.5274355 -3.61691420.8021618 -2.7118229 7.5498592 -2.0486273 1.27509790.3828310 0.5274355 -2.0486273 3.3299015 0.63561070.9020923 -3.6169142 1.2750979 0.6356107 21.5283570 氦核注：虽然矩阵内部数字改变，但是正交结果不会变化，我们成功将该位置元素转化为0，而且没有损失信息。 1L[2:2,] -0.00705475494350042 2.05665301740704 0 0 0 0 整个化为下三角的过程可以用一个while控制. 1234567891011121314151617181920212223p &lt;- dim(x)[2] #x的列数k &lt;- 2 #被删掉的变量的下标Lk &lt;- L[-k,]Lk #删掉后第k个变量后的系数矩阵Lmk &lt;- k##从第k列起，逐列将对角线以上的那个非零元约化为0#关键句if：使进行givens变换后那两项下面的部分，进行循环更新while( mk &lt; p )&#123; mx &lt;- Lk[mk,mk:(mk+1)] #进行givens变换前的两项 lmx &lt;- sqrt(sum(mx*mx)) Lk[mk,mk:(mk+1)] &lt;- c(lmx,0) #进行givens变换后的两项 if( mk &lt; p-1 )Lk[(mk+1):(p-1), mk:(mk+1)] &lt;- Lk[(mk+1):(p-1), mk:(mk+1)] %*% gives(mx, lmx) #注：p-1是为了指示系数发生变化的位置有没有到达最后一列（因为涉及到mk+1列上元素） mk &lt;- mk + 1&#125;LkLk &lt;- Lk[,-p]Lk#与重新进行Cholesky分解的结果进行对比xtxk &lt;- xtx[-k, -k]mchol(xtxk) mgives函数上面以一个例子展示了原理.下面的mgives函数就是实现这个功能的函数.这个函数在原有的$X_{k}^T X_{k}$的Cholesky分解的$L$基础之上,利用Givens变换计算删去第k个变量后新的$X_{k}^T X_{k}$的Cholesky分解$L_k$.它以上一步的$L_k$和这一次被删掉的变量的下标k为参数,返回$L_k$. 12345678910111213141516171819202122232425mgives &lt;- function(L,k)&#123; p &lt;- dim(L)[1] if( k&gt;p ) return (&quot;Wrong input of k!&quot;) Lk &lt;- L[-k,] mk &lt;- k while( mk &lt; p )&#123; mx &lt;- Lk[mk,mk:(mk+1)] lmx &lt;- sqrt(sum(mx*mx)) Lk[mk,mk:(mk+1)] &lt;- c(lmx,0) if( mk &lt; p-1 )&#123; Lk[(mk+1):(p-1), mk:(mk+1)] &lt;- Lk[(mk+1):(p-1), mk:(mk+1)] %*% gives(mx, lmx) &#125; mk &lt;- mk + 1 &#125; return(Lk[,-p])&#125;##查看每次删去一个变量后的Lk,并与重新进行Cholesky分解的结果比较for (k in 1:dim(L)[1])&#123; print(&quot;k=&quot;) print(k) print(mgives(L,k)) xtxk &lt;- xtx[-k, -k] print(mchol(xtxk)) #可以用paste写的更漂亮些。。。&#125; [1] “k=”[1] 1 [,1] [,2] [,3] [,4] [,5][1,] 2.05666512 0.0000000 0.0000000 0.0000000 0.00000[2,] 0.15840137 3.4457975 0.0000000 0.0000000 0.00000[3,] -0.74932067 -0.6720807 3.1096792 0.0000000 0.00000[4,] -0.02895095 1.4368864 0.3519308 2.7493046 0.00000[5,] 0.52731588 1.3429521 1.0401498 0.6622768 2.21406 [,1] [,2] [,3] [,4] [,5][1,] 2.05666512 0.0000000 0.0000000 0.0000000 0.00000[2,] 0.15840137 3.4457975 0.0000000 0.0000000 0.00000[3,] -0.74932067 -0.6720807 3.1096792 0.0000000 0.00000[4,] -0.02895095 1.4368864 0.3519308 2.7493046 0.00000[5,] 0.52731588 1.3429521 1.0401498 0.6622768 2.21406[1] “k=”[1] 2 [,1] [,2] [,3] [,4] [,5][1,] 2.4663786 0.0000000 0.0000000 0.0000000 0.000000[2,] 1.0463703 3.2869014 0.0000000 0.0000000 0.000000[3,] -1.4298928 -0.2854816 2.9252667 0.0000000 0.000000[4,] 0.7702886 1.2597355 0.5508706 2.6952319 0.000000[5,] 0.2755799 1.3455560 0.9281248 0.6243126 2.316575 [,1] [,2] [,3] [,4] [,5][1,] 2.4663786 0.0000000 0.0000000 0.0000000 0.000000[2,] 1.0463703 3.2869014 0.0000000 0.0000000 0.000000[3,] -1.4298928 -0.2854816 2.9252667 0.0000000 0.000000[4,] 0.7702886 1.2597355 0.5508706 2.6952319 0.000000[5,] 0.2755799 1.3455560 0.9281248 0.6243126 2.316575[1] “k=”[1] 3 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.00000000 0.0000000 0.000000 0.000000[2,] -0.007054755 2.05665302 0.0000000 0.000000 0.000000[3,] -1.429892844 -0.75422991 2.8407433 0.000000 0.000000[4,] 0.770288640 -0.02630887 0.4336785 2.994311 0.000000[5,] 0.275579906 0.52826428 0.9607745 1.164281 2.422916 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.00000000 0.0000000 0.000000 0.000000[2,] -0.007054755 2.05665302 0.0000000 0.000000 0.000000[3,] -1.429892844 -0.75422991 2.8407433 0.000000 0.000000[4,] 0.770288640 -0.02630887 0.4336785 2.994311 0.000000[5,] 0.275579906 0.52826428 0.9607745 1.164281 2.422916[1] “k=”[1] 4 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.00000000 0.000000 0.0000000 0.000000[2,] -0.007054755 2.05665302 0.000000 0.0000000 0.000000[3,] 1.046370303 0.16199157 3.282907 0.0000000 0.000000[4,] 0.770288640 -0.02630887 1.262566 2.7495274 0.000000[5,] 0.275579906 0.52826428 1.321126 0.8128221 2.396478 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.00000000 0.000000 0.0000000 0.000000[2,] -0.007054755 2.05665302 0.000000 0.0000000 0.000000[3,] 1.046370303 0.16199157 3.282907 0.0000000 0.000000[4,] 0.770288640 -0.02630887 1.262566 2.7495274 0.000000[5,] 0.275579906 0.52826428 1.321126 0.8128221 2.396478[1] “k=”[1] 5 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.0000000 0.0000000 0.000000 0.000000[2,] -0.007054755 2.0566530 0.0000000 0.000000 0.000000[3,] 1.046370303 0.1619916 3.2829071 0.000000 0.000000[4,] -1.429892844 -0.7542299 -0.2486123 2.829844 0.000000[5,] 0.275579906 0.5282643 1.3211265 1.080541 2.288278 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.0000000 0.0000000 0.000000 0.000000[2,] -0.007054755 2.0566530 0.0000000 0.000000 0.000000[3,] 1.046370303 0.1619916 3.2829071 0.000000 0.000000[4,] -1.429892844 -0.7542299 -0.2486123 2.829844 0.000000[5,] 0.275579906 0.5282643 1.3211265 1.080541 2.288278[1] “k=”[1] 6 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.00000000 0.0000000 0.000000 0.000000[2,] -0.007054755 2.05665302 0.0000000 0.000000 0.000000[3,] 1.046370303 0.16199157 3.2829071 0.000000 0.000000[4,] -1.429892844 -0.75422991 -0.2486123 2.829844 0.000000[5,] 0.770288640 -0.02630887 1.2625664 0.546270 2.694715 [,1] [,2] [,3] [,4] [,5][1,] 2.466378596 0.00000000 0.0000000 0.000000 0.000000[2,] -0.007054755 2.05665302 0.0000000 0.000000 0.000000[3,] 1.046370303 0.16199157 3.2829071 0.000000 0.000000[4,] -1.429892844 -0.75422991 -0.2486123 2.829844 0.000000[5,] 0.770288640 -0.02630887 1.2625664 0.546270 2.694715 增加一个变量:$L_k$的计算forupdate函数上面一部分讲了减少一个变量后求解$L_k$的简化计算方法。那在增加一个变量的时候，同样可以通过矩阵运算简化计算。当在$X$的末尾再加一列$x_k$时,新的$X_{k}^T X_{k}$的分解$L_k$是在原来$L$的基础之上在底下多加了一行,多加的元素的计算可以通过矩阵运算很快的得到.下面的这个forupdate函数就是完成在$L$的基础之上计算$L_k$的功能的. 1234567891011121314151617181920forupdate &lt;- function(L, xxk, xkxk)&#123; lk &lt;- mforwardsolve(L, xxk) #xxk是新的变量在变量列表里的位置（序号） lkk &lt;- sqrt(xkxk - sum(lk*lk)) #lkk是化简用的矩阵，运算中间步骤 return( as.matrix( rbind( cbind(L,0),c(lk,lkk) ) ) )&#125;x &lt;- matrix(rnorm(60),10)xtx &lt;- t(x)%*%x#在3,2,4个变量的基础之上，加入第5个变量A &lt;- c(3,2,4) #注意：A里面带有变量的顺序，因为有些情形需要对变量的重要性等进行目标规划L &lt;- mchol(xtx[A,A]) #三个变量的xtx的分解k &lt;- 5 #加入第五个变量xxk &lt;- xtx[A,k,drop=T] xkxk &lt;- xtx[k,k] #计算扩充L时所需要的数forupdate (L, xxk, xkxk)A &lt;- c(A, k)mchol(xtx[A,A]) #和重新分解对比 2.18726152 0.0000000 0.000000 0.000000-0.31876255 2.5768506 0.000000 0.000000-0.09862756 -1.7100174 2.244832 0.0000000.17257491 0.2771965 1.469177 2.626423 2.18726152 0.0000000 0.000000 0.000000-0.31876255 2.5768506 0.000000 0.000000-0.09862756 -1.7100174 2.244832 0.0000000.17257491 0.2771965 1.469177 2.626423 逐步回归的实现(实际操作)很长的铺垫过后，我们已经得到逐步回归增减变量时简化计算的工具,下面就可以开始实现逐步回归算法了.首先导入数据并查看使用R自带的函数的回归结果. 1234library(ElemStatLearn)data(prostate)data &lt;- prostate[,-10]head(data) &gt;lcavol lweight age lbph svi lcp gleason pgg45 lpsa-0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829-0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189-0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189-1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.16251890.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636-1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 1234lm1= lm(lpsa~.,data)summary(lm1)extractAIC(lm1) # nln(RSS/n)+ 2*9, not AICstep(lm1, direction=&quot;both&quot;) 123summary(lm1)extractAIC(lm1)AIC(lm1) 结果没贴，大家粘贴一运行便知。 下面实现我们写的逐步回归。 首先是把所有变量都加入模型进行回归,求解系数,并计算该模型的RSS和AIC。 123456789101112131415161718192021222324252627282930np &lt;- dim(data)n &lt;- np[1]p &lt;- np[2]-1xn &lt;- names(data)[1:p]x &lt;- as.matrix( cbind(1,data[,1:p]) ) #将1与矩阵并联起来，将生成一列1，这是代码高手的简化写法（划重点）y &lt;- data[,p+1]xtx &lt;- t(x)%*%x #x平方阵xty &lt;- drop(t(x)%*%y) yty &lt;- sum(y*y) #y平方阵L &lt;- mchol(xtx) #进行Cholesky分解，下三角矩阵Ltb &lt;- mforwardsolve(L, xty) b &lt;- mbacksolve(t(L), tb)bRSS &lt;- yty - sum(tb*tb) ###神奇操作，如何理解？AICF &lt;- n*log(RSS/n) + 2*(p+1)AICF#A1 &lt;- rep(TRUE, p)#A &lt;- c(TRUE, A1)A &lt;- 1:(p+1)LA &lt;- LMAIC &lt;- AICFmAIC &lt;- AICFflag &lt;- rep(FALSE,p) 对于下段代码中的 RSS &lt;- yty - sum(tb*tb) 有如下解释： mforwardsolve(…)的结果为$L^T \cdot \beta$，其中$L$是$X^T X$经过Cholesky分解后的下三角阵，$L L^T=X^T X$。$ \begin{equation}RSS=(Y-X\beta)^T(Y-X\beta)\\=Y^T Y-\beta^T X^T Y-Y^T X\beta+\beta^T X^T X \beta\end{equation} $因为$\beta^T X^T Y$是一个数，相乘的项一样，因此所得结果也必然一样，所以$\beta^T X^T Y=Y^T X\beta$，所以$ \begin{equation}RSS=Y^T Y-\beta^T X^T Y\\=Y^T Y-\beta^T X^T X \beta\\=Y^T Y-\beta^T L L^T \beta\\=Y^T Y-(L^T \beta)^T(L^T \beta)\end{equation} $ 整个逐步回归的过程由一个repeat完成. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293repeat&#123; #print(flag) if( length(A) &lt; p+1 )&#123; #B &lt;- setdiff( 1:(p+1), A) B &lt;- (2:(p+1))[flag] #flag[B-1] &lt;- TRUE #print(flag) &#125;else&#123; B &lt;- NULL &#125; AB &lt;- c(A,B) bm &lt;- matrix(0,p,p+1) AICm &lt;- rep(0,p) #A &lt;- c(TRUE, A1) #a1 &lt;- (1:(p+1))[A] #c1 &lt;- (1:(p+1))[!A] #tp &lt;- length(a1) ff &lt;- 2 #print(AB) for(k in AB[2:(p+1)])&#123; if( !flag[k-1])&#123; Lk &lt;- mgives(LA,ff) tA &lt;- A[-ff] xtyk &lt;- xty[ tA ] #print(Lk) #print(xtyk) tbk &lt;- mforwardsolve(Lk, xtyk) bk &lt;- mbacksolve(t(Lk), tbk) #tA &lt;- c(1, tA) #print(bk) #print(tA) bm[k-1,tA] &lt;- bk #print(bk) RSSk &lt;- yty - sum(tbk*tbk) AICk &lt;- n*log(RSSk/n) + 2*( length(A)-1) AICm[k-1] &lt;- AICk ff &lt;- ff+1 if( AICk &lt; mAIC )&#123; mink &lt;- k mtA &lt;- tA mAIC &lt;- AICk mLA &lt;- Lk hb &lt;- bm[k-1,] &#125; &#125; else &#123; #print (k) xxk &lt;- xtx[A,k,drop=T] xkxk &lt;- xtx[k,k] Lk &lt;- forupdate (LA, xxk, xkxk) tA &lt;- c(A,k) xtyk &lt;- xty[tA] tbk &lt;- mforwardsolve(Lk, xtyk) bk &lt;- mbacksolve(t(Lk), tbk) bm[k-1,tA] &lt;- bk RSSk &lt;- yty - sum(tbk*tbk) ###神奇操作 AICk &lt;- n*log(RSSk/n) + 2*(length(A)+1) AICm[k-1] &lt;- AICk if( AICk &lt; mAIC )&#123; mink &lt;- k mtA &lt;- tA mAIC &lt;- AICk mLA &lt;- Lk hb &lt;- bm[k-1,] &#125; &#125; &#125; #print(bm) #print(AICm) #print(flag) if(mAIC &gt;= MAIC) break if (mAIC&lt;MAIC )&#123; #print(flag) flag[mink-1] = !flag[mink-1] A &lt;- mtA MAIC &lt;- mAIC print(MAIC) hhb &lt;- hb LA &lt;- mLA &#125;&#125;re &lt;- data.frame(matrix(hhb[c(TRUE,!flag)],nrow=1))names(re) &lt;- c(&quot;inter&quot;, xn[!flag])re#&#125; 将上述逐步回归的过程函数化,用一个steplm()来完成.它以样本数据(最后一列为因变量)为参数,返回最优的模型的结果. 下面以上面加载的数据data为例,使用了这个函数.并且例子显示,不论变量顺序如何都不影响最终结果. 最终函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103steplm &lt;- function(data)&#123; np &lt;- dim(data) n &lt;- np[1] p &lt;- np[2]-1 xn &lt;- c(&quot;inter&quot;, names(data)[1:p]) x &lt;- as.matrix( cbind(1,data[,1:p]) ) y &lt;- data[,p+1] xtx &lt;- t(x)%*%x #x的平方阵 xty &lt;- drop(t(x)%*%y) yty &lt;- sum(y*y) #y的平方阵 L &lt;- mchol(xtx) tb &lt;- mforwardsolve(L, xty) b &lt;- mbacksolve(t(L), tb) #b RSS &lt;- yty - sum(tb*tb) AICF &lt;- n*log(RSS/n) + 2*(p+1) #AICF#A1 &lt;- rep(TRUE, p)#A &lt;- c(TRUE, A1) A &lt;- 1:(p+1) LA &lt;- L MAIC &lt;- AICF mAIC &lt;- AICF MFLAG &lt;- c(TRUE, rep(FALSE, p)) flag &lt;- rep(TRUE,p+1) hbb &lt;- b repeat&#123; if( length(A) &lt; p+1 )&#123; B &lt;- (1:(p+1))[!flag] &#125; else&#123; B &lt;- NULL &#125; AB &lt;- c(A,B) bm &lt;- matrix(0,p,p+1) AICm &lt;- rep(0,p) ff &lt;- 1 #print(AB) for(k in AB)&#123; if(MFLAG[k])&#123; ff &lt;- ff+1 &#125; else &#123; if(flag[k])&#123; Lk &lt;- mgives(LA,ff) tA &lt;- A[-ff] xtyk &lt;- xty[ tA ] ff &lt;- ff+1 &#125; else &#123; xxk &lt;- xtx[A,k,drop=T] xkxk &lt;- xtx[k,k] Lk &lt;- forupdate (LA, xxk, xkxk) tA &lt;- c(A,k) xtyk &lt;- xty[tA] &#125; #print(A) #print(B) #print(k) tbk &lt;- mforwardsolve(Lk, xtyk) bk &lt;- mbacksolve(t(Lk), tbk) #tA &lt;- c(1, tA) #print(tbk) #print(tA) bm[k-1,tA] &lt;- bk #print(bk) RSSk &lt;- yty - sum(tbk*tbk) AICk &lt;- n*log(RSSk/n) + 2*length(tA) AICm[k-1] &lt;- AICk if( AICk &lt; mAIC )&#123; mink &lt;- k mtA &lt;- tA mAIC &lt;- AICk mLA &lt;- Lk hb &lt;- bm[k-1,] &#125; &#125; &#125; if(mAIC &gt;= MAIC) break if ( mAIC&lt;MAIC )&#123; flag[mink] = !flag[mink] A &lt;- mtA MAIC &lt;- mAIC hhb &lt;- hb LA &lt;- mLA &#125; &#125; re &lt;- data.frame(matrix(hhb[c(flag)],nrow=1)) names(re) &lt;- xn[flag] return(re)&#125;steplm(data)data &lt;- data[,c(sample(1:8,8),9)]steplm(data) 下面是R自带的逐步回归的函数. 1coef(step(lm1, direction=&quot;both&quot;)) 注：理论部分参考：徐树方, 高立, 张平文. 数值线性代数.第2版[M]. 北京大学出版社, 2013.]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘|偏差方差分解&模型评价标准]]></title>
    <url>%2F2019%2F03%2F06%2F190306%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3%26%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%2F</url>
    <content type="text"><![CDATA[提供了一种关键思想：偏差-方差分解，以及几个模型评价标准。 一、基本原理假定的真实模型为y = f(x) + \epsilon ,其中$E(\epsilon)=0,V(\epsilon)={\sigma}^{2}.$ 对给定训练集$\{(x_{i},y_{i})\}$下训练出的假设$h(x)=ax+b$，我们通过预测误差$E_{P}{(y^{}-h(x^{}))}^{2}$对其进行评价 二、基本思路及待讨论解决问题我们假定训练集样本点的概率分布满足$E_{P}[Z]=\underline{Z}$,于是 \begin{split} & E_{P}{(y^{*}-h(x^{*}))}^{2}\\ = & E[{(h(x^{*})-\underline{h(x^{*})})}^{2}] + {[\underline{h(x^{*})}-f(x^{*})]}^{2} + E[{(y^{*}-f(x^{*}))}^{2}]\\ = & Variance + Bias^{2} + Noise \end{split}然后用足够多的训练样本点$(x_i^,y_i^)$去评价假设$h(x)$，对每个给定的$(x_i^,y_i^)$，有 Variance_i = E[{(h(x_i^*)-\underline{h(x_i^*)})}^{2}]Bias_i^2 = {[\underline{h(x_i^{*})}-f(x_i^{*})]}^{2}于是我们可以得到对假定模型$h(x)=ax+b$最终的评价 Variance = E[E[{(h(x_i^*)-\underline{h(x_i^*)})}^{2}]]Bias^2 = E[{[\underline{h(x_i^{*})}-f(x_i^{*})]}^{2}]下面我们用程序来完成这个过程，并在这个基础上讨论当误差项$\epsilon$的方差${\sigma}^{2}$变化时，$h(x)$复杂度的选择问题。 三、代码部分代码思路真实函数y = 2{e}^x + \epsilon ,其中$\epsilon\sim N(0,\sigma^2)$。对$f(x) = 2{e}^x$进行泰勒展开，有f(x) = 2{e}^x = 2 + 2x + \frac{2x^2}{2!} + \frac{2x^3}{3!} + ... 于是选择不同光滑度的函数h(x)进行训练，下列公式光滑度由低到高： h(x)_1 = a_1x+bh(x)_2 = a_1x+a_2x^2 + bh(x)_3 = a_1x+a_2x^2+a_3x^3+b代码内容自定义函数lml函数$lml$的输入为训练集TR与测试集TE，输出为以TR为训练集训练出的$h(x)$关于TE的预测集。 TR和TE变量结构相同，TR为训练集，TE为测试集，第一个均为因变量 mlm以向前法的顺序，添加自变量，建立相应的回归模型，并给出与之对应的TE的因变量的预测值 预测结果排列为1个向量 12345678910111213141516171819202122mlm &lt;- function(TR, TE)&#123; vname = names(TR) yn=vname[1] #因变量名称 xn=vname[-1] #自变量名称 mp=length(xn) #自变量的维数 ypr=NULL tm=paste(yn,xn[1],sep=&quot;~&quot;) #将因变量y和第一个自变量粘贴成y~x.1的形式，建立lm函数。 fam=formula(tm) #公式化 cp=1 repeat&#123; lm1 = lm(fam,TR) #运用公式化后的lm函数对训练集TR做回归，得到相应的回归系数。 ypr=c(ypr,predict(lm1,TE)) #给出与之对应的TE的因变量的预测值 #使用向前法，依次添加自变量x.2、x.3、……、x.p，重新建立lm函数。 if(cp&gt;=mp) break cp=cp+1 tm=paste(tm,xn[cp],sep=&quot;+&quot;) fam=formula(tm) &#125; as.vector(ypr) #返回结果为TE的因变量的预测值，其为一个拉直的向量。 &#125; 自定义函数BV计算给定光滑度和方差后得到的预测函数的偏差bias和方差variance 有k个样本量为n的训练集dataTR，与一个样本量为N的测试集dataTE，其中p为做线性回归的自变量的最高阶数，$\sigma$为随机误差的标准差 123456789101112131415161718192021222324252627282930313233343536library(plyr)BV &lt;- function(p, sigma, N, n, k)&#123; ##构造两个结构相同的数据集，其中自变量维数均为p，第一个均为因变量 #构造样本数为N的测试集dataTE nx &lt;- runif(N,-1,1) #一个样本数为N的均匀分布的随机数 ny &lt;- 2*exp(nx)+rnorm(N,0,sigma) #真实函数为y = 2*exp(x)+epsilon 其中epsilon~N(0,sigma^2) nz &lt;- matrix(nx,N,p) #构造一个自变量矩阵，其中自变量维数为p,样本数为N for(i in 2:p) nz[,i] &lt;- 10*nz[,i]*nz[,i-1] #乘10为了量纲不要差太多，因为是(0,1)间的，平方会差很多 dataTE &lt;- data.frame(y=ny,z=nz) #构造样本数为n*k的训练集dataTR x &lt;- runif(n * k,-1,1) y &lt;- 2 * exp(x) + rnorm(n*k,0,sigma) z &lt;- matrix(x,n*k,p) for(i in 2:p) z[,i]=10*z[,i]*z[,i-1]; dataTR &lt;- data.frame(y=y,z=z) index &lt;- rep(1:k,rep(n,k)) ##使用自定义函数mlm，对样本量为n的训练集dataTR做关于自变量x从1到p阶的线性回归 ##得到与之对应的关于测试集dataTE的预测。 ##重复k次，计算偏差和方差。 PRE=daply(dataTR, .(index), mlm, TE = dataTE) #预测值，为一个k*(N*p)的矩阵 b = ( apply(PRE,2,mean) - rep(2*exp(nx),p) )^2 #(E(h(x))-f(x))^2 b = matrix(b, nrow=N, byrow=F) #按列排成N*p的矩阵，第j列的对应的回归模型的自变量最高阶数为j b = apply(b, 2, mean) #求不同阶数对应的预测函数的偏差，E[(E(h(x))-f(x))^2] v = apply(PRE, 2, var) #var(h(x)) v = matrix(v, nrow=N, byrow=F) #按列排成N*p的矩阵，第j列的对应的回归模型的自变量最高阶数为j v = apply(v,2,mean) #求不同阶数对应的预测函数的方差，E[var(h(x))] mse= b+v list(mse=mse, bias=b, var=v) &#125; 模型复杂度选取使用自定义函数，分别计算误差项方差$\sigma^2$为$1^2、5^2和10^2$时，不同光滑度对应预测函数$h(x)$的Bias和Variance，对比不同复杂度下模型的优劣。 12345678#定义训练集、测试集的容量、阶数p和sigma的取值N=2000; k=2000; n=500msigma=c(1,5,10) NL=length(msigma)ps=cbind(3,msigma)print(NL)print(ps) 12345678910#计算不同h(x)的bias和varianceset.seed(1)timestart &lt;- Sys.time()#每一次循环均为给定一个sigma，使用自定义函数BV计算最高阶数为p时的偏差和方差for(nl in 1:NL) print(BV(ps[nl,1],ps[nl,2], N, n, k))timeend &lt;- Sys.time()runningtime &lt;- timeend-timestartprint(runningtime) #计算运行时间 结论1. 偏差BiasBias度量了模型预测期望函数和真实函数的偏离程度，体现了模型的拟合效果。 在给定较低的噪声方差$sigma^2$时，偏差会随着模型复杂度的增加而减少；而当噪声方差过高时，模型复杂度的增加会导致预测函数偏离真实函数，即偏差增加。 2. 方差VarianceVariance表示不同的训练数据集训练出的差异，体现了模型的稳定性。随着模型复杂度的增加，模型的简洁和稳定性均会下降，这体现在方差的增加上。 模型评价与相关指标1. 推广误差及期望推广误差记基于样本$L,$ 我们通过某个方法或者算法, 例如最小二乘法得到模型$h(x；L).$ 推广误差为E_{x,y}(y-h(x;L))^2. 期望推广误差为E_{L}[E_{x,y}(y-h(x;L))^2]. 2. 误差估计方法（测试误差） 数据挖掘实践中通常按照7:3的比例把原始的样本分为训练集和测试集，基于训练集训练模型，基于测试集评价模型，评价误差被称为测试误差. 可以认为测试误差在估计推广误差，也可以认为在估计期望推广误差. 如果原始训练集如果大，上述两种看法事实上均可以. 为什么呢？ 3. 交叉验证误差 交叉验证误差指把原始数据分为$K$折, 每次去掉原始数据中的一折，构建模型，并用去掉的那折评价模型, 最后再把$K$个误差平均. $K$越大， 越接近在评价推广误差，$K$越小（当然不能太小）, 越接近在评价期望推广误差. 如果不能理解后半句，试着把期望推广误差的定义中两个期望换序. 4. Bootstrap误差 Bootstrap误差和交叉验证误差类似，每次从原始数据中有放回抽取和原始数据样本量大小一致的自助训练集，训练模型，使用原始数据中未进入自助训练集的数据评价模型. 重复多次，计算平均误差，即Bootstrap误差. 5. AIC $AIC = n\log(RSS/n) + 2k + C$ AIC在什么情况下可以用？]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不务正业|干员和玩家定位分析]]></title>
    <url>%2F2019%2F02%2F02%2F190202%E5%BD%A9%E8%99%B9%E5%85%AD%E5%8F%B7%E5%B9%B2%E5%91%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[彩虹六号干员分析一、枪械优势进攻方：t1：ash（稳定，高伤害），电车（射速，弹道规律），BB（对枪王者），glaz（高伤害，烟中恶鬼） ash一般单推比较厉害，她的枪开枪和换弹声音比较独特，能听出来2.26最新消息，R4C这把神枪的2.5倍镜要砍掉，天国的2.5（提前）众所周知电车的技能是f2（她的枪dps真的刚猛BB虽然被砍了但还是对枪王者（貌似多60血）glaz要学着了解烟的使用时间（封烟&amp;推进烟）2.26听说推进烟也要砍了。。。 t2：emp，热切（高射速/伤害，稳定），zofia，hibana（稳定，子弹少），小马哥（M4），IQ（552还行，有点用不来） emp和大锤的枪都还可以，中远距离算是不错的枪zofia还是单推hibana子弹少，还好副手冲锋枪还能打打（被砍了以后也吃屎一样小马哥枪稳IQ。。。KD和胜率奇低 弱：dokaebe（半自动），finka&amp;lion（低伤害），fuze（高伤害低射速，飘），巴西队长（射速慢） 打call比的半自动不好使，但是近距离的副武器很强悍finka和lion的枪都没啥伤害巴西队长枪比较慢ak还是有点飘的 防守方：t1：耶格（唯一步枪），rook&amp;doc&amp;echo&amp;mastro（四个2.5倍镜），bandit（射速快），ela（射速） 步枪意味着远距离衰减低2.5也是提供远程火力（双胖mp5被砍了伤害，但还是很强）班迪是职业赛击杀位，还有那么强的技能，简直太强了，mp7机瞄也贼好用ela，哭 t2：诡雷&amp;刘醒&amp;白裤裆（激光枪），毒气&amp;WIFI（dps最高枪smg），mira（高射速vector）…… 激光枪意味着伤害要被砍，但是不管什么伤害，爆头就一枪smg司马光这把枪猛mira子弹比较少 弱：女鬼&amp;女武神&amp;心跳&amp;龙鳞板（稳定，但是糖豆发射器），alibi&amp;夹子妹（低伤害） 女鬼手枪被砍，但还是很强，主武器垃圾李白和架子妹都没啥伤害，枪基本不用压 二、战术优势进攻方 t1：lion（限制防守方走位），emp（不用解释），热切&amp;hibana&amp;小马哥（“切”位），dokaebe（情报），Buck（垂直进攻），glaz（烟中恶鬼） lion被官方ban了emp变相加强切位基本不可少电话妹算是辅助buck垂直和单推都厉害glaz。。。 t2：大盾（推进，获取信息），闪盾&amp;脚气（抓野推进），巴西队长（多功能），ash&amp;zofia（推进，平衡型），IQ（针对性强） 大盾，萌新就别玩了，不开盾的大盾是傻x，挡队友的大盾是傻x，知道进不去还一根筋在门口蹭的大盾是傻x闪盾，是火车王哒巴西队长的火+烟IQ主要针对女武神、心跳、白裤裆之类干员 谨慎选择：ying（先问自己你能自己进点一打多吗），fuze（有一起打垂直的配合吗），电车（玩一局小车） 那个女人进点了我就是要救人质，-500黑镜我来了 娱乐选择：fuze（救人质），finka（均衡），大锤（自己开路，审问女鬼） 防守方 t1：女武神&amp;clash&amp;mira（强情报），bandit&amp;凯德（电工），耶格（阻止投掷物）,echo（阻止进攻），smoke（阻止推进下包） 别说了，强的女武神黑镜会让人怀疑人生，黑镜也被称作另一个女人、那个女人clash新干员没怎么使过，感觉需要很懂的队友电工比较重要耶格echo阻止推进能力max，ban掉echo也是职业赛基本操作毒气就平凡一些，不过改造地形的英国喷子挺好用 t2：邪眼（情报），WIFI（限制情报），龙鳞板（改造地形），心跳（情报），刘醒（蛊针情报），白裤裆（隐蔽，威慑），ela（震动棒），女鬼（技能躲猫猫） 意大利助攻王，可以在关键位置阻止下包WiFi是小车噩梦龙鳞板萌新不要碰心跳有些图好用，垂直防守能力强刘醒枪好使，针也能提供很多情报白裤裆更多是一种威慑踩了ela 的棒棒，那就基本要被骑脸了不要被女鬼贴脸！（闪盾除外 娱乐：诡雷&amp;夹子妹（防rush，触发情报），机枪哥，啊李白（真假李白） 三、单杀优势1.all快枪手，天下武功唯快不破，代表干员：ela、bandit 2.高伤害+稳定，代表干员：小马哥，ash，zofia，热切（尽管我觉得热切枪不好压 3.女鬼 4.大盾、闪盾（开盾时机是单向情报，可以成为优势因素） 四、战术配合1.推进木头人电话妹、lion 能够暴露或限制敌人走位，是rush的好伙伴 hibana/热切+glaz 开洞，封烟架枪，小车探点，标记，突进击杀 大盾 推进，和队友站成交叉火力，开盾形成威胁，左右摆头会被echo完克，被震到会开盾，很危险 2.骚扰威慑1.小马哥 加固墙无声开洞，多开几个来回切换防止自己被反打 2.刘醒、诡雷、架子妹 陷阱干员，阻止快速突进，同时能获取一定信息 3.白裤裆（男鬼）、女鬼（混迹野区） 旁边有个白裤裆找不到在哪。。。自带迷彩cav是捉迷藏专业毕业，专治脚气 4.电车 专克手残，打不着看到黑镜就开气泵 彩虹六号玩家定位分析一、首先是分类1.anchor镇场选手特质：优秀的意识，过硬的技术（枪刚，身法好） 2.fragger击杀型选手特质：枪刚，个人发挥优秀 3.roamer游走选手特质：信息获取敏感，时机抉择果断 4.supporter支援型选手特质：道具支援，交叉火力支援 5.lurker潜伏选手特质：一般是架枪，打野，会保护自己的位置信息，或者做到声东击西 6.AS-supporter激进支援型选手特质：处理死角位，封烟断连接 这些分类依据的是玩家的个人实力，并且是你在一支反恐队伍中的位置。路人局中我们常常会选出Ash，Sophia，IQ，Buck之类的自由人干员，有很强势的枪械优势，同时也有着一定的突破、侦察、清理障碍和击杀能力。 一般而言，相近水平的团队之中，不存在简单的纯位置，队伍也不存在核心和非核心之说。我们完全可以在战术之上选择自己熟练的位置进行游戏，不论是辅助还是自由人，都有一定的作用。 之前提到的大盾+lion阵容，就可以作为强势推进组合，只需要两个人就可以在lion的两次技能后形成火力区域与击杀后的人数优势。 还有我们耳熟能详的热切+emp阵容，双辅助切墙可以很好的带领团队突破或架枪。类似的是，如果有人能打开顶板或一些关键墙面形成火力网，将对防守方造成很大的防守压力。 二、养成计划正如上面所言，有着如此多的定位（其实也不多），首先找到自己的位置，确认你自己适合的几个位置。 笔者本人属于枪水人傻型（好了，不适合这个游戏，下一个），除了地图记得比较清楚没什么本事。打了五百小时后，大概觉得自己适合游走、支援和潜伏。不论选什么干员，都能看出自己的作战风格。因为架枪往往打不赢拉枪的，于是宁愿让炸弹守家，也会尝试改变（架非常规位置或者绕后）。 移动：奔跑速度快，但是声音很大。在建筑内，如果没有绝对的信息显示无人在附近，或者位置已经暴露，才可以奔跑。蹲姿走路稍慢，但是移动声音大幅减小，是建筑内移动的选择之一。同时蹲姿爆头线较低，站姿爆头线较高，二者之间的距离在拉枪时要能快速切换。站姿是最普通的姿势，移动较快，比较适合短距离快速转移。注意所有移动的同时一般不开镜，架枪移动除外。 对枪：利用猎杀kb份子学会拉枪线：蹲姿站姿爆头线（现在的kb份子都学会蹲起骗枪了。。。我们也可以用于身法）。闪身（peek）在实战中可以和预开火（prefire）结合，利用掩体进行一定的拉枪，保护自己不被架枪人秒杀。peek一到两次后建议换位置，再peek容易死。下趴枪（dropfire）不考虑，以后应该都不行了。开镜时间，看自己，近距离不开镜也可以，开镜同时就可以开枪，此时子弹打到的位置还是瞄具准星。甚至在近距离拼枪时，使用手枪用单身20年手速快速射击都要精准很多（下版本要砍了，单身狗没狗权啊）。 开局处理：开局时一定要看/打摄像头，被打掉的前两个摄像头能看出复活位置大概方向，进攻时一样，要记忆一下摄像头位置。进攻前小车尽量不要丢，利用充足的信息能够很快击杀敌人，小车时进攻方推进、反打野的重要道具。 进攻：单人小车使用时，不要离自己太远。有队友在身边时，小车推进可以让队友帮忙架枪或者跟车一起推进。部署小车声音很大，能被隔壁的人清楚地听到，这一点反过来同时可以作为诱饵，逼迫/引诱敌人出现，所以一定要使用进阶部署无人机，在部署之后可以架枪保护自己。推进时打摄像头（包括防弹、黑眼）。如果敌人选择了陷阱干员，一定要留心必经之路（架子：楼梯、窗下、路边；刘醒蛊：门口，一般看不见。。。；edd：大门角落，窗口，不要破门跳跃，一定要把门板打掉再进入；echo：天花板，被震了不要跑，越跑越黑）。道具不是白给的，闪光弹突进、烟雾弹封路封视野（突破黑镜防守时）、阔剑地雷防止绕后（队伍里一定要有）、贴片炸药一般不带。多用道具，形成道具优势，同时小心道具会暴露自己位置，敌人的c4也会礼尚往来。 防守：多看摄像头（没事的时候），注意声音，有声音时适当穿墙穿门穿窗射击抽奖。道具也要多用，但是多注意自身周围情况，有没有摸过来的进攻方（摔炮可以开墙、注意摔炮压血线效果不是很理想、c4抛物线越过障碍和掩体击杀敌人、c4还可以从楼下击杀可破坏地板上的楼上敌人、铁丝网放在很恶心的位置、机动盾看情况和需求携带） 残局处理：非常考验能力的时刻，多打少没什么好说的，获取信息，形成交叉火力，不浪就ok。少打多的时候，首先屏蔽沙雕队友，不要看聊天窗口，获取信息要靠自己，同时要冷静。击杀步骤如下：发现敌人，击杀敌人，架枪，换点，换弹夹，再获取下一个敌人信息。注意，一定要冷静，相信自己1v4和1v5还是可以做到的。 （未完待续）]]></content>
      <tags>
        <tag>彩虹六号</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[规划一波|研究生考试复习企划]]></title>
    <url>%2F2019%2F02%2F02%2F190314%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[考研英语规划基础阶段：（3月1日~5月30日）真题：1997-2004年 共37篇 语言角度1.单词都要认识2.切分句子，能翻译最好3.找出处，词汇和句子，记忆情境 信息角度选出关键句，每篇文章小于10句 解题角度四月初开始做题解题思路（定位、替换、排除） 参考书目每日一句（要抄下来，练字）时文精析（先看中文，再看英文） 五月开始练习写作，和四六级一起 强化阶段：（6月1日~8月31日）真题：2005-2013年 共36篇6月考试月作文：训练“思路+语料”的写作思维 巩固拔高：（9月1日~10月30日）真题：2014-2018年 共20篇过两遍真题时文精析21版（8月出？）作文：每日两到三个语料]]></content>
      <tags>
        <tag>学习生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析报告|Valve-Steam“买断制”游戏平均价格时间序列模型]]></title>
    <url>%2F2019%2F02%2F01%2F190201ts%2F</url>
    <content type="text"><![CDATA[最后编辑于：2020.01.02 14:18 Valve-Steam“买断制”游戏平均价格时间序列模型一、背景作为精神文明消费品，游戏已成为每一个人或多或少都会接触的事物。游戏消费已经成为当代大学生重要的开支，而valve-steam游戏软件消费平台作为近乎垄断的最大的pc（个人电脑）游戏中介平台，已成为大多数电脑游戏玩家不可或缺的电脑软件。 很多游戏也有“年货”之称，而很多游戏的发布时机和销售价格都有一定的变化规律。本研究将基于steamspy中的steam游戏消费数据，希望探索对游戏的月平均价格的变化趋势。 （数据来源于steamspy） 二、变量说明 变量名 含义 变量水平 变量简介 AVERAGE_PRICE Steam平台上售卖的游戏的平均价格 连续型 发行游戏总价值/发行游戏总数 Time 月平均价格对应的时间 整数连续 04年1月到18年12月 Value-Steam平台售卖的游戏软件只计算了买断游戏的价格，不包括游戏内购项目的花费。因此本报告仅以“买断制”游戏的平均价格为研究对象。同时，为研究游戏市场整体的行情，我们不以游戏销量作为权重，单纯研究价格的变化趋势。 三、探索性数据分析“买断制”游戏往往受公司销售业务安排的影响，拥有固定的折扣期，折扣期能很好地促进游戏的销量。value-steam游戏销售平台也拥有自己的折扣期，这使得游戏价格在原有基础上产生了上下波动的现象。 根据图1，我们能看出平均价格总体呈现增长趋势，并且有一定的波动模式。随机性趋势有但是不明显，且没有非常明显的季节趋势。 自相关图和偏相关图都表明有一定的自相关性，在二阶以前都存在显著不为零的部分，能够初步识别时间序列的ARIMA模型阶数。 四、模型探索1.简单线性回归首先使用简单线性回归模型对数据进行简单拟合。以AVERAGE_PRICE（平均价格）为因变量，以Time（时间）为自变量进行线性回归。 图二展示了用最小二乘线性模型拟合数据的结果，线性拟合只能拟合出线性增长的趋势，但是完全无法显示出随机波动的趋势。 estimate Std.error T value Pr(&gt;t) (Intercept) 3.957189 0.222438 17.790 &lt; 2e-16 *** x 0.009442 0.002155 4.381 2.03e-05 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Multiple R-squared: 0.09832, Adjusted R-squared: 0.09319 F-statistic: 19.19 on 1 and 176 DF, p-value: 2.028e-05 虽然拟合参数都显著通过，通过观察调整的R方，我们能够看到模型的解释能力不足10%，拟合效果并不好。因此我们采取数据变换的方式尝试调整模型。 2.对数线性模型对原始数据做对数变换，再使用线性模型拟合。 estimate Std.error T value Pr(&gt;t) (Intercept) 1.2602098 0.0495757 25.420 &lt; 2e-16 *** x 0.0028057 0.0004804 5.841 2.46e-08 *** Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Multiple R-squared: 0.1624, Adjusted R-squared: 0.1576 F-statistic: 34.11 on 1 and 176 DF, p-value: 2.459e-08 拟合参数依然显著通过，通过观察调整的R方，我们看出模型有了一定的优化，但依然拟合不好。可能不是数据的问题，所以接下来我们对模型进行调整。 图3表明，对数变换后的数据虽然有很好的上升趋势，但是随机波动也随之变大，模型太过简单，损失了很多波动信息。不应采用。 五、时间序列模型识别1.ARIMA时间序列模型首先，我们对模型进行一阶差分，去数据的趋势，削减模型的不平稳性。 通过看到数据趋势基本消除，模型虽有波动，但是趋于规律。观察自相关函数和偏自相关函数，发现自相关性，初步推断模型是ma1与ar4的结合。通过R软件自动拟合，我们能够得到下表估计的模型系数。 ar1 ar2 ar3 ar4 ma1 intercept 系数 0.1020 0.3871 -0.1103 0.0033 -1.0000 0.0099 s.e. 0.0753 0.0757 0.0767 0.0811 0.0192 0.0032 sigma^2 estimated as 1.833: log likelihood = -307.08, aic = 628.15 我们发现ma1的系数为-1，该ARIMA模型不可逆，于是尝试增加一阶来拟合。利用R软件得到新拟合模型估计的参数。 ar1 ar2 ar3 ar4 ma1 ma2 intercept 系数 -0.7327 0.4642 0.2125 -0.0646 -0.1614 -0.8385 0.0099 s.e. 0.2219 0.0975 0.1262 0.0887 0.2085 0.2082 0.0032 sigma^2 estimated as 1.83: log likelihood = -306.91, aic = 629.81 可看到，如此调整，模型的aic有稍许的增加。但是模型平稳可逆，性质好。 2.模型评价首先对模型残差进行shapiro正态性检验，利用R软件输出结果如下： W = 0.93636, p-value = 4.695e-07 拒绝了残差正态性假定，说明模型拟合的还不够好。 接着分析模型自变量因变量的残差，进行Ljung-Box检验，利用R软件输出结果如下： X-squared = 1.7957e-06, df = 1, p-value = 0.9989 对残差进行游程检验，p值0.881，不能拒绝其纯随机假定。 再进行McLeod-Li检验，看到模型不存在条件异方差。 根据图5，观察模型残差的Q-Q图也可知残差无正态性。 以上分析表明，非季节项表明可以使用的模型为arima(4,1,2)或arima(4,1,1)，考虑模型更好的性质，使用ARIMA(4,1,2)更优秀。 3.乘法季节模型根据原始数据的PACF偏自相关（图7）看出，在季节性周期lags12处有spike，因此进一步做季节差分。 在进行季节差分后，对差分后数据进行探索性数据分析。画出趋势图、ACF和、PACF图进行研究。 由图8能够看出，季节差分后，模型趋势几乎完全消失，只剩下周期性较强的波动和随机性特点。自相关函数和偏自相关函数都显示了模型的一定特征，再lag为12处依然能看出较强的相关性，但是其他位置也存在一定相关性。 所以对于非季节项，我们只做了一阶非季节差分，故d=1,从PACF看出p=4，从ACF看出q=2。 而对于季节项，也只做了一阶季节差分，故D=1，再看在lag12,24处看是否有spike,从PACF看出P=3,从ACF看出Q=1。 因此综上所述，识别阶段最终确定的模型为Arima(4,1,2)(1,1,3)[12]。 六、模型评估利用R软件，对原始数据进行自动拟合分析，得到以下含有趋势的需要进一步差分。因为对aic计算时利用了近似，可能产生一定的误差。 ARIMA(2,1,2) with drift : 624.6381 ARIMA(0,1,0) with drift : 748.4041 ARIMA(1,1,0) with drift : 646.942 ARIMA(0,1,1) with drift : 647.6507 ARIMA(0,1,0) : 746.4034 ARIMA(1,1,2) with drift : 631.9857 ARIMA(3,1,2) with drift : 626.6319 ARIMA(2,1,1) with drift : 631.1114 ARIMA(2,1,3) with drift : 625.6269 ARIMA(1,1,1) with drift : 645.4117 ARIMA(3,1,3) with drift : 623.1056 ARIMA(3,1,3) : 621.1225 ARIMA(2,1,3) : 626.5835 ARIMA(4,1,3) : 608.4453 ARIMA(4,1,2) : 606.3408 ARIMA(3,1,1) : 624.4274 ARIMA(5,1,3) : 611.0288 ARIMA(4,1,2) with drift : Inf ARIMA(3,1,2) : 624.5451 ARIMA(5,1,2) : 609.452 ARIMA(4,1,1) : 612.635 Now re-fitting the best model(s) without approximations… ARIMA(4,1,2) : 632.7907 ar1 ar2 ar3 ar4 ma1 ma2 系数 -0.3316 0.4072 0.0356 -0.0492 -0.5276 -0.3943 s.e. NaN NaN NaN 0.0543 NaN NaN sigma^2 estimated as 1.971: log likelihood=-309.06 AIC=632.13 AICc=632.79 BIC=654.36 Best model: ARIMA(4,1,2)，即依靠AIC筛选出非季节项的模型是ARIMA(4,1,2)，模型的精确aicc为632.7907，bic为654.36，选取该模型和我们之前的分析的选择相同。 七、模型预测用确定的模型Arima(4,1,2)(1,1,3)[12]预测接下来的五个阶段的平均价格。利用R软件得到的预测值见下表。 Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 1 6.878770 5.079546 8.677993 4.127095 9.630444 2 6.508401 4.691427 8.325374 3.729579 9.287222 3 6.068549 4.087607 8.049491 3.038959 9.098139 4 6.042168 4.060691 8.023644 3.011761 9.072574 5 5.920171 3.904982 7.935360 2.838205 9.002137 Lo指下界，Hi指上界；80、95指显著性水平0.2、0.05。 短期内呈现下降的波动趋势，但是长期来看却呈现上涨态势。预测区间较大，长期预测精确度不够。 八、原因探讨与结论由于物价上涨，生活成本不断提高，游戏购买价格也随着游戏开发成本不断增加而上涨，但是由于游戏独特的促销手段，使其价格受折扣波动非常明显。而每年都有固定的折扣季，促进了游戏的消费。 但是同时需要注意的是，预测范围还是较大，长期精度较低，但是产业总体的发展趋势依然较好识别。建议消费者在购买游戏时，尽量赶在游戏的折扣期购买。相关企业进入市场时，也要注意游戏销售的价格。 参考文献[1]刘杰华.2018年上半年游戏产业报告[R]. 北京: 伽马数据（CNG中新游戏研究）.2018.]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[京东数据分析心得]]></title>
    <url>%2F2018%2F12%2F14%2F181214%E4%BA%AC%E4%B8%9C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BF%83%E5%BE%97w%2F</url>
    <content type="text"><![CDATA[京东数据分析心得w报告人|刘亚男1.知识储备2.分析思路与案例3.产品 1.知识储备对象：政府，企业，个人 统计+计算机。。。 方法：持续跟踪，模型算法，定制研究 大数据分析京东：电商为主，41000台服务器 数据量级支持的分析内容不同，以京东为例 1.内部业务创新2.经济指数3.行业报告4.高校政府科研5.区块链、供应链溯源（数字货币，支付盐酸，数字票据）6.平安城市、智慧城市 知识储备1.数据 合理的样本数据，拿到的数据可能是有偏误的，注意样本多样性。 有质量的样本数据，小心空值错误值，数据全周期全链条。指标唯一性。 eg数据中台：阿里全领域，京东全链路（2016年开始，趋势） 2.方法硬方法：描述统计，假设检验，列联表，判别，聚类，ROC等…… 在一定假设前提下，科学严谨的推导。数据分析报告很高大上。。。 并非必须做相关分析之类，更多会使用软方法 软方法：如用户分层 3.延伸企业中，知识+场景 2.思路1.界定问题2.分解问题3.分析问题 目的：找到原因，落实方案 经验能够给出描述，精准地找出定义，找到最根本的原因。 界定问题SMART原则 S具体M目标要指标化A可行动R目标相关性，要能掌控T时效性 利用五个为什么，确保研究的有效性 麦肯锡的总分金字塔 分解问题分解方法1：议题树（结构化思维）下层需要上层的支持（广度上一致性，深度上相关性） 首先不需要很全局的思考，从几个具体问题开始思考，挖掘分论点传播 -&gt; 人群 推崇团队，不同的人解决不同的问题，方便达成共识 分解方法2：MECE原则（相互独立，完全穷尽） 分解方法3：完整论证链路，组织故事，表头，易懂（PPT贼赚钱） 3.数据分析背后的产品思维营销，数据分析产品经理 业务知识，行业背景，技术 在垂直领域中做深 玻璃瓶子换塑料瓶何易拉罐 为何饮料瓶的颜色区别？年轻人感官刺激 感受：耐摔，印刷，方便喝，环保结构化：商家：成本，运输，印刷，质检。用户：便捷，口感，爽感，品牌流程化：原料，调制，罐装，印刷，质检，运输，摆放，购买，引用，回收 六阶1.感性2.结构化，流程化3.聚焦，关键节点4.价值5.抽象6.行业生态，发展 人性 &gt; 理性 时间管理，数据分析能力提升，行业探究，针对已有产品进行功能性迭代，PPT制作 项目管理和汇报，梳理价值，独立负责能力]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|突然之间克哈的霓虹都为我闪烁]]></title>
    <url>%2F2018%2F11%2F30%2F1130%2F</url>
    <content type="text"><![CDATA[星际争霸人族战术收集（版本4.7.1开始）多年以后一场大战惊醒沉睡的我，突然之间克哈的霓虹都为我闪烁 东方有股神秘的力量向我伸出了手，是一峰和老仙依旧在远方为我在守候 好了不扯淡了，星际争霸2作为一款高门槛即时战略（rts）游戏，让无数英豪望而却步。氦核也是深受其荼毒的三十万猛男中的一员。 战术某种程度上也可以叫建造序列（build order）。一般一个玩家的bo就反映了其生产序列和战术意图。 就不多说了，下面就让我们直接进入无敌泰伦的世界！（然后被干死 TVZ天火三玄变鉴于天梯环境恶劣，TVZ对抗中虫族在运营上比人族更加强势，所以天梯虫族的九阳神功三开狗毒爆压制很多。 如何让这套九阳神功成为九阳豆浆机，就成了每个人族玩家应该思考的问题。 天火三玄变战术建造序列戳这里 对付虫族偷逼一打一个准，骚到一斧子就能滚雪球，干死zerg！！ （持续更新）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TVZ天火三玄变]]></title>
    <url>%2F2018%2F11%2F30%2F1130TVZ%E5%A4%A9%E7%81%AB%E4%B8%89%E7%8E%84%E5%8F%98%2F</url>
    <content type="text"><![CDATA[TVZ天火三玄变鉴于天梯环境恶劣，TVZ对抗中虫族在运营上比人族更加强势，所以天梯虫族的九阳神功三开狗毒爆偷逼很多。 如何让这套九阳神功成为九阳豆浆机，就成了每个人族玩家应该思考的问题。 正常开，兵营气矿，气矿满采，兵营好时求稳拉第一个农民看对手开局，基地变星轨（都是基操，勿6）兵营造死神（建议送死神开）同时可以拍下二基地。 星轨变好路口补一个房子，探路农民停在三矿，两分十秒下vf，接着有气就给兵营挂上双倍挂件。采矿14农民时下二气（也是满采）。死神随便骚一骚，不建议送，停在另一个三矿或者跑一跑。 vf换上挂件，开始产火车（不停）。下vs挂科技挂件。农民不停，兵营好兄弟不停（大概能造三个）。 三分钟看三矿。 三分二十秒之前极限下va。vs造运输机。两火车带死神骚一骚，剩下的火车藏在家防绕狗。尽早开始堵二矿口，同时小心卡人口。。。（多么痛的领悟） 第一变 青莲变 极限你爹侠六火车（家里四个，外面俩）带运输机（刚造好）三枪兵（运营好一点可能更多些。。。） 前压 停火车，兵营和vs挂科技挂件（升级兴奋剂和隐形女妖）出枪兵和俩女妖，下三基地。vf起飞再造一个双倍挂件 五个女王是干不过火车侠带运输机的，对面送狗最好不过。 火车接着骚的时候，家里vf起飞再挂科技，出坦克。农民补两兵营（在空挂件上）和两be（正常运营攻防） 火车顶脸。可以死，但是要多换卵换狗换农民，杀女王。（卡毒爆虫巢timing，不会有毒爆虫出现。。。） 第二变 天地变 速接两女妖三基地变星轨时就可以下三四气。如果防守不够猛烈，小心飞龙开 家里开始出坦克，枪兵。6分十秒升攻防。两女妖出门，补到五兵营。下防空（防飞龙开） vs起飞挂双倍出运输机，三基地飞出来 女妖骑脸！！！杀杀杀 对面变眼虫 三基地下两气，造房子堵三矿路口，两兵营下双倍挂件 第三变 琉璃变 坦克连环拳两坦克出门，后续兵等在家 这波兵有啥打啥，坦克架住，清房子，压基地。谨记：毒爆来了就装船。。。 九分钟左右家里补到八兵营，双vf坦克，余钱拍基地。攻防不要忘。一百五十人口一拳超人。 总结没什么好总结了，干死z，找回属于泰伦的荣光！！！]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闻歌如面|乱世圣人]]></title>
    <url>%2F2018%2F11%2F22%2F20181122%E4%B9%B1%E4%B8%96%E5%9C%A3%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[原曲《漠殇》（漆柚）氦核填词 段1天生我才 应执铎弄澜走重关 总施善 风乎舞雩 盛世梦幻潇潇雨阑 何人 衰草倚栏 提笔心丹访名山 拜灵潭 窥得古今妙谈 曾舞经阁诗百三 偶结古贤列仙班醉醒浊清解无端 百口辩是非为难江湖纷乱书生灾 深知乱世正道无人传闻道不论晚 可贵此生术业专 副歌1入世何难 人心水一碗或治或乱 不与易兼善列国辗转 刍狗元元一般书生微命 兴亡责担 段2大道至简 演繁千千万天下瞒 若寒蝉 各扫门前 无事相安江湖纷乱 何干 天地长久 若水上善唯不争 无所患 莫能与我争断 桑田沧海朝代换 知黑守白也坦然制礼自有人犯难 又何妨不见经传任士仁人多名满 却由大国陷唇亡齿寒百姓无聊生 便以这天地为棺 副歌2出世何难 乱世怎偏安曳尾于潭 富贵不在贪生也有涯 总羡无涯浩瀚桑田沧海 转瞬荏苒 过片出入何难 任尔兵戈乱或许平凡 飒踏亦无憾 副歌3功遂不还 立书桃李自满或守或战 三千叶开枝散这群雄纷乱 往事回首那堪百家千言璀璨 何人笑谈 破立 聚散 讨教 闭关 求道多艰难沉默 辩白 生死 决断 卫道总徒然（完）]]></content>
      <tags>
        <tag>歌词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|理解游戏，享受理解]]></title>
    <url>%2F2018%2F11%2F12%2F181112%E7%90%86%E8%A7%A3%E6%B8%B8%E6%88%8F%2F</url>
    <content type="text"><![CDATA[寻找神奇|理解游戏，享受理解好久没有写东西的 氦核核前些天忙碌，生前身后，满面烟火。等到尘埃落定忙完那天，我又一次下载了炉石，想开上几包玩玩，放松一下。我以前打过一阵的炉石，但可能不到50场，也没有氪过金，更没怎么看过直播了。只是因为我是个暴雪玩家，对炉石既陌生又熟悉。 一个同学和我一起接触的这款游戏，那时我们才刚刚初中毕业，还在那个玩保卫萝卜的年龄。但是他入了炉石坑就再没有出来，我则打完几个英雄后就再没有碰过炉石。更早的启蒙就是和弟弟毫无规则爽了就好的盗版游戏王，后来也玩过简单的万智牌，类似的卡牌游戏也都尝试过，都不了了之。 精挑细选选了一套机械战士卡组，在休闲中打了几盘，觉得还不错，但是总觉得哪里还差点。结果遇见会玩的或者老玩家总能暴打我。不禁开始思考，牌组也不是那么弱，combo也有思路，但是在我手里怎么就打不出效果呢？ 于是找到了那个一起入坑的老同学，和他倾诉以后他决定和我打两盘，用一模一样的卡组。只用了一盘，他就得出了结论：牌还不错，combo有想法，不过“过牌”少了点。 我这才知道了“过牌”这个概念，就是将牌库里的牌过到手上或者场上，利用一些随从的技能或者魔法就能实现。而一场战斗如果仅仅靠每回合开始抽的那一张牌来打，那胜算肯定比同时掌握自己卡组几张核心牌要高。 摸到炉石门道的我十分兴奋，给自己的卡组加入过牌以后，战胜对手的几率又高了一些（当然还是被吊锤）。对游戏的理解是玩好这个游戏的要务，而获得这种理解需要的是长时间的琢磨和思考，需要大量的对局，或者需要过来人的经验。这就是我们能从主播或者up的视频中学到的东西。 我很少能注意到自己对一个游戏的理解。就像我打星际争霸一样，玩过的人都知道用雷神打飞龙，用不朽锤蟑螂这种基本克制关系，可是这就是对游戏单位的理解。而即时战略游戏需要的不仅仅是理解单位，还有更多的东西需要注意。 大哥孙一峰说过一句话：学会了哥的运营，剩下的就只有a了。战术是星际的核心，而运营就是战术的载体。运营就是建造or训练的顺序，精确的运营可以在一场游戏的前几分钟里做到完全复制一个战术。而战术才是考验自己的理解（大多数会模仿职业选手的思路），其他决定胜负的70%都是拼手速和眼力。 我不喜欢玩游戏时的这种记忆和刻板，喜欢随心所欲，但是那样招致的后果就是被克制，或者在数量和效率上不敌对手。游戏是需要思路的，而仅仅有思路也是没法在竞技层面上击败对手。但由此产生的很多游戏无限放大了竞技的元素，而忽略了思路的元素，而我真正喜欢的便是思路的元素。因此很多时候我并不能享受我玩过的大多数游戏，不是找不到快乐起来的point，就是那个游戏太过浅显，缺少持续促进我深入享受的动力，玩过一两个月的手游说扔就可能扔了，毫不惋惜。 彩虹六号，战舰世界，红色警戒，星际争霸，还有极其简单的moba风暴英雄，理解在某种程度上决定了我们在游戏中水平的高度，剩下的才是技术层面的因素。我享受这样的游戏。以前觉得“玩游戏，就是要赢”这话没错，但现在不是了，或者说不仅仅是了。 （完）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闻歌如面|晚安，今秋]]></title>
    <url>%2F2018%2F10%2F20%2F181020%E6%99%9A%E5%AE%89%E4%BB%8A%E7%A7%8B%2F</url>
    <content type="text"><![CDATA[少年不识愁滋味，爱上层楼，为赋新词强说愁；而今识尽愁滋味，欲说还休，却道天凉好个秋 晚安，今秋文|氦核灯火夜终阑 今宵却无眠 清秋来气爽 迁客赋悲怜 有朋去远方 三叠不足别 瑶池舞榭忙 金车向天边 花不语 人不语 清风弄月绸 雾里花 傲枝头 却笑我怕羞 聚散忙 亲故走 尘债白日纠 几杯过 又念愁 游子归梦休 凉薄好梦夜 过客梦难填 不觉粉巷浅 但少一人闲 俯仰促湖舟 叶下知秋渐 千年只一念 秋水共长天 对杯酒 吊角喉 月上寒城楼 哪年诗 吟觉旧 难栖何怨秋 朝闻道 便知休 开落任去留 不过是 士无右 危台路通幽 形色皆匆忙 执念始停留 燃灯不需火 神明自举头 难及的幸福 不必争拥有 谁记多年前 动人的邂逅 花未央 枢不朽 三更生不休 风雨前 读书后 慵懒且贪秋 七八个 星天外 窗楹为仙留 也许我 辗转间 沧海人如旧 （完）]]></content>
      <tags>
        <tag>填词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|成熟学长]]></title>
    <url>%2F2018%2F10%2F16%2F181016%E6%88%90%E7%86%9F%E5%AD%A6%E9%95%BF%2F</url>
    <content type="text"><![CDATA[成熟学长氦核 18.10.16新学期的新政策实施，原本雄踞三楼的会计朋友们搬去了本部，我们对面就住进了很多大一的新生，具体哪个学院倒是不怎么清楚了。因为我们宿舍位于大一和大三的分界处，就不免经常闯入一些不明真相的小友，给我们做诸如学生会或社团招新之类莫名其妙的安利，听得少会很新奇，听多了也不免厌倦和困扰。总觉得自己是沙河的钉子户，学校才会如此安排，把拆迁办的朋友安排在我们对面。 被当作新生在老生看来并没有什么特别的感觉，有时还会有小小的优越感。“我是不是看起来比实际要年轻呢”这样的想法就冒出脑海，新同学就让人讨厌不起来。可那终究是偏女性化的想法，在我住的猛男公寓，尤其是统数这种有猛男加成的学院，被当作新生会招致些许的反感，尤其是那些敲开门第一句话先问“你们是大一新生吗”这样的愣头青。猛男们的想法比较简单：“我们比大一要成熟多了，难道你看不出来吗”第一句比大一新生“成熟”并不是重点，反倒是旁观者第一眼能不能看出来最为关键。一旦被认出是高年级的孩子，猛男们就有一种不可名状的满足感，仿佛成熟或不成熟都已经无所谓了。但是这都是猛男的小心思，你亲口问他们，绝对没有人承认。 偶尔也能听见相关的差评，诸如三楼的朋友就不要坐电梯之类，我最初置若罔闻。后来目睹了三楼的些许“害群之马”从满电梯的人中挤出来，我不得不确信了这个对自己不利的风评。不禁思忖着，人们口中的“三楼的朋友”，指的莫非也包括了老生？自诩半个猛男的我通常不会坐电梯上三楼，虽然体育课训练之后可能会趁着没人娇气一下，但在人多的时候也绝对不想给高楼层的同学添堵。当然，既然有电梯，那便是一种可以享受的便利，这世上从没有一条规定言明去三楼不可以乘坐电梯，那是不是成了别的楼层同学谋求自己方便的欲望在作祟呢？电梯是那么方便的东西，一旦停的楼层多了，就会和原本的方便形成高下鲜明的对比，人们就会变得烦躁，而转念直接加罪于三楼。身为三楼的一员，都会为之感到不公。不过根据我的明察暗访（没错干统计是要学会这一手），所有坐电梯的人都不觉罪恶，不坐电梯之人也大都与被批驳的群体撇清关系，老生皆如此。大家丝毫没有“楼层荣辱”的概念，被这种类型的道德束缚简直是愚蠢。新生其中的一些也意识到了不妥，但是脱离了群体的“道德独行”毕竟是用来约束自己的，在默默接受加罪的同时，他们只有“独善其身”才能以此宽慰了。以一个局外人的角度，我很同情所有被误会和批评的同学，但当我被批评时，我又立刻倒打一耙，说着“可怜之人必有可恨之处”这样的话，开始归咎责任了。 我虽然不是有意要听新生们的谈话，但是有意思的部分我还是会留意。什么阿鲁巴的多种说法，南北方人的口音和误会等等，内容上和我们老生并没有什么本质上的差别，猛男们也是喜欢旧日无限风光的，怀旧也是讨论时间里必不可少的环节。可是除了内容相似以外，老生和新生的区别就判若云泥了。新来的孩子们大都被嘱托过，要好好地融入这个集体。而在旁观者看来，他们在集体里做的第一件事，就是团结地一起“叫”。这个叫不是叫外卖那种叫，就是字面意义的叫。在宿舍狂放地大吼大笑，其实是一件很考验个人气势的事情，毕竟要顶着那么多嫌弃的目光，本身的开心氛围将会瞬间烟消云散。不过，这仅限于一个人笑，如果一个宿舍都在爽朗中面对嫌弃，那内心一定会多一分坦然。而且似乎为了展现他们优秀的团队建设，家家舍舍经常不闭门户，回宿舍倒像是来到了鸡犬相闻的桃源盛世。 这份“叫”的功夫，并不全是天生的，还有培养的加成。自他们在学校里军训之后，那不明所以的起哄就换作了一齐高歌《强军战歌》以抒发大家的不可名状的复杂感情。这让我想起来我们当年军训的故事，在军事基地里，对猛男可能要狠一点，至少澡是不能多洗的，两周的军训时光里只有一次机会。这比每天拉练听起来要残酷很多了。洗澡这天我们期待了很久，大家在微冷的小风中抱着盆子，看着一批批人进澡堂别提多激动了。轮到我们队伍洗澡的那一刻，大家都跑着跳着一拥而入。而进去才傻了眼。首先是几个人合用一个淋浴头，那些年纯情的男孩子也大多不情愿和素未谋面（还没认全）的战友共享自己的淋浴，可是这人数无疑宣告了生米已成熟饭，大家只好硬着头皮洗。我却非常坦然，大概是自己的高度近视让自己远离了这场群体羞涩，只剩自己和雾蒙蒙的世界。除了脸红心跳，猛男们的体温也升高了不少，我们愈发觉得洗澡水有些冰冷，这样冰冷的感觉经过语言的传播，让整个澡堂都寒气逼人，一点没有洗澡的幸福感。不一会，竟然有猛男大声地唱起《强军战歌》，霎时间整个澡堂都应和着，回荡起“听把新征程号角吹响”来，一时间令人热血沸腾，失去视觉感受的我也过了一把嘴瘾。那是整场军训我印象最为深刻的一幕。而今又有幸在宿舍听见大一朋友们唱战歌，让我觉得怀念。但是总感觉我们这些隔了两年住在同一层学生很可能是一丘之貉，但瞬间这个想法被脑子里做“成熟学长”的念头抹消掉了。 （完）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|神触和我]]></title>
    <url>%2F2018%2F09%2F16%2F180916%E7%A5%9E%E8%A7%A6%E5%92%8C%E6%88%91%2F</url>
    <content type="text"><![CDATA[看到神触就觉得自己傻逼。很多人都有这样的想法吧。 暑假的时候，曾被shangri-la这首歌的旋律所吸引，为它填了一篇词。这篇词写了很久，很多词句能够脱口而出，但有些字段却只能慢慢推敲琢磨，挑选词句。灵感丰沛的前半篇，灵感枯竭的后半篇，泾渭分明。不过假期人浮躁，那样的作品放在那里，也就不了了之。 而今天想找一个midi做v3填词的时候，却发现了十二玄樱大佬填的词。古有崔颢题诗黄鹤楼，到了今天，我也无法回避地发现了前人的创作。而相较之下，十二大大的作品整体风格更加飘逸，字句之间的情感和思维也更有活力，全篇读下来全然不觉牵强或生硬。反观我的作品就显得更加拙劣幼稚，很多方面都显现出语言的匮乏。文字也太过天真，完全不像是以文学为爱好的成年人能写出的东西。 var ap = new APlayer({ element: document.getElementById("aplayer-mUmsSiVm"), narrow: false, autoplay: false, showlrc: false, music: { title: "【苍穹之法芙娜】苍穹尽碎（shangri-La中文版）", author: "十二玄樱 填词", url: "http://p5590g7cf.bkt.clouddn.com/%E3%80%90%E8%8B%8D%E7%A9%B9%E4%B9%8B%E6%B3%95%E8%8A%99%E5%A8%9C%E3%80%91%E8%8B%8D%E7%A9%B9%E5%B0%BD%E7%A2%8E%EF%BC%88shangri-La%E4%B8%AD%E6%96%87%E7%89%88%EF%BC%89.mp3", pic: "", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 回想做填词的一路，曾经那昙花一现的“尖锐”的可能，都已被我扼杀在通向成熟的道路上。 生活里，俗不可耐的自己，写出来的东西也这么幼稚朴素，愁绪千丝万缕，甚至到了无病呻吟的地步。这倒是没什么好惊讶的，只是和一年前的文风比都大相径庭，这种奇异成长倒还真的让我有些惊讶……开始满足于口嗨，开始逃避自己断断续续的灵感，开始放弃曾经对痛苦的执着，选择去歌唱别人的人生……我本来以为这是成长，但仔细想来这是一种逃避，我放弃了原本痛苦暴躁的我，选择了更加温柔但也更加小家子气的我。 人不会突然变傻，变傻都是连续的过程，其中变化可能很小，但是久而久之再聪明的人也会失去敏锐。我不再能感受到曾经追寻发光发热的自己所体会的痛苦，也不再会为哪件事而冲动。曾经敢打敢杀的我，丢掉了原本锋利的武器和盔甲，缩进了安逸的角落。再不会有荷戟独彷徨的氦核，再不会有寻斧开天辟地的氦核，再不会有轨迹无人能改变的氦核……这就是成长的代价。我成长了，但是顺从了环境对我的改造，为了自保而收起了对外的刀枪。 曾经男生宿舍里有个小哥，喜欢洗澡的时候来两句。爱好无可厚非，只是小哥的歌声实属独特，听罢不禁有种人间失格之感，他本人看起来丝毫没有相关的意识。我对其澡堂高歌之事有些不爽，只是并不熟识，加上洗澡本是短暂的相逢，也没有开口的必要。而他随着学院一起搬走了，我反倒对那走音的歌声印象深刻。我对他有些谜之羡慕。 一年写了三十篇歌词，从追求感情表达，到了追求点睛妙笔。写的词是越来越没水平，嘴里的话倒是越来越多了。眼里是只能保护自己的功利，行动也仅仅是为了让自己变得更坚强可靠，团队合作中从来不愿主动依靠队友。我自以为人是有界限的，不能完成所有的工作。但是为了自保，为了不受更多伤，我还是选择了很多对成长毫无意义的努力。曾经我认为，大丈夫穷则独善其身，穷而不困；达则兼善天下，达而不匿。而我有些自卑的心态告诉自己，我从来都无法成为一个豁达通达之人，因此这一年，我走上了独善其身的道路。 有人曾在我见过的一场网上论战中说，“教育是需要成本的，我不会费口舌来教育你。”凭什么我应该努力改变他人，凭什么我一定要拼上我自己的立场、声望以及时间精力等等去和一些顽固的人争论呢？我于他们又有什么关系呢？这一年来，我一度被自己的懒惰和害怕圈在了这样的问题圈里。要知道，一个正直的人，不需要知道应当做什么，只需要清楚所有不应做的事就行。其他的事就尽管放手去做。不做，半点正直都谈不上。 每次听见有人准确地评价我，或者完全错误地评价我，我都会笑出来，因为两种情况下，我都不知道该如何对答。有人把我曾经的痛苦全部用“自卑”二字来概括，我笑了。今天有人对我说，我的词大多有种怅然和温柔。我愣了愣，回想了一阵，又连忙去翻旧账。看着可爱而处处懦弱的作品，我不经意间又笑了。 我真的热爱这个世界，这个世界对我却没有理想中的那么友好。我还是在成为一个善良的人的路上努力着，也从不苛求世界为我做出改变，只希望我那微弱的声音能够稍微地填补世界。希望我的每一个读者都能得到神的祝福。 （完）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闻歌如面|谁说词人多痛苦（下）赏析]]></title>
    <url>%2F2018%2F08%2F14%2F%E8%B0%81%E8%AF%B4%E8%AF%8D%E4%BA%BA%E5%A4%9A%E7%97%9B%E8%8B%A6%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[左思右想，再三斟酌，我终于斗胆继续提笔续写这个系列了。之前的大把时间，我进行了浅层次的学习，对歌词文化有了一定的了解。当然，我的水平也还完全不够，毕竟填词这门学问真的有很高的上限，平时接触到的大都离佳作甚远。这也是我每有会意便“欣然忘食”的原因了。 上回重点说了歌词的两个方面，也是写歌词的两招。 盛开一支思路，万言不在纸端。 虽说不是什么重要的学问，也并非歌词届独有，不过用好“情感爆发”这一招，足以搞定大部分想写的感情。剩下的就是多多体会，仔细观察生活，常怀一颗细腻的心了。 本篇目录 （一）天赐应手，莫论优劣 （二）文字之外，词的使命 这次我们来说说歌词主题和赏析方面的事情吧。 谁说词人多痛苦 下篇（一）天赐应手，莫论优劣王国维说境界不分高低，只分大小。词在境界这方面也一样，境界之高低无关词的好坏。 歌词很重要的一部分是情歌。情歌的歌词写人与人之间很奇妙的感情，每个人的感受在感情的不同阶段都不尽相同。而手法更是多种多样。晦涩深沉者，奇诡玄幻者，混沌者清澈者，直言者隐喻者，色彩明丽者，渲染定格者……如此种种的手法，能够给听众立体的感受，将歌曲的灵魂传达给听众，就无所谓高低优劣之分。 更应当关注的，在于歌词之应手。每一次发问，设下一出戏或一场境，戏本如何行进，主人公会给出的应答，直接关系到整首歌词能否得到听众的认可，能否让歌词直射读者心底。 忘了谁对我说过：“只有听众默认了作者的论调在作品中的权威性和唯一性，才能进入作者的导向，以此为舟，划入作者的心灵之海。” 便是这个道理。而这并非用辞藻能做到的，需要灵感爆发和妙手偶得。而品一首词，其中的金句往往正是作者在某个处境下的妙手，短短十几字将所有人的心抓住。 现在听以往的歌，就有一种类似的感觉。我们现在流行的歌曲中，金句变少，口嗨增多。好不容易有了一两句妙笔，却发现早已入了古人歌中。 要想真正理解处境和应手，氦核还是得举个例子，体会体会词中主人公的处境和ta做出的行动或回应： 化身孤岛的鲸曲：我们都被忘了词：沃特艾文儿 段1我是只化身孤岛的蓝鲸有着最巨大的身影鱼虾在身侧穿行也有飞鸟在背上停 我路过太多太美的奇景如同伊甸般的仙境而大海太平太静多少故事无人倾听 我爱地中海的天晴爱西伯利亚的雪景爱万丈高空的鹰爱肚皮下的藻荇我在尽心尽力地多情直到那一天 副歌1你的衣衫破旧而歌声却温柔陪我漫无目的四处漂流我的背脊如荒丘而你却微笑摆首把它当成整个宇宙 你与太阳挥手也同海鸥问候陪我爱天爱地四处风流只是遗憾你终究无法躺在我胸口欣赏夜空最辽阔的不朽把星子放入眸 段2我是只化身孤岛的蓝鲸有着最巨大的身影鱼虾在身侧穿行也有飞鸟在背上停 我有着太冷太清的天性对天上的她动过情而云朵太远太轻辗转之后各安天命 我未入过繁华之境未听过喧嚣的声音未见过太多生灵未有过滚烫心情所以也未觉大洋正中有多么安静 副歌2你的衣衫破旧而歌声却温柔陪我漫无目的四处漂流我的背脊如荒丘而你却微笑摆首把它当成整个宇宙 你与太阳挥手也同海鸥问候陪我爱天爱地四处风流只是遗憾你终究无法躺在我胸口欣赏夜空最辽阔的不朽把星子放入眸 副歌3你的指尖轻柔抚摸过我所有风浪冲撞出的丑陋疮口你眼中有春与秋胜过我见过爱过的一切山川与河流 曾以为我肩头是那么的宽厚足够撑起海底那座琼楼而在你到来之后它显得如此清瘦我想给你能奔跑的岸头让你如同王后（完） 对比反衬，妙处不再详解，自当意会。 歌词中有一个很重要的派别，就是同人歌曲。同人歌曲不需要歌词中做任何铺垫，写人写事都没有凭空而来的风险。在创作的基础上再次发掘创造，将作品的深度和广度无限扩大。 同人歌曲的歌词也因此可以天马行空，其中更以描绘个人命运的居多。整篇歌词已经存在看不见的底子，所以看似胡乱呓语也存在其中的逻辑。听众们对歌词描述的原文本内容自然有着自己的认同和理解，所以不需歌词中再次铺设架构。 可是这也造成了一个局限，只有那些事先了解过原作品的人才能享受到歌词中的深意。（贝塔） 「β受体阻滞剂与星辰」《To The Moon》同人歌 -作词 : Kevinz 段1一转眼逃过多少年我还嵌在这空房间馥郁的感知逶迤着爬行在破旧时间线解构与架建都是显像管里断续的残片愿我驻定这梦境月球上燃第一抔火焰 或藉由契机仍能寻回游园观星那天却遗失时间票根与久远更远青稚侧脸他等待邻星来访此端彼端一千零一年愿你在通晓之前之后择一可酣然安眠 —-Music—- 我祈你终归抵达每座灯塔矗立的彼岸也能彼此簇拥光线兴高采烈目色交换遗落于真实那些片段都折进纸间裁线从此我们所睹见是初阳拔地辉煌诗篇 他在最后那刻填进逐寸褪淡的执念你忘记一切同指令中伙伴将星流俯瞰人们在烫金长桥上聊起从前拍出鼓点何其庆幸世界可反复重写一遍又一遍 副歌1然而最后一秒回望时你笑得多温暖就如夏日初慕恋 昏黄灯光旧影院以及某一段流光翔翩南风过境的夜晚目光所及的星天 目光不及的明天 我急忙寻回埋在土壤里交错时光伏线愈拼命试图看穿 愈万物沉默无关残存梦想植根浮离于我视界苍莽彼端多疲多痛多不堪 多彩多美多耐看 —-Music—- 段2他们肩并肩嬉游穿越闹市街巷和童年寂寞和不安次第揉进雨点轻易就打散沿途好景里分享玩具火车与美味三餐口袋揣满发光理想在同一片夜空入眠 记得嘉年华那天星辰分外夺目耀眼悬崖风吹过那名少年手中背包和衣衫这孤独风景本美如宇宙群星赴向盛宴我却为何险些放逐眼泪坠出脸颊边缘 副歌2我数清多少灯盏毕剥于银海中缱绻看尽光芒的棱边 听懂星群的语言却忘了在某个时空中我们曾一同悠闲茂密年轮中谈天 掌纹共目光绘线 我将生途走完带着观者不掩饰的艳羡拢一捧月华饱蘸 涂抹风铃与屋檐「感谢命运感谢神祇感谢众生允我此恋」樱花飓风里相看 我称之幸运圆满 过片从何时记忆水路开始断裂开始封锁开始改变凛冽日光缝隙生长出成片青色橄榄是否有什么掷地应声粉碎在那夜海渊不然为何每当敲响琴键 乐谱标题都擅自消湮Ah… 副歌3你还记得吗 何其短暂的久远人生第一句诺言 穿越阻滞的区段如同冰封深海底涌起滚烫新鲜的烈焰如夤夜星云飞散 如雨后蛩声唱遍 我回头看见来路层层叠叠彼此纠缠须臾婚典中花瓣 刹那就泥泞万千我跌撞辨不清你缘何笑得寂寥无端那秒钟亦真亦幻 相伴也同时擦肩 （你以为是墨香大大的系列？选一个你们都没看过的hhh，图样图森破~~） 想知道这首歌的他和她都发生什么了？不如玩一下《to the moon》。听说这游戏要做成电影了，当年氦核玩这游戏都能哭成泪人，绝对良心（拖走） 对于知道那段故事的人来说，这个词可以说太棒了。 （二）文字之外，词的使命但如果不在意意境和手法的高低优劣，那歌词还有什么可品道的呢？答案是，能够引起美感之上——共振——的要素们。 有人把歌词比作有音乐的诗，我并不认为词人和诗人完全一致。这到不是因为内涵的区别，而是一种在控制文字节奏上完全不同。诗歌的节奏氦核不太懂，但是歌词的节奏是判断一篇词好坏的肯綮。 如果抛开歌词存在底子的可能性，凭空取景的歌词如何评价呢？有，有三个评价的方向，推进力，契合度，稳定性。 词中故事或感情推进的快慢，与歌曲起伏节奏契合，加上不成废子，避免突兀，松紧有度，紧跟韵律，这些条件结合在一起所形成的稳定就能hold住任何一首词。 推进和契合粗看是文字功夫，其实考量着填词人对歌曲曲调本身的理解。何处婉转，何处直白，何处高昂，何处消弭，主歌副歌长短对应着感情的铺垫和爆发，过片感情变化或是妙语升华等等…… 文字和歌曲一样出入自如，这样的词才能在契合的基础上更加和谐，使人动容。 听众没有理解基础时，一定要谨慎取词，相互并无必然联系的意象，纯为渲染，就不如以物寓情来得自然。还有一种讲究协同的运用，但跟之前说的契合度不同的是，之前重点在节奏上与曲的契合，强调的是形式，即字数，音节。而也可从语感上的轻重疾徐入手，若掌握得好，必定加分。 这里说的只是词的评价方法，至于怎么写，还得拜托老天慈悲赐予的灵感和平时有心或无意的积累啊。 看一篇大佬的词和另一个大佬的词评吧。多读总没错。 「凡心不改」词：临帖知字繁 眷恋晚霞铺展天际是你挥毫绚烂一笔这世界斑驳陆离年华流转心未曾甘熄 我愿敞开赤裸胸襟呼唤亘古朴素生灵与天地交相辉映腾越这一番云岚虹霓 也曾迷醉春山的诗意谈笑鸿儒云集心尖上潋滟的梦是你迷藏太多孤寂我甘愿隐居这山林吹来拂去漫天流萤 山麓千里若得往昔不负春风写意薄云作衣喧嚣归于宁静咫尺天地如何相依 洗耳聆听雨过青山 笛声渐远渐近千碑林立相照虚实会意清风鉴心明澈如溪 不再迟疑 追寻你的踪迹在所不惜 翻越山河背脊我曾随倦鸟归栖 也投身入海底 在遥远的清晨醒来我静坐观浮云散开把天地纳进胸怀徜徉我内心逍遥自在 当迎风面容不再神采浪漫褪去情怀也如山头那一轮月白此生与你同在我知这世间如沧海但会牢记凡心不改 山麓千里若得往昔不负春风写意薄云作衣喧嚣归于宁静咫尺天地如何相依 洗耳聆听雨过青山 笛声渐远渐近千碑林立相照虚实会意清风鉴心明澈如溪 不再迟疑 追寻你的踪迹在所不惜 翻越山河背脊我曾随倦鸟归栖 也投身入海底 日月轮番更替 空留几许传奇只为那一句 无悔亲临 去放歌天际电光石火一息 回音渐次结冰最怕是你不应 只因钟情怯懦抬眼目光投射于你浮生虚妄皆空是悲或喜不卑不倚 不离不欺我亦不曾 揣测过你心境遥不可及 也渴望某日能与你灵魂二合为一一身赤血燃尽 心还滚烫无比 词评：花不去落日红霞，指间余晖，眷恋既生，年华难摧。浮光掠影，执迷一醉。凡心久郁，青欢几味。漫天流萤，孤寂成堆。迷藏渐结，春盟安兑。欲寻有迹，遥溯无对，山麓薄云，若离若慰。奈何雨霁笛起，未悟未立。心魔如此，空写传奇。谓之是钟情，实是独角戏。高烧50度，自然烫无比前三节不错，辞藻逸秀，酣畅写意。但心绪略大，显得凿痕过深。腾越、吹来二句骨肉皆有，甚是生动，好句。立意上看，有以前作品的情境，少了新意。整体读来，开篇吸睛，三节后渐显凑数，笔力亦有走弱之貌。此词有点像即兴写了个开头，有感觉，想凑完，结果自己把自己给绕进去。不过,若有曲子加上去，说不定也挺好听 （帮你理解：这词写得挺嗨，口感不错，不足处，肌理过充盈，显得纷繁了些） 词评也是大佬才能做的呢。小生氦核还要多读多体会，这篇比起上一回缩水不少，因为氦核自己读写得少了，又成了小菜鸡。 其实用这些小技巧慢慢分析理解，我们一定能发现每一篇词自己的灵魂，抓住那个线索或灵魂，一切意象都能迎刃而“解”。 在词坛一起进步吧，哈哈。 （完） 另附上篇链接：谁说词人多痛苦（上）]]></content>
      <tags>
        <tag>填词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|学棋]]></title>
    <url>%2F2018%2F06%2F21%2F%E5%AD%A6%E6%A3%8B%2F</url>
    <content type="text"><![CDATA[氦核20180621 我曾经是极度喜爱围棋的。虽然现在也是极度喜欢，可是两种喜欢有着本质的差别。和许多小朋友一样，我第一次接触围棋是在我很闲的幼儿园时期。当年流行学点什么，我先尝试了画画，画了一个月的公鸡和云彩，一笔一划认认真真。不过后来我妈形容那些画如同煮好的面放久了，搅成一团越吃越多。最后画的东西从纸上溢出来，洒得欣赏者身上到处都是，甚至恬不知耻地散发出浓浓的油香。英明的老妈立刻斩断了我和画画的牵连，再也没去过那个画室。虽然后来我在小学搞起了漫画社，漫画依然看起来像面，但是受到了几个好友的喜爱，当然这是后话。 没事干的小孩子每天当然不是只能遐想。在那个接触电脑游戏还为时过早的年代，同学们大多学起了珠心算。我个人是喜欢计算的，尤其喜欢有规律的计算，找规律和计算永远是我在寒暑假作业的小本子中首先写的部分。可是看见每个珠心算的小朋友每天放学都要去上课，我从那时便展现了我天生的懒惰，毅然决然地无视了珠心算的盛情。小时候我也没有展现出学习的天赋，在幼儿园也看似是个乖乖小孩，实则是个极端害怕老师同时又极端厌学的毒瘤。 当我还在想象自己长大变高的普通的一天，有位围棋老师带从距离幼儿园三百米外遥远的小学来到这里，上了一节围棋启蒙课。可能我从小就是个极端的人，在平时的游戏中就展现了过人的“将快乐建立在他人的痛苦之上”的能力。那时见得不多的我，了解了围棋可以将对手的棋子从棋盘上驱逐的奥秘后，在放学的报名会上和老师对上了眼。也许这本身就是一条兴趣的路，中间辛苦了点，但是终究是从兴趣开始的路。我像是第一次吃蜂蜜，对未知无限好奇，并且享受其中。同时报名的还有三个同学，皆是我的好兄弟，其中两位自幼儿园起就失散了，只剩泽林老哥和我现在偶尔假期里约个球，一起吃个雪糕喝几瓶碳酸饮料。 学棋的路就这么开始了。我的进步飞快，不久就去小学，和小学里的大孩子一起学习围棋，上了小学就更方便些。其他同学放学去上英语班、数学班的时候，我却在围棋盘上享受游戏的快乐。不同于普通的游戏，围棋从一开始便是争夺胜负的游戏。小孩子和大人不同之处便体现出来了，没有杂七杂八的消遣放松娱乐交友锻炼大脑之类的种种理由，小孩子玩游戏就是为了胜利，也因此进步神速。当时村里有个老师也在教围棋，我就偶尔去他家里。还记得当时是暑假，老师指导我在电脑上下棋，有一盘棋下出了一个征子，从左下角一直杀到右上角，最终对方全军覆没，我这才发现手里的绿豆雪糕化了汁，流的满手都是。 但是小孩子终究是小孩子，我有着人们印象里小朋友拥有的全部特点。我除了每周两次的下棋时间，很少有集中精力做事的时候。我比周围人想象地更加活泼，邻里间也是出了名的“人来疯”。但是欢脱的一面只占据了生活的少部分，我受了气哭的比谁都多。我妈说我小时候表情总是很凝重，大概在心里时常揣着几件国家大事，忧国忧民。我倒是没有什么印象，可能围棋发掘出了我的真性情，该让就让，该杀就杀。对于提不起劲的事可能转眼就忘，相反感兴趣的事化成灰了都会记得。 学棋是有测试棋力的比赛的，这也是所有棋手奋斗的目标。这其中不乏一些需要到省城去打的比赛。我先先后后参加了不少。在一年级一次“三鹿”冠名的育苗杯比赛上，我竟然定上了二级，奖品不是一箱牛奶，而是一张合影和一张奖状。这是各科成绩平平、在期末期中考试连双百都没有得过的我，不喜在人前表现也因此从来未被老师注意过的我，得到的自出生以来最大的荣誉——围棋业余二级证书。虽然连个段位都不算，也并没有称霸我的小学，可正是从那时起，我开始意识到围棋给我带来的快乐之外的东西。围棋让我的心中多了一份沉甸甸的使命感和荣誉感。我也从那时起爱上了喝牛奶，于是个头飞涨。 定上围棋二级可以称得上是一个分界点，我的老师开始推荐我去市里的围棋班上课。第一次去那里下棋，遇见一个不大的小哥哥，杀得我片甲不留，看我抓耳挠腮时和我互换了黑白子，结果又一次杀得我片甲不留。我当时就对他另眼相看，中午还和他一起去少年宫一起吃面。我们棋风有些相像，他擅长推陈出新，大杀大合却都用的巧劲。我们也都喜欢吃面，后来不知为何在没见过他下棋了。 那些年我除了学校的文化课以外，剩下的时间大都下了棋。我的老师是一个善良而含蓄的人，有一双炯炯有神的眼睛，在那目光之下什么都藏不住。老师也算围棋江湖中人，我跟着他见到了很多下围棋的人。我也慢慢走出了自己村里那一亩三分地，渐渐发现了周围很多的下棋的人。在一个有点冷的冬天，我寄住在了老师家里。每天跟着老师去棋校，回来就摆摆棋谱，检讨棋局或者上网下棋。那时我跟着老师去他不同的授课点，和不同的高手对战。我们就像行走江湖，潇潇洒洒。那时经常和老师在中午走进一家粥屋，点两碗黑米粥叫几碟小菜，饭后在享受一个超大苹果，晚上回到家门口的小饭馆里吃一顿鱼香肉丝。在老师家没住多久，我就回家了。在同一年，全市的比赛里，我七战全胜升上了初段。 小孩围棋界也可以称得上一个小江湖。我小学二年级的时候，从村里一起同行的一个初中大姐是我最好的朋友，老妈看我们同路，每周末就把我托付给她。我们一起学棋，但是当年我从来没有赢过大姐。虽然赢不了，但我已经悄悄把大姐作为了我的目标。下棋的人似乎就是如此矛盾，又是好友，又是对手。平时下完棋走上棋校的天台，小朋友们就在那里休闲活动，欺负大姐就成了当年的热门项目。大姐也很善良，陪我们疯。中午大姐也会领着我去吃饭，有时是令人满头大汗的麻辣烫，有时是散发浓香的烫手煎饼果子，路边摊总是好吃的，大姐有时也会请我吃烤面筋，不过总是害怕我吃坏了肚子。我意外地进步飞快，一段时间后大姐竟然也时常输给我，下完棋以后，正在开心的我注意到她悄悄地哭了，突然意识到她的难受。学业和生活繁忙，她没有我那么有闲空，在练上二段后也没有很明显的进步，于是很快她就放弃学棋了。后来我们没再联系，去年从围棋老师那听说她已经结婚，总觉得有些恍惚。 三段比赛那回，我借住在省城亲戚家里。不过意外的是，我感冒了，发起了高烧。第二天要去比赛，大家连夜带我去最近的医院。我脑子里难以装进任何多余的思考，我突然觉得下棋是如此痛苦的一件事。可是在高烧之下，我的运气变得非常好，一盘接一盘，连胜五盘打上了三段。这就是我下围棋的巅峰时刻，我也再也没有像这样置死地而后生过。巧的是，四连胜时我的对手，在大学的一次比赛中又与我相见，两人一见如故。 四年级暑假我又去打新的比赛，输掉了升段比赛的那天，我回到酒店，中午时分，正逢电视机里播放奥运会，一百一十米栏，刘翔受伤退赛。我默默地看完了全程的直播，默默地在厕所抹眼泪。后来我忙着其他的事，也没有什么精力再去下棋，更没有什么成绩了。 值得一提的是，学棋这么多年，我从初学变成了四段，老师也从四段变成了五段，但我因为很弱，还是会被老师授三四子。后来我因为学业和心态，慢慢背负了太多东西，反而下不出好棋，虽然还是很厉害的小棋手，但在瓶颈里待得久了，就有点懈怠了，慢慢也退出了这个江湖。上了中学之后，再没有怎么下过棋。倒是当年没什么起色的学习成了我的主要任务，可以说是很意外了。而我依然保持的一些习惯只是一些关于其他无聊琐事的边边角角。毛毛躁躁的我，完全看不出来是个下围棋的人了。]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|拉屎的人和放屁的人]]></title>
    <url>%2F2018%2F05%2F27%2F%E6%8B%89%E5%B1%8E%E7%9A%84%E4%BA%BA%E5%92%8C%E6%94%BE%E5%B1%81%E7%9A%84%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[中午吃完饭大学生最喜欢做的一件事竟然是拉屎。 中午吃完饭大学生最喜欢做的一件事竟然是拉屎。早上忙，下午接着忙，中间这大好时间就一定要拉屎，清肠减负。也许是约定好的，每天中午大学生宿舍的厕所都是爆满的，甚至还有在坑外等待的人。而今天我本身没有想法，可是室友冲进来说厕所竟然为了拉屎排起了队，说的我也觉得肚子不忏，想去拉屎。我蹲在那里，其实也没什么货，两个屁就完事了。事后想想真是做了件脱了裤子放屁的事，但自己的身体发出的信号，自己再怀疑也没什么用。 在大学，自己的事不光是拉屎，大学生的学习生活中，学习那一部分至少得占一半。我是很喜欢上课的，偶尔也喜欢好的老师，不过更喜欢习得真理。但是这种浅薄的热爱完全招架不住前一天晚上的夜生活，大学生的夜生活还是比较丰富的。如果前一天晚上嗨的有点晚，白天上课就会犯困。人偶尔靠着精神气还是能撑住的，可是大学生往往需要在课堂上打出有声的哈欠，并且哈欠的声音长短高低都影响这个哈欠的舒服度。毕竟大学生不能因为简单的困意而埋头睡觉放弃听课，就靠简单易行的哈欠来缓解困意。老师们也睁一只眼闭一只眼，以教学质量为主、以师长尊严为次。但是哈欠是会传染的，一个人打了，周围人就会打，进而全班都染了困意。不困的人都会觉得自己精神不振。至于传染的原因，我觉得可能是人们精神间有某种连接，在打哈欠的时候那种快乐感染了周围的人。 除了周围的困意，我总是能感觉到，周围的人们还总是十分热情。他们对我伸出一张张嘴，和我谈天说地无所不聊，而我是喜欢聊天的。不过和人们有一点点差别，人们聊天倒是不怎么在意有没有理解者，一张嘴一吐就是半个盛唐，听者能不能装下就不关己事，反正重要的事都是有文字呈现的。久而久之，我也习惯了说一些无所谓的话，让人摸不着头脑，反正重要的又不是观点，重要的是敢把话说出来。一个连观点都不愿意表达的人，人们会怎么看待？所以说一些脏话，不明所以的话，不切实际的话，甚至敢说谎话并不是什么不光彩的事，这些人往往能成为有本事的令人们尊崇的人。 可是我也偶尔有了偷懒不想说话的时候。在我不想说话的时候，也总有人找我说话。我倒是有那么些喜欢说话的朋友，不过在我需要时来同我说话的人虽然有一颗好的心，但是并不一定能帮上忙，显然很多时候他们的重点完全没在我身上。切实地问了，然而他的开心和自信并没有如他所愿传染到我身上，我依然垂头丧气，甚至更加不愿意开口。他觉得我有些内向、有些敏感，久而久之内向、敏感竟变成了我的标签。我偶尔也喜欢自己的标签，觉得内心丰富的人是真正强大的人，不需要表达就能展现出来，可是也没那么喜欢，因为这些闪耀是不实用的，对于争夺和大家认可的努力来说毫无价值。时间一长，这些标签让我对外界有了恐惧，甚至让我变得有些自卑了。 我喜欢帮助大家完成任务，也喜欢学生工作。我很享受团队合作的感觉，在队伍中，最有趣的环节就是群体讨论。整场讨论的走向往往在于控制节奏的人嘴中的一两句话，不过不论这两句话有没有价值，他们都不会停下刚刚轰轰烈烈的思维。仿佛自己的革命般的肤浅，经过五个人认同以后就能变得高明一样。在讨论中只需要学会怎么用别人的词藻获得听众们的认同，毕竟只是一个观点的表达过程，每个人的发言都是事先准备好的，甚至思考思路也是事先准备好的。明明简单的能够说清的东西，复杂一点看起来就会更漂亮更让人满足，多么神奇的化学反应。而我不喜欢说太多话，我喜欢听大家说话，然后冒出一两个抽象的过度凝练的点子，甚至不知道是否切题。因此后来也没人愿意和我在一起工作了。他们听不明白，觉得不是自己愚蠢就是我太愚蠢。答案是显然的。 我也喜欢读书听报告。这些活动算是接受有充足准备的知识盛宴，会意不会意反倒是次要的，重点是得到所谓知识。知识是个很抽象的词，不过小时候听说它能改变命运，就觉得这个词很强大。于是但凡是有知识二字的地方，必有我的身影。我喜欢自己的理解，哪怕是一个错误的理解。我喜欢更感性的更美的认识，但是往往那种印象只有最初那最模糊的一次。同学们掌握的东西和我掌握的仿佛不是一回事，他们强大着，我自己觉得自己像阿Q般强大着。其实内心还是有些自卑吧。 但我好歹也是有点货的，虽然我总想在不经意间展现我的奇货。基本上我能坚持下来的努力都是有一定水平的。以前有人说我音乐方面愚钝，可是我不信邪，学了些乐器，还爱上了听歌。虽说音乐也确实没成什么气候，但是意外探索出写歌词的奇怪天赋。歌词圈公认的是：有多高的天赋就能有多妙的理解和应手。不光是写歌词需要天赋，理解这个世界也需要天赋。歌词是用来引起感情共鸣的文字，因此理解这个世界就显得格外重要。当我理解了这个世界以后，我才发现了这个世界构造挺简单的。大概一边是真的要拉屎的人，另一边是没屎也会随拉屎的人一起脱裤子放屁的人。不管是越悟越傻还是越悟越真，当我恍然大悟时，我又觉得肚子不舒服了。 氦核 20180524 后记 讽刺了很多人，也批判了自己。此中有真意，欲辨已忘言。 希望我的读者都能得到神的祝福。]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[标书+实践任务！]]></title>
    <url>%2F2018%2F05%2F22%2F%E6%A0%87%E4%B9%A6%E4%BB%BB%E5%8A%A1%E5%88%86%E5%B7%A5%2F</url>
    <content type="text"><![CDATA[最终话！袁家村产业经营模式 普通村庄如何自己翻身？靠什么？ 三产借助陕西特有民俗文化的植入路径 首先要有 文化，传下来的真东西，陕西不缺这个。其他地方没有，因此也只有这个才能振兴陕西。这个算是 灵魂 前期深挖文化的可能性 小手工，小作坊，硬实力，硬口碑 逐步形成家庭为单位的集体经济主体，效率很高 城市里完善，乡村中得到重视的比较少，仿古老街的失败 主要是前期需要大资本投入，后期客流量难以完成一个循环积累的过程 中期中期，形成辐射，在游客的观赏和体验中，哪里不足补哪里 周边村民在特色的辐射下，进行补充 产业植入的过程除了很重要的机会，还要照顾产业完整性 可以将地区的一二三产业结合 后期管理手段更新 规范融资 形成规模，cbd效应 简述任务两个人：导师邀请函修改，企业与机构申请函 项目简介提炼（jz 相关问题问卷*？（多方调研）—问卷组 —团队文化组—宣传组宣传平台第一次推送和新闻 标书任务分配：一个人背景（yf一个人意义（yt一个人实践特色（jz一个人经费预算，安全预案，（可行性分析（半成品）） （yx一个人实践准备（文献+政策 （kj一个人排版（zj一个人校对（yh 每个人都要做：寻找回函问老师要签名（当然也需要问问题）个人介绍100~150家长同意函（院里自己写）]]></content>
      <tags>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开题阶段的资料收集]]></title>
    <url>%2F2018%2F05%2F17%2F%E5%BC%80%E9%A2%98%E8%B5%84%E6%96%99%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[一、中央文件我国发展不平衡不充分问题在乡村最为突出，主要表现在：农产品 阶段性 供过于求、供给不足并存，农业 供给质量 亟待提高；农民适应生产力发展和市场 竞争的能力不足，新型职业农民队伍建设亟需加强； 农村基础设施和民生领域欠账较多，农村环境和生态问题比较突出，乡村发展整体水平亟待提升；国家支农体系相对薄弱，农村金融改革任务繁重，城乡之间要素合理流动机制亟待健全；农村基层党建存在薄弱环节，乡村治理体系和治理能力亟待强化。 二、陕西文件扎实推进特色现代农业建设，按照“产业兴旺、生态宜居、乡风文明、治理有效、生活富裕”总要求，以实现高质量发展为主线，以增加农民收入为核心， 以壮大村级集体经济为突破口，以推进农村环境综合整治为着力点， 以深化农村综合改革为根本动力，把打好精准脱贫攻坚战作为底线任务，建立健全城乡融合发展体制机制和政策体系，统筹推进农村经济建设、政治建设、文化建设、社会建设、生态文明建设和党的建设，推动农业提质增效、农村文明进步、农民增收致富，为谱写新时代陕西追赶超越新篇章奠定坚实基础。 1.推进特色现代农业建设苹果，猕猴桃，就是一大堆特色农业w 2.提高农业综合生产能力。全面落实永久基本农田特殊保护制度加强农田水利建设加强科研机构、设备制造企业联合攻关，加快研发适合特色种养业、丘陵山区的农林机械，继续实施农机深松整地项目。加快推进现代农作物、畜禽、水产、林木等方面商业化育种，打造杨凌“种业硅谷” 3.提升农业发展质量。推行农业标准化生产，健全优势农产品标准体系。建设食品放心工程，强化监管责任、健全监管体系，开展食品安全专项整治，实行从田间到餐桌全链条监管、全过程追溯 4.促进农村一二三产业融合发展。统筹规划农产品流通设施布局，大力推行农超对接、农社对接、农校对接等直采直销方式。推广延安市精准对接消费需求、实施苹果产业后整理的做法，提高产业综合效益。加快实施电子商务进农村综合示范县建设，鼓励大型电商企业在我省建设打造涉农电子商务平台。实施休闲农业和乡村旅游精品工程，打造一批田园观光类、民俗风情类、农业体验类、民宿度假类等特色鲜明的旅游名村和主题园区。 5.把小农生产引入现代农业发展轨道。研究制定我省扶持小农生产的政策意见。围绕小农户融入现代农业发展，把对新型经营主体的政策扶持力度与带动小农户的数量挂钩，鼓励将政府补贴量化到小农户、折股 到合作社，发展农民专业合作和股份合作注：新型经营主体的资料见链接（百度百科 详细明了介绍：2017年06月05日 培育新型农业经营主体的意见开展联耕联种、代耕代种、统防统治等直接面向小农户的农业生产托管，为小农经营提供产前、产中、产后的全产业链社会化服务。完善土地入股、订单带动等利益联结机制，引导龙头企业、农民合作社等新型经营主体带动小农户共同发展。搭建多种形式的培训平台，以技术服务“110平台”为载体，把“土专家”“田秀才”纳入专家库，强化对小农户实用技术和经营管理培训。 6.强化农业科技支撑。太专业，似乎不好入手 7.推动农业对外交流与合作。很虚了 三、有趣的入手点：1.新型农业生产经营主体新型农业经营是什么？所谓新型农业生产经营主体，当然强调的是“新”了，就是在农村新出现的生产模式，主要是指在完善家庭联产承包经营制度的基础上，有文化、懂技术、会经营的职业农民和大规模经营、较高的集约化程度和市场竞争力的农业经营组织。 其中包括：专业大户：生产的农产品较为单一家庭农场：商品化水平较高，生产技术和装备较为先进，规模化和专业化程度较高，生产效率极高农民合作社：特点是分工明确，从生产、加工到销售都有专门的团队在做，其生产效率也因此得到提高农业产业化龙头企业：涵盖到整个产业链条，从农产品的种植与加工、仓储、物流运输、销售甚至科研都组织化程度和专业化都比较高 新主体的发展特点《新型农业经营主体发展指数调查(六期)报告》 引进新设备、采纳新技术、引进新品种的积极性较高，尤其在研发投入层面具备优势。主要通过购买（最多）、租赁、政府补助、自己建造等方式获得设备。对经营主体的现金流量提出了更高的要求。 龙头企业是研发主力：新型农业经营主体的创新意识普遍较强，核心竞争力凸显，农业产业化龙头企业是涉农新技术研发的一支重要力量 新品引进重“优质”“抗病（虫）”：新型农业经营主体引进的品种更看重“优质”和“抗病（虫）”特征，对于是否能够增产并不过分看重 信息技术：总体而言，新型农业经营主体促进了信息技术的推广应用。应用比例由高到低的信息技术依次为APP等移动互联终端、农业物联网技术、办公自动化系统、基于大数据的市场行情分析和云计算、云管理平台，分别占有效样本的15%、14%、13%、11%和5%。 西部地区品牌知名度更高：品牌建设取得一定成果，知名度范围较广的品牌出现，展现出新型经营主体巨大潜力 计划较积极融资较保守:大部分新型农业经营主体生产营销方面的发展计划较为积极，但融资计划却较为保守 对政策支持有更高期待:新型农业经营主体所获得的政策支持力度尤其是金融支持力度还远远不够 新型主体与传统承包农户长期并存的态势：农业部将适度的土地规模界定为当地户均承包土地面积的10倍至15倍，这符合现阶段我国农情。我国不可能在短时间内实现所有农户的土地规模扩张，多数农户仍是小规模经营。在推进适度经营规模的同时，必须考虑传统农户经营可持续性的问题，这涉及利益联结机制问题。新型主体发展优不优，不是规模越大就越好，而是看联农带农的作用。 问题：不同经营主体反映最多的就是融资难、用地难等问题。多年来，我国农村基础设施比较落后，农村道路、水利设施、土地平整等欠账多，不少农业投资者需要为此付出不菲的“补课费”。尽管在融资方面想了不少办法，但不少新型主体尚未树立起自己的信誉，融资能力差。同时，在农村搞产业离不开土地，用地难题也是一大瓶颈。一些农民合作社反映，设施农业用地按农用地管理，但备案难的问题依然存在。 困局和支持陕西省今年将启动新型农业经营主体培育行动计划 陕西金融业着力支持新型农业经营主体 在支持新型农业经营主体发展方面:人行西安分行不断加大窗口指导力度，精准支持新型农业经营主体。 该行先后下发了关于做好新型农业经营主体金融支持的指导意见和信贷工作指引，要求落实新型农业经营主体主办银行制度，按照“储备一批、培育一批、支持一批、做强一批”的原则，鼓励对新型农业经营主体提供阶梯式、层次化金融服务。要求各分支机构综合运用现有货币政策工具，对信贷政策导向效果评估和一定比例存款用于当地贷款考核达标的、贷款投向主要用于“三农”等符合条件的法人机构，合理增加其扶贫再贷款额度，优先安排部分扶贫再贷款额度用于支持法人机构开展新型农业经营主体金融服务。该行省内分支机构联合农业部门对发展势头好、辐射作用强的新型农业经营主体进行摸底，实行“名单式”管理，要求地方法人机构优先向名单上的主体投放扶贫再贷款。部分分支机构还建立了新型农业经营主体监测评价报备制度，制定金融支持新型农业经营主体调查表，要求各主办银行每季末报备金融支持新型农业经营主体数据，及时对新型农业经营主体金融需求和急需解决的其他问题进行梳理，并对机构支持情况进行综合考评。 为满足新型农业经营主体多样化的金融需求，陕西省内各涉农金融机构在不断加大信贷投入的同时，还结合专业大户、家庭农场、农民合作社等新型农业经营主体特点，相继开发出“果商贷”“红枣贷”“阳光贷”“兴农易贷”等适合其金融服务需求的专属产品40余项，并探索运用产业链贷款，推行“金融+产业联盟+合作社+农户”“金融+龙头企业+基地+农户”“龙头企业+合作社+农户”等贷款模式，支持新型农业经营主体做强规模、提速发展，带动普通农户脱贫致富。陕西省内各涉农金融机构还紧抓“两权”抵押贷款试点契机，通过实行“土地承包经营权抵押+”模式，探索推出“农地+龙头企业+农户”“农地+双基联动+农户”“农地+金融精准扶贫”“农地+信用”等信贷融资新模式，2017年，向新型农业经营主体发放农村承包土地经营权抵押贷款余额较上年同期增长58.6%，较好地满足了新型农业经营主体的信贷需求。 针对新型农业经营主体融资需求及生产特点，陕西省内各涉农金融机构进一步调整贷款执行利率，适当延长贷款期限. 有的金融机构对国家级农业产业化龙头企业贷款执行基准利率，对专业大户、家庭农场贷款执行基准利率上浮不高于30%的信贷政策；还有金融机构对新型农业经营主体单笔100万元以上贷款执行定价利率，并根据其对机构业务贡献度给予一定的利率优惠；多数涉农金融机构对新型农业经营主体日常生产经营和农业机械购买需求，提供1年期以内短期流动资金贷款和1至3年期中长期流动资金贷款支持；对于受让土地承包经营权、农业社会化服务体系建设等，提供3年期以上贷款支持；对于从事林木、果业、茶叶及林下经济等生长周期较长作物种植的，贷款期限最长可为10年。 在加大信贷投放和支持力度的同时，陕西省内各金融机构还根据新型农业经营主体特点，创新担保方式，构建风险缓释体系。宝鸡市以“财政+金融+担保”的合作方式，由市财政和财投控股筹资建立2亿元农业产业扶贫信贷担保基金，通过市中小企业融资担保公司为参与扶贫的新型农业经营主体贷款提供融资担保，费用由财政补助、利息由财政贴息；陕西部分农村信用社积极与农民合作社对接，建立“信用社+合作社”担保合作机制，合作社将吸纳社员入股（1万元/户）建立的担保基金存入农信社，农信社再以10倍的担保比例向社员发放贷款，发挥基金杠杆效应，支持新型农业经营主体。 “下一步，我们将继续积极引导涉农金融机构开展权属清晰、风险可控的林权抵押、大型农机具抵押、大额订单质押、土地流转收益保证贷款、应收账款质押贷款等业务，鼓励有条件的新型农业经营主体之间开展互助担保业务，对资信状况良好且符合相应条件的新型农业经营主体以信用方式发放贷款，切实满足新型农业经营主体的多层次融资需求，助力乡村振兴战略取得实质进展。”人行西安分行相关人士告诉记者。 涉农金融机构刚刚上面有一点矛盾： 明明有着贷款难，用地难的问题，但是大多在融资计划上显得保守 发展计划方面，新型农业经营主体生产营销方面的发展计划较为积极，但融资计划却较为保守。计划扩展销售渠道和增加市场推广的新型农业经营主体分别有59.21%和58.48%，计划引进先进设备扩大经营规模的新型农业经营主体有58.73%， 计划增加融资规模的新型农业经营主体仅为39.53%，明确表示不增加融资规模的新型农业经营主体也高达42.45%，这在一定程度上反映了新型农业经营主体融资信心疲软。 收集资料：风险：陕西银监局召开农村中小金融机构监管工作会 罚单最多的陕西w：2018年一季度银保监会对农村金融机构的监管分析 我们注意到陕西省农村金融机构一季度共接到62张罚单，数量远超其他地区，占一季度全国农村金融机构总罚单比例的27.8%，其中处罚相关责任人49名。大量罚单的背景是陕西、河南银监局在1月份依法查处了辖内银行业金融机构质押贷款案件，对两地涉及该案的19家银行业金融机构共计罚款5250万元，并处罚104名责任人。陕西银监局对涉及该案的18家银行业金融机构罚款合计5000万元，其中对陕西省11家县级农村金融机构罚款3600万元，涉及多名责任人。这一事实也反映了银监部门显著加大对违法责任人监管追责力度的趋势。 发生了案件：陕豫银行业金融机构质押贷款案件 作为银行占比最大的传统业务，农信机构信贷业务违规 所受处罚的数量最高，总计131单；违反审慎经营规则 是一类表述比较笼统的案由，很多违法违规行为都可以归入这一案由中，可能涉及信贷业务、票据业务、同业业务、内控制度，或无法区分到任何一类违法违规行为中。该案由紧随“信贷业务违规”之后，总计100单；内控管理及操作违规达到33单，处于罚单数量第三位，此类违规主要涉及到员工的个人操作行为，此次数量较大，又一次反映了监管机构加对违法责任人（个人）的监管追责力度加大；违规投资属于广义的同业业务范畴，从去年以来一直是监管部门的监管重点，此次达到25单，处于罚单数量第四位。违规授信、同业业务违规、违规处置/掩盖不良资产、贷款五级分类违规、票据业务违规这几种常见案由也占了较高的比例。 [陕西省农村信用社联合社三届六次社员代表大会暨全省农村合作金融机构2018年工作会议在西安召开] 5.12日：陕西省联社召开农信机构新一代信贷管理系统实务操作培训会 陕西凤翔农商银行成功实现“扫码”支付 陕西省联社召开全省农信机构2018年度工作会议 六点意见：一是坚守市场定位，全面回归支农和服务实体经济本源。 二是坚守风险底线，综合治理各类经营风险。 三是强化成本核算意识，积极推进财务管理转型。 四是强化依法合规意识，切实规范经营管理行为。 五是强化主动跟进意识，确保新一代信贷管理系统平稳运行。 六是强化服务意识，持续提升普惠金融能力和水平。 2.逆城市化（我没调查）综合以上的结论，我的题目想法陕西是一个有众多传统农村的地区。农村发展的基础比较好，但是任何改革的推进也比较难。 如果对于那些只想扩大生产计划，没有融资需求的人来说，再多的贷款政策也难以发挥原本计划的作用。 同样的，涉农金融机构的风险也一样，如果在求量的同时，保证不了质，也只能事倍功半。用金融领域的杠杆效应发展农业，是否真的合理，这有待调研。 换一个角度想，农民有农民的想法，这和农村发展现状分不开。 首先、谈谈规模 农业产业龙头企业肯定寥寥无几，多数农户依然是小规模经营。因此要考虑可持续性的问题。这涉及利益联结机制问题。 新型主体发展优不优，不是规模越大就越好，而是看联农带农的作用。这个产业链条中，哪一部分才是牵动农民发展核心的脉络？ 第二、再说政策支持 政策的金融支持力度够不够，这不能光看农民的需求（欲望无限），要考虑市场的需求。 现在讲究精准对接，还讲究电子平台（可以看二维码的新闻），可是这样做只是杯水车薪，而不是星星之火。但是却起到了很好的带头作用。 在金融方面的支持 六点意见：一是坚守市场定位，全面回归支农和服务实体经济本源。（啥也没说）二是坚守风险底线，综合治理各类经营风险。（基本要求）三是强化成本核算意识，积极推进财务管理转型。（不能舍重就轻）四是强化依法合规意识，切实规范经营管理行为。（基本要求）五是强化主动跟进意识，确保新一代信贷管理系统平稳运行。（具体操作）六是强化服务意识，持续提升普惠金融能力和水平。 最近的会议也挺多，其实看了半天，主动跟进才是金融扶持政策的核心。大概现在也没有好办法。 实体市场和农户对接的怎么样？农户的收益怎么计算（股份制合作社）？农户的困局在哪？ 如果是振兴，最重要的是考虑三产业的联合，做一条完整产业链，增加创收的渠道。 其实我还想查查看三产的情况…… 坚持“取之于农，用之于农”服务方向，开展普惠金融建设，全力支持乡村振兴。积极推动零售业务，创新信贷产品，助力新旧动能转换，服务实体经济发展。积极响应区委、区政府号召，担当社会责任，着力推动精准扶贫。 3.入手点3—乡村品牌化建设以十九大提出“乡村振兴战略”为契机，各地掀起了一轮乡村振兴的新热潮： 有的地方从产业入手，兴修扩建大肆发展产业；有的地方从环境美化入手，拆掉旧宅换新居；还有的地方着力开发旅游景区，发展乡村旅游…… 振兴思路大同小异 笔者不禁为乡村振兴的未来担忧，怕是又会出现“千城一面” 的状况。 那么，乡村振兴到底应该怎样实施呢？怎样才能真正做到解决就业、带动产业、打造经济富裕、安居乐业的城乡融合发展的幸福局面呢？ 笔者认为：乡村振兴的过程应该是基于大设计的乡村品牌化过程。 纵观华夏文明 5000 年发展史，中华文明深深植根于中国广袤的农村，农村的振兴恰恰应该是中国文化的复兴，5000 年的历史造就了中国不同地域的文化、民风、习俗，不同的乡村已然形成各具特色的文化风貌。因此，乡村振兴只有以挖掘区域文化为切入点，以系统化、整体化的“大设计观” 为指导，通过环境规划、产业植入、到民生配套、富民计划、再到品牌开发和推广并最终形成区域品牌产品的全产业链思维，才能使乡村振兴真正落到实处，并同步提升中国文化的国际地位和国际影响力。应该说：乡村品牌化是乡村振兴的必经之路。 例如：日本的一村一品，就是通过区域产品品牌的发展带动乡村产业的发展，反过来又通过乡村产业的整体发展反哺品牌的发展，形成了乡村发展的良性循环。日本有 47 个行政区域，每个区域都有自己的品牌产品，并借由每年 n 届的 D47(Design of 47 district )展览、市集为大众所追捧。另外，消费者热衷的产品到了生产地还能真切观赏产品的工艺生产过程，让人知其然，更能知其所以然，不仅带动了产品销售，当地产业旅游也蔚然成风，成功的完成了农村发展的各类资源导入和汇集，为村域经济的发展奠定了扎实的基础。 知乎的思路启发要致富先修路是硬道理，楼主说的基础设施应该包含交通、电力等基础设施，这当然这是基础，想想山区农村农民能做什么 一般来说在农村不可能像大城市那样搞工业，没有基础，并且没有任何特色的普通农村估计也没有天然的自然资源招商引资，多半还是只能种植农作物或者养家禽，外加便利的交通就能够快速的输送到有需求的城市。 但是农民缺少技术和知识也不了解市场到底要种什么、养什么，这个时候作为省直机关的干部的你就可以在这方面去调查投入，了解市场需求，根据农村的环境适合做什么，鼓励农民种植什么，养殖什么，并且可以派相关技术人员下乡协助，再下政策把农作物的价格调上去（如果可以的话），并且对种植作物的农民进行相应的补贴（补贴是应该能够很好的鼓励农民去种植农作物的方式）。 当然每个农村的实际情况和地方政策也不一样，如果有地方特色的话就很好，比如地方的特色美食、风景旅游等 所以，应当探讨农村的问题在哪？（逆向思维）农村发展的环境制约 （一）村庄没有规划，旅游区配套设施建设缺乏村庄建筑布局零乱、拥挤，地面肮脏，垃圾遍地、污水横流。 （二）农村环境污染严重，缺乏整治和保护农村环境污染主要表现在三个方面：一是农业生产污染，二是日常生活污染，三是水污染。 污染的原因 （一）农村垃圾发生了变化，缺乏有效的处置办法（二）农民环境意识较差，政府缺乏宣传教育（三）基层领导对农村环保的重要性认识不到位（四）环保基础设施缺失，环境监管保障体系不健全 整治的建议 （一）政策法规先行，舆论宣传引导提高城乡干部群众生态观念，形成全民参与农村生态环境保护的良好氛围 （二）基础设施先行，监督管理落实重点解决饮用水水源地保护、农村生活污水和垃圾处理、畜禽养殖污染治理等问题。农民向社区集中，以便生活垃圾和污水集中净化处理 （三）整治生活污染，注重环境保护 目标最终目标都必须使经济有持续稳定的发展，而旅游业和新型农业是发展经济最好的项目。这些绝不是在市区内发展起来的，更广阔的空间是在广大的农村，或与农村紧密相连的大自然。 农村旅游资源除文化优势外，就是山水风光。（这个……有待探讨……） 特色商品可以开发，自然环境一定要保护，人居环境一定要美化，农业和旅游业都要恢复生态自然，只有清新、舒适，健康快乐，人们才愿意不辞路远、不辞辛苦地前来旅游，地方上新型经济才有可能蓬勃发展，试验区建设目标才能最终实现。 袁家村相关（只是一个想法） 具体选择这里面：环境规划、产业植入、到民生配套、富民计划、再到品牌开发和推广。我觉得每一条都有东西可以挖 环境规划和产业植入，更重要的还是产业植入的方式，其他可以作为辅助手段和调研的附加内容 我们喊口号要“一二三产业融合”可是具体的植入路径如何确定，这值得我们去调研（在我们能调研清楚的范围内） 我们本次实践的特点：和民俗不同，我们在调研这类有特点的村庄时，可以主要研究产业植入和发展的模式过程，而不仅仅停留在民俗和文化产业上。包括家庭作坊手工业，农家乐集群服务业，村庄或集体管理组织模式等。]]></content>
      <tags>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社会实践时间轴]]></title>
    <url>%2F2018%2F05%2F16%2F%E7%A4%BE%E4%BC%9A%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[时间轴本周四：各位成员自行调研查资料，找文献确定自己的选题（可以在群里讨论，在周五讨论前最好不要否决任何方案） 本周五：周五讨论确定选题重要的还是选题找到好的问题和好的切入点 有了选题就可以进一步构思队名等等要素。。。 选题一:产业振兴包含但不限于基础农业帮扶、特色农业发展、农产品增值、旅游民宿服务业发展、村庄闲置资源利用、金融帮扶需求与对接等。选题二:人才振兴包含但不限于乡村人力资源梳理、新型农民培训、返乡青年创业环境营造、农村教育帮扶、外部人才与乡村需求对接等。 讨论：1.确定选题2.确定团队成员，团队基本信息3.写标书4.联系导师，联系实践地点5.分配实践任务，根据情况做进一步修正 一些职务职务 姓名 职责队长 王源禾 组织策划，把握节奏，联络成员，督促成员，团队安全副队长 董涵 协助队长外联1 外联接洽，邮件发送和收取，汇报外联2 外联接洽，邮件发送和收取，汇报宣传 微信公众平台，微博新闻稿记录 会议记录，拍照，录像技术 Ps，制作后期视频文案 新闻稿，整理活动资料后勤 管理经费，记录开销；安排食宿，管理物资 参考会议内容会议第一项：确定团队口号。会议通过，团队口号为：Tide Hunter GO-getter会议第二项：确定队服队徽。在队员提前设计好的基础上，其他组员对此提出意见并进行投票。队服队徽问世。会议第三项：初步修改标书。队长提前将初步组合完成的标书发送到QQ群。队员们仔细阅读后，通过对实施实践的进一步探讨，队员们就实践形式、实践环节、问卷、采访等部分提纲提出建议，并再次分配完善标书的任务。会议第四项：再次联系导师。队长分配与导师沟通的任务。总结：在与老师初步交流后，队员们对此次实践有更多的灵感。此次会议主要成果在于对具体的实践内容实践方法有了很大的改进。作为第一次全员集合的会议，为期近两个小时的会议使得队员间有更多思想的摩擦和碰撞，促进了队员之间的交流与合作。 标书分割社会实践标书分割 5月23日 要求上交团队信息表（只是为了以学院为单位领取介绍信，便于进一步索要调研单位回函， 团队内部信息比如人员，队名，课题后续可以变动，团队信息以标书中团队信息表为准。） 6月6日 提交标书、接收函大功告成]]></content>
      <tags>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计计算课程代码笔记（二）]]></title>
    <url>%2F2018%2F04%2F27%2F%E7%BB%9F%E8%AE%A1%E8%AE%A1%E7%AE%97%E8%AF%BE%E7%A8%8B%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[还是熟悉的feng.li老师，还是熟悉的瓜皮禾禾 第四章 如何画出精美的统计图其实并不精美（拖走4.14.24.34.4 船新的画图利器-recharts 第五章 代码背后的逻辑-牛顿迭代篇其实并没搞懂（抢救一下5.15.25.35.4 课后经典作业代码矩阵法求线性回归系数 待续…]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[政经研究|东方智慧给出的社会主义出路]]></title>
    <url>%2F2018%2F04%2F25%2F%E4%BA%BA%E7%B1%BB%E5%91%BD%E8%BF%90%E5%85%B1%E5%90%8C%E4%BD%93%2F</url>
    <content type="text"><![CDATA[东方智慧给出的社会主义出路形式与政策老师是个美女。倒不是说老师长得很好看，但和第一节课的臃肿世俗的男老师一比就知道什么是地狱与天堂。老师不仅仅美丽，还有种仙女气，可能研究马克思的人都有种仙气儿，从她的身上总能察觉那不同凡响的魅力。这堂课很神奇，我产生了很多很多遐想。 以前的人们辛勤劳作，农民生产粮食，劳动越努力，获得的粮食就越多。农民们凭借结余往往能够使自己生活更加富足，成为富农。而在资本主义中，工人们当然也辛勤的劳动。但是工人们生产越多，老板就获得越多，反过来自己仍然获得相同的报酬。而那被不断提高的标准，也成为一条淘汰的线，不停筛选着不能达标的工人。 “生产关系”从古到今产生了微妙的变化。每学期的思想政治课，我们都会提到这玄乎的“生产关系”，但其实要理解这个词很简单。即“所有权+分配权”就是生产关系。任何社会现状，都是因为生产关系而产生的。任何改变如果不触及生产关系，产生的表面问题可能能够隐藏，但是问题的诱因绝不会消除。 有人过分看待了“教育”的作用。在日本，教育已成为阶级固化的象征。什么经济基础决定了什么样的教育，低收入阶级绝对上不了最顶尖的大学。反观中国，教育资源从来就没有公平过，这不仅仅是分配不公平的问题，更有“所有权”的问题。 社会不公平自古有之，全世界也普遍存在，但是甚至从未有减小的趋势，这也是由生产关系决定的。贫富差距扩大的开关在40年前的某一天一瞬间打开了。如果我们的改革只停留在“教育”“经济结构”这种表面的框架上，不公平的问题将永远不会得到解决。 我曾经听过一些不同与主流的言论，声称工业化的基础是军重工业。根据某he的历史知识，这一点也惊人地正确。新中国经历了很多次周期性的经济危机（有人祸的成分，但总归是经济危机），依然建立起了属于自己的完整的产业结构，完成了资本的积累进入了工业化，把投入回报不合理的军重工业发展了起来。 纵观世界，那些成功完成了（重）工业化的国家，无一没有在军事上实现大投入和大发展。完成了工业化的国家才能在世界上拥有立足之地，欧洲、日本、韩国实现了，而巴西、东南亚失败了。资本全球化之后，没能完成工业化的国家，自然会成为落入生产剥削链的底端。 为了实现工业化，中国过去牺牲了农业。牺牲了小农的利益。为了富强，我们也看着城乡的贫富差距不断变大。中国特色的市场经济，现在已经走上了国企股份化的道路。这是马克思所说的资本主义的“最高形态”。我们引进了市场经济，走向了富强。当然也带来了资本主义生产关系的弊病。而这正是困扰我们社会许久的问题。 但是不实现工业化，又何谈发展经济？更别提经济基础之上的建筑了。这是发展道路的必然，是依靠资本主义的市场发展生产力导致的“生产关系”的必然。 政治经济的经济问题就先到这里，政治的话题也不想说太多，反正都是送命题，还是再想探讨一下未来。 马克思构想的社会主义的未来是什么样的呢？《资本论》中说，社会主义同样是为了解放生产力，但是生产关系却和资本主义截然不同。社会主义将构建一种全新的，甚至现在人无法想象或认可的“生产关系”。私有制产生了资本主义。也许只有某一种全新的个人所有制，才会孕育出“初心般的”社会主义吧。 这种全新的个人所有制，甚至都没有理论研究过的全新制度，超前了当今所有的思想很多年。那么，在当今的价值观之下，它如何才能诞生呢？我觉得，不是没有办法的。我们还有一样法宝没有使用。那就是“中国特色”。只有依靠自己，在没有前人经验的前提下，运用我们的智慧在顺水推舟下才有可能做到所有制观念的变革。 而中国特色对当今政治局势的答案呢？东方总是有着超脱世界经验的智慧的。我想，我们的答案也许正是“人类命运共同体”吧。只有时间会告诉我们这张东方智慧答卷的得分了。20180425氦核ps希望别被查水表]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[datasets（R自带数据包）]]></title>
    <url>%2F2018%2F04%2F15%2Fdatasets%EF%BC%88R%E8%87%AA%E5%B8%A6%E6%95%B0%E6%8D%AE%E5%8C%85%EF%BC%89%2F</url>
    <content type="text"><![CDATA[众所周知，r语言自带很多好用的数据。他们就像代码功能大江之下的暗流，虽然使用者通常难见其全貌，但它却能却将它深深的影响传给江面上的我们。今天氦核就领着大家来见识一下这神奇的暗流~ 向量euro #欧元汇率，长度为11，每个元素都有命名landmasses #48个陆地的面积，每个都有命名precip #长度为70的命名向量rivers #北美141条河流长度state.abb #美国50个州的双字母缩写state.area #美国50个州的面积state.name #美国50个州的全称 因子state.division #美国50个州的分类，9个类别state.region #美国50个州的地理分类 矩阵、数组euro.cross #11种货币的汇率矩阵freeny.x #每个季度影响收入四个因素的记录state.x77 #美国50个州的八个指标USPersonalExpenditure #5个年份在5个消费方向的数据VADeaths #1940年弗吉尼亚州死亡率（每千人）volcano #某火山区的地理信息（10米×10米的网格）WorldPhones #8个区域在7个年份的电话总数iris3 #3种鸢尾花形态数据Titanic #泰坦尼克乘员统计UCBAdmissions #伯克利分校1973年院系、录取和性别的频数crimtab #3000个男性罪犯左手中指长度和身高关系HairEyeColor #592人头发颜色、眼睛颜色和性别的频数occupationalStatus #英国男性父子职业联系 类矩阵eurodist #欧洲12个城市的距离矩阵，只有下三角部分Harman23.cor #305个女孩八个形态指标的相关系数矩阵Harman74.cor #145个儿童24个心理指标的相关系数矩阵 数据框airquality #纽约1973年5-9月每日空气质量anscombe #四组x-y数据，虽有相似的统计量，但实际数据差别较大attenu #多个观测站对加利福尼亚23次地震的观测数据attitude #30个部门在七个方面的调查结果，调查结果是同一部门35个职员赞成的百分比beaver1 #一只海狸每10分钟的体温数据，共114条数据beaver2 #另一只海狸每10分钟的体温数据，共100条数据BOD #随水质的提高，生化反应对氧的需求（mg/l）随时间（天）的变化cars #1920年代汽车速度对刹车距离的影响chickwts #不同饮食种类对小鸡生长速度的影响esoph #法国的一个食管癌病例对照研究faithful #一个间歇泉的爆发时间和持续时间Formaldehyde #两种方法测定甲醛浓度时分光光度计的读数Freeny #每季度收入和其他四因素的记录dating from #配对的病例对照数据，用于条件logistic回归InsectSprays #使用不同杀虫剂时昆虫数目iris #3种鸢尾花形态数据LifeCycleSavings #50个国家的存款率longley #强共线性的宏观经济数据morley #光速测量试验数据mtcars #32辆汽车在11个指标上的数据OrchardSprays #使用拉丁方设计研究不同喷雾剂对蜜蜂的影响PlantGrowth #三种处理方式对植物产量的影响pressure #温度和气压Puromycin #两种细胞中辅因子浓度对酶促反应的影响quakes #1000次地震观测数据（震级&gt;4）randu #在VMS1.5中使用FORTRAN中的RANDU三个一组生成随机数字，共400组。 #该随机数字有问题。在VMS2.0以上版本已修复。 rock #48块石头的形态数据sleep #两药物的催眠效果stackloss #化工厂将氨转为硝酸的数据swiss #瑞士生育率和社会经济指标ToothGrowth #VC剂量和摄入方式对豚鼠牙齿的影响trees #树木形态指标USArrests #美国50个州的四个犯罪率指标USJudgeRatings #43名律师的12个评价指标warpbreaks #织布机异常数据women #15名女性的身高和体重 列表state.center #美国50个州中心的经度和纬度 类数据框ChickWeight #饮食对鸡生长的影响CO2 #耐寒植物CO2摄取的差异DNase #若干次试验中，DNase浓度和光密度的关系Indometh #某药物的药物动力学数据Loblolly #火炬松的高度、年龄和种源Orange #桔子树生长数据Theoph #茶碱药动学数据 时间序列数据airmiles #美国1937-1960年客运里程营收（实际售出机位乘以飞行哩数）AirPassengers #Box &amp; Jenkins航空公司1949-1960年每月国际航线乘客数austres #澳大利亚1971-1994每季度人口数（以千为单位）BJsales #有关销售的一个时间序列BJsales.lead #前一指标的先行指标（leading indicator）co2 #1959-1997年每月大气co2浓度（ppm）discoveries #1860-1959年每年巨大发现或发明的个数ldeaths #1974-1979年英国每月支气管炎、肺气肿和哮喘的死亡率fdeaths #前述死亡率的女性部分mdeaths #前述死亡率的男性部分freeny.y #每季度收入JohnsonJohnson #1960-1980年每季度Johnson &amp; Johnson股票的红利LakeHuron #1875-1972年某一湖泊水位的记录lh #黄体生成素水平，10分钟测量一次lynx #1821-1934年加拿大猞猁数据nhtemp #1912-1971年每年平均温度Nile #1871-1970尼罗河流量nottem #1920-1939每月大气温度presidents #1945-1974年每季度美国总统支持率UKDriverDeaths #1969-1984年每月英国司机死亡或严重伤害的数目sunspot.month #1749-1997每月太阳黑子数sunspot.year #1700-1988每年太阳黑子数sunspots #1749-1983每月太阳黑子数treering #归一化的树木年轮数据UKgas #1960-1986每月英国天然气消耗USAccDeaths #1973-1978美国每月意外死亡人数uspop #1790–1970美国每十年一次的人口总数（百万为单位）WWWusage #每分钟网络连接数Seatbelts #多变量时间序列。和UKDriverDeaths时间段相同，反映更多因素。EuStockMarkets #多变量时间序列。欧洲股市四个主要指标的每个工作日记录，共1860条记录。]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression II]]></title>
    <url>%2F2018%2F04%2F11%2Fnew-article%2F</url>
    <content type="text"><![CDATA[r语言-线性回归模型 Donner Party: The DataIn 1846, a group of 87 people (calling themselves the Donner Party) were heading to California from Illinois. The leaders were stranded in the Sierra Nevada and were stranded there through the winter. Many people died due to the cold and lack of food. 12x &lt;- read.csv(&quot;Donner.csv&quot;, as.is = TRUE)head(x) Let’s polish up the Gender variable a bit for clarity. 1x$Gender &lt;- factor(ifelse(x$Gender == 1, &quot;Male&quot;, &quot;Female&quot;)) Modeling SurvivalDo Females Have a Higher Rate of Survival?1table(x$Gender, x$Survived) We might want to study whether or not females are better able to survive through the harsh conditions. How do we answer this question? 12m1 &lt;- glm(Survived ~ Gender, data=x, family=binomial)summary(m1) We haven’t talked about where our p-values come from, but you can see from the summary that Gender is significant in predicting the probability of survival. Interpretation of this coefficient: Males were on average $e^{-1.28}=0.28$ times as likely as women were tosurvive. This might seem odd. Logistic regression is meant to fit an S-shaped curvefor the probability of survival here, but we have a categorical predictor. How doesthis work? Essentially, by passing in a categorical predictor as a factor,R created a dummy variable that equals 1 when Gender == &quot;Male&quot; and 0otherwise. So if we were to look at a plot of GenderMale on the x-axisagainst Survived on the y-axis, logistic regression is essentially stillfitting an S-shaped curve, but with only two points of interest, when $x=0$ andwhen $x=1$. Is Age Associated with Survival?Suppose we now consider a different question, studying whether or not Age can help us predict survival. We need to first transform Age into a quantitative variable; it is currently character type because there is an asterisk somewhere, standing in for a missing value. 123class(x$Age)x$Agex$Age &lt;- as.numeric(x$Age) Your book ignores any assumptions for logistic regression, but that doesn’t mean there aren’t any. The linearity assumption requires that the log-odds be a linear function of the predictor variable. We need to be able to estimate log odds for various age groups. (The reason why it doesn’t make sense to estimate log odds for each age is that we simply don’t have enough data at each age.) To start,we will want to create a categorical version of Age (using equal sized bins). 1x$AgeGrp &lt;- cut(x$Age, 6) Let’s practice checking the linearity assumption (although in general cases, we won’t bother). 12345binned.x &lt;- tapply(x$Age, x$AgeGrp, mean) # average age per binbinned.p &lt;- tapply(x$Survived, x$AgeGrp, mean) # rate of survival per binplot(log(binned.p/(1-binned.p)) ~ binned.x, main=&quot;Empirical Logit Plot&quot;, pch=16)plot(binned.p ~ binned.x, main=&quot;Pr(Survival) by Age&quot;, pch=16) As we can see, this assumption is hard to check with few observations, becausesome of the bins are going to be extremely sparse (have very little data). When there are a large number of observations, it’s much easier to check this assumption between a single predictor and the response. However, when there are a lot of observations, there will likely be multiple predictors, and then checking this assumption for every predictor can be tedious as well. As a result, we won’t bother checking this assumption in general. 12m2 &lt;- glm(Survived ~ Age, data=x, family=binomial)summary(m2) You can see again that Age is significant at the $\alpha=0.05$ level. There is also a negative coefficient, suggesting that we would have a downward sloping logit curve. This suggests that older people have lower chances of surviving the extreme conditions. 1234plot(Survived ~ Age, data=x, pch=16)tmp &lt;- data.frame(Age=seq(0,70, by=1))pred &lt;- predict(m2, newdata=tmp, type=&quot;response&quot;)lines(tmp$Age, pred, lwd=2, col=&quot;blue&quot;) Both Age + Gender12m3 &lt;- glm(Survived ~ Age + Gender, data=x, family=binomial)summary(m3) It appears that both Age and being male decreases the odds of survival. 95% confidence intervals for both coefficients are: 1confint(m3, level=0.95) Undoing the log-transformation yields: 1exp(confint(m3, level=0.95)) Between two individuals of the same age but of different genders, the male was between 0.11 to 0.78 times as likely as the female to survive. Between two individuals of the same gender but with age differing by 1 year, the older individual is 0.93 to 1.00 times as likely to survive as the younger individual. In this model, we have not interacted Age and Gender, so essentially it is likefitting 2 parallel lines to the log-odds of survival. 12345678plot(Survived ~ Age, data=x, pch=16, col=ifelse(Gender==&quot;Female&quot;, &quot;red&quot;, &quot;blue&quot;))agetmp &lt;- seq(0,70, by=1)pred_fem &lt;- predict(m3, newdata=data.frame(Age=agetmp, Gender = &quot;Female&quot;), type=&quot;response&quot;)lines(tmp$Age, pred_fem, lwd=2, col=&quot;red&quot;)pred_m &lt;- predict(m3, newdata=data.frame(Age=agetmp, Gender = &quot;Male&quot;), type=&quot;response&quot;)lines(tmp$Age, pred_m, lwd=2, col=&quot;blue&quot;) If we believe that the slopes of age should differ with respect to each gender, then we should interact the two variables: 1m4 &lt;- glm(Survived ~ Age * Gender, data=x, family=binomial) 12345678plot(Survived ~ Age, data=x, pch=16, col=ifelse(Gender==&quot;Female&quot;, &quot;red&quot;, &quot;blue&quot;))agetmp &lt;- seq(0,70, by=1)pred_fem &lt;- predict(m4, newdata=data.frame(Age=agetmp, Gender = &quot;Female&quot;), type=&quot;response&quot;)lines(tmp$Age, pred_fem, lwd=2, col=&quot;red&quot;)pred_m &lt;- predict(m4, newdata=data.frame(Age=agetmp, Gender = &quot;Male&quot;), type=&quot;response&quot;)lines(tmp$Age, pred_m, lwd=2, col=&quot;blue&quot;) 1summary(m4) Was this a good thing to do? No, because now none of the predictors are statistically significant. 12345par(mfrow=c(1,2))hist(x$Age[x$Gender == &quot;Male&quot;], xlim=c(0, 70), main=&quot;Age of Males&quot;, breaks=seq(0,70,by=10), ylim=c(0,18))hist(x$Age[x$Gender == &quot;Female&quot;], xlim=c(0, 70), main=&quot;Age of Females&quot;, breaks=seq(0,70,by=10), ylim=c(0,18)) Significance of PredictorsDeviance is -2 times the log-likelihood of the model and can be thought of as something like residual sum of squares in linear regression, that is a quantity for which a smaller number represents a better fit of the model. 1m4$deviance Note that this corresponds to the line in the regression output called residual deviance. The null deviance is the deviance of a model containing no predictors (just the intercept). We can use the drop-in-deviance test for evaluating the significance of individual predictors: 1anova(m4, test = &quot;Chisq&quot;) This presents an alternate way to test for the significance of individual predictors, and is often thought of as more reliable than Wald’s test, which is used for p-values in the regression summary. We can also test nested models. Recall: 12coef(m1)coef(m4) eval1anova(m1, m4, test=&quot;Chisq&quot;) Why didn’t this work? Let’s try this again… 12m1a &lt;- glm(Survived ~ Gender, data=x, subset=!is.na(x$Age), family=binomial)summary(m1a) When you use anova(lm.1,lm.2,test=”Chisq”), it performs the Chi-square test to compare lm.1 and lm.2 (i.e. it tests whether reduction in the residual sum of squares are statistically significant or not). Note that this makes sense only if lm.1 and lm.2 are nested models. 12345anova(m1a, m4, test=&quot;Chisq&quot;)anova(m1a, m2, test=&quot;Chisq&quot;) # ??anova(m3, m4, test=&quot;Chisq&quot;) Confusion MatricesAnother way to compare models is to examine confusion matrices whichcompare $y$ to $\hat y$. This can be used even if models are not nested. 12table(fitted=(fitted(m1) &gt;= 0.5)*1, actual=x$Survived)mean((fitted(m1) &gt;= 0.5)*1 != x$Survived) By the way, is this good? Significantly better than random guessing? 12table(fitted=(fitted(m2) &gt;= 0.5)*1, actual=x$Survived[!is.na(x$Age)])mean((fitted(m2) &gt;= 0.5)*1 != x$Survived[!is.na(x$Age)]) 12table(fitted=(fitted(m3) &gt;= 0.5)*1, actual=x$Survived[!is.na(x$Age)])mean((fitted(m3) &gt;= 0.5)*1 != x$Survived[!is.na(x$Age)]) 12table(fitted=(fitted(m4) &gt;= 0.5)*1, actual=x$Survived[!is.na(x$Age)])mean((fitted(m4) &gt;= 0.5)*1 != x$Survived[!is.na(x$Age)])]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[品不为|“玩家”]]></title>
    <url>%2F2018%2F04%2F11%2F%E5%A4%B4%E5%8F%B7%E7%8E%A9%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[头号玩家简评（无剧透） 清明去看了被刷屏的《头号玩家》，看完第一感觉大呼过瘾，许久没有享受过如此视觉盛宴。痛快之余竟然也觉得稍微有些头晕目眩，但是反过来也不禁惊叹——“这才是真正的大片”。 影片讲的是这样的一个群体：比起在对战中取胜，他们更喜欢寻找游戏中的更难获得的彩蛋；比起组队享受互动和配合中产生的乐趣，他们更倾向独自探索和钻研游戏；比起胜利的结果，他们最关心的还是在游戏中取胜的道路……他们便是“玩家”。 作为一个游戏爱好者，我也接触过不少游戏作品。守望先锋里的D-VA有这么一句台词：“玩游戏，就是要赢！”我曾经觉得这种太在意胜负的心态永远享受不了游戏全部的乐趣，游戏就是放下胜负去玩耍，是放松自己的工具罢了。这大概就是游戏爱好者和玩家的区别吧。任何身份，一旦带上一个“家”字，就会产生微妙的差别。玩家，和我们爱好者不一样。 这部影片并不是想给我们留下“参与到游戏中比起争取胜利更加重要”这样的感觉。可其实真正的共鸣在于：在强烈的胜负感下参与游戏和比赛，拼上所有只为战胜对手取得领先的过程。只有真正为了胜负去做某些事，我们才能感受到那种别样的感情。有人夸张地说这是“浪漫”，不是没有理由的。 举个好理解的例子：别人总说围棋是一项修身养性的运动，下棋就是陶冶情操，贪胜之心要置之盘外，每一盘棋就是和对手的“手谈”。更有人提出了“快乐围棋”的说法，小朋友学会和人下围棋是为了锻炼思维，还能交朋友……不好意思，那不是围棋。 围棋在十九路棋盘之上，没有安乐的死，没有苟且的活，没有尊严的负，没有屈辱的胜。有的是胜则滴水不漏不动如山，有的是负则宁为玉碎不为瓦全，攻就是最辛辣的杀招，防就是最顽强的抵抗。展示出最强的自己，既是对对手最大的尊重，也是围棋体育精神所在。盘外谦谦君子，盘中胜负斗士，这是每个棋手的终极目标！ 玩家，也应如是了。真正想赢的人，看了这部电影，大概会明白一点自己的那颗不断跳动的心吧。 （完）4/11/2018]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这是一个Rmd测试文档]]></title>
    <url>%2F2018%2F04%2F09%2Fhtml%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[测试html文件上传，测试图标显示 开门见山~ 小禾禾那天研究了下rmd文档，发现用rmd（Rmarkdown）生成的html文档还挺漂亮。 作为外貌协会钻石会员，小禾禾不禁想把做好的html上传进自己的博客。 传送门a only a test~]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言程序包分类一览（转）]]></title>
    <url>%2F2018%2F04%2F09%2Fr%E5%8C%85%2F</url>
    <content type="text"><![CDATA[R语言由近几年随着数据挖掘、机器学习在国内兴起而大热，现在R已经发展成为一个社区语言，有者非常多的packages支持工程应用，几乎任何问题都可以在R的packages中找到解决方案，这是R优于SPSS和SAS（模块化分析）的一个强大功能。此外，对于没有编程基础的统计学和金融学领域分析人员，R语言以较易的代码任务而胜过python（需要专门训练的编程项目）。 目前在CRAN和GitHub上的packages大约超过1万个。整理了一些常见学科使用频率较高的 packages： 教育类：learnr：支持使用markdown等工具创建交互式的教材，进行R和R包的学习 olsrr：辅助进行普通最小二乘回归的学习和实践 rODE：内置大量函数，展示学生的一些自然状态下的表现，如常微分方程如何求解，如何更有效的构建方程等。 repurrrsive：包含R对象、JSON和XML的形式递归的列表，用于在教学中使用示例，包括调色板、GitHub库等。 金融:alphavantager ：提供了alpha vantage API，可以获取股票、物理货币和数字货 币等历史交易数据。 ​ 音乐:billboard 包含了从1960年到2016年公告牌百强榜单上歌曲的数据集，包括给定年份的排名、音乐特征和歌词。 地图：mapsapi：提供了谷歌地图数据接口。 学术语义：microdemic：支持在Microsoft Academ Graph中对学术文章的程序化访问。 数据挖掘：Rattle：可视化数据挖掘工具 是使用RGtk2包提供的Gnome图形用户界面 Rattle的安装：1）install.packages(“RGtk2”) 2) install.packages(“rattle”) 3)需要安装XQutrz启用X11 交互：GoogleVis API在R制作网页HRML，调用Google charts作图，交互式的HTML图表 Manipulate：人际互动 Rcharts：用R与javascript做交互式可视化产品 Shiny：制作嵌入网页的交互式R程序平台 Solidify：制作和发布基于R的报告 文本处理：wordmatch：用于两个word文本对照 readtext：支持文本文件或格式化的文本文件导入， :.csv,.tab,.json,.xml,.pdf,.doc,.docx,.xls,.xlsx 数据库：hdf5r:提供一个使用R6类的HDF5 API的面向对象包装器。 RMariaDB：实现与MariaDB and MySQL数据库兼容的DBI接口。 可视化（Data Visualization）:rgl：3D可视化 ggplot2:高级绘图包 ggplotgui：支持shiny app创建和优化ggplot2图形，并产生要求的R代码 lattice：高级绘图包 wordcloud2:绘制词云 ggjoy：joyplots提供了时间和空间分布的变化 DescriptiveStats.O Beu:包含部分函数为OpenBudgets.eu数据集提供估计和返回可视化所需的相关参数。 统计学：adaptiveGPCA：自适应的主成分分析算法实践 sfdct：支持针对简单的对象构建德劳内三角 cnbdistr：条件负二项分布的分布函数 llogitstic：密度、分布、带参数的l-logistic分布分位数和随机数生成函数 RBest：提供支持贝叶斯证据合成的工具集，包括荟萃分析、历史数据的先验推导、操作特性分析 SMM：提供了多状态离散时间半马尔可夫和马尔可夫模型多模拟和估计功能。 BayesRS：适用于分层线性贝叶斯模型，并计算出与savge-dickey密切比有关的群参数的贝叶斯因子。 CovTools：提供了几何和推理工具，方便分析协方差结构和多元统计中的协方差分析。 emmeans：提供了许多函数，计算线性／广义线性／混合模型的估计边际均值（EMMS） NLP：自然语言处理 Natural Language Processing tm：英文分词工具 jiebaR：中文分词工具,本身是C++写的，具有极高的运算处理速度 Rwordseg：中文分词工具，Mac版依赖旧版本的Java，较难安装，版本更新慢，不推荐使用 chinese.misc：中文分词工具，内核基于Rwordseg rmmseg4j： snowball:英文词干化 lexiconPT：为情绪分析提供对葡萄牙词汇的访问 越往后越扯淡…… QAQ 数据处理（Numerical Mathematics）：Matrix MASS gsl polynom signal GetLattesData：实现了一个API，支持从Lattes直接下载和读取XML数据 贝叶斯分类（Bayesian Inference）：BACCO bayesm bayesSurv LaplacesDemon MIBayesOpt：提供了一套支持贝叶斯优化方法的计算框架，辅助支持向量机、随机森林和极端梯度增强模型优化超参数。 生物：CytobankAPIstats：提供了从细胞库获取和处理细胞数据的工具。 聚类（Cluster Analysis &amp; Finite Mixture Models）：apcluster clusterSim clustMixType evclust trimcluster apcluster 原文地址：CSDN博客]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闻歌如面|谁说词人多痛苦（上）下笔]]></title>
    <url>%2F2018%2F04%2F05%2F%E8%B0%81%E8%AF%B4%E8%AF%8D%E4%BA%BA%E5%A4%9A%E7%97%9B%E8%8B%A6%2F</url>
    <content type="text"><![CDATA[简介歌词写作系列谁说词人多痛苦（上） 本篇目录：1.简介苦大词作2.盛开一支思路3.万言不在纸端 谁说词人多痛苦 上篇壹 词作苦大愁深（才没有！）词作是什么?能吃否?虽说在好歌背后，词作的存在感一般都很低。这倒不是因为大家不喜欢词，而是写歌词是件说简单很简单、说难又很难的事。 但是不管怎样，歌词也是一种文学。写歌词，首先是朗朗上口，接下来表达真情实意。这么想的朋友，恭喜你，你已经成功打开了“填词”这个新技能。 其实仔细想来，填词是很简单的，无非把想说清的事情说清楚，将想表达的情感传达出去，在音韵上稍加注意，一首简单的歌词就悄悄写成了。 主流的歌词正是为了传达感情而存在的。至于其他类型我们先不谈。 词作和读者仿佛在进行一场博弈，词作用尽笔墨创作，读者挑剔批判品读。而胜负就在于歌词中传递的那一份感动。哪一方被感动，就算是输了。 有时作者几句妙笔，就让听众涕泗横流；可有的作者江郎才尽揣摩良久，自觉满篇触动而读者们却回响平平，这可能就是词作者的失败了。 词作是一个什么存在呢？大概过着比别人痛苦的生活吧？大概是把热情献给世界，把孤勇留给自己的单身狗吧？ emmmm……其实并不是这样的…… 可能词作者们只是稍稍的有那么些敏感，有人也很粗犷耿直，但是总的来说都是敏感而富有感触的人。但凡一个有经历的人，也许都能成为填词人。 上面废话完了 最简单的歌词说了半天,还是让我们先从简单的歌词开始聊吧~ 最简单的歌词长什么样呢？ 《认真地老去》词：张希1忘了什么时候开始到清晨才能入睡也忘了什么叫做结尾又有谁在乎呢凌晨三点的窗前播放着那段时光有一个骄傲的少年隐藏他的青春嗯…… 不如让我忘了自己你觉得怎么样呢在每个向往的地方释然一个遗憾躺在我怀里的吉他好像厌倦了我重复最熟悉的段落好像无话可说这生命正值春光别装作刀枪不入的模样 别错过年轻的疯狂时光很匆忙别错过日落和夕阳不论在哪里呀来不及认真的年轻过就认真地老去又一次和你擦肩而过一毫米的距离 2请你（让我）再次抱起吉他为你唱那一首歌重复最熟悉的段落就当明天不在没有永远的年轻没有唱不完的歌当所有人都离去我也将要离去这生命正值春光别装作刀枪不入的模样 别错过年轻的疯狂时光很匆忙别错过日落和夕阳不论在哪里呀来不及认真的年轻过就认真地老去又一次和你擦肩而过一毫米的距离 别错过年轻的疯狂时光很匆忙别错过日落和夕阳不论在哪里呀来不及认真的年轻过就认真地老去又一次和你无话不说开始对话以前（完） 上面这首词便是很好的。基础词人的巅峰水平便是如此。真情实意，稍稍押韵，给人简单的触动和波澜。满足这几点就很好。 歌词不像诗词，语言上不需要花大量精力做精细的雕琢（当然能雕琢是最好的），好歌词一定是传情达意的杀器，往往一语一行就能戳中听众的内心，引起听众和作者的共鸣。有时要借助歌曲的帮助，这是后话。 反例处刑现场（你们懂得） 《跨越无数的夜晚》氦核 渣填词段1冷树寒路漫漫人潮散逢野村寻酒栈 见店门已关 何夜波澜逆旅横榻独自言欢白发向镜中窜 对杯又空向晚 请月谈 草消没会新展 心境相系音却断经年影吊形单 前后无门困茫然故关 山水妙五湖四海愁不完 情肠断花柳巷陌淡然 重聚一梦黄粱散驻马霜林尽染 黄叶扫净离人叹蓦然回首见灯火阑珊 是你藏衰草看 倚栏 爱恋与哀婉 跨无数夜晚我反侧辗转 佳人在一方笑谈数尽了千帆 弦音湿青衫归来无盼 独对整晚 段2闲棋凉茶曳曳烛火残对镜添妆也懒 呆坐镜前看 银丝半绾红笺寄何 今夜月满仙姑几时下凡 邀我美人帖传 登楼看 画伤心丹青难 琉璃脆弱彩云散青山眉凝峰峦 花戴鬓头为谁绽小寒 夏虫绝秋日半春被冬拦 忆鸣蝉千金掷君不还 泪三千烈酒亦淡不只蜀道路难 君家无信怎能堪才下眉头却往心头灌 空闺夜夜守穿 泪弹 爱恋与哀婉 跨无数夜晚我反侧辗转 佳人在一方笑谈数尽了千帆 弦音湿青衫归来无盼 独对整晚 间奏:天涯歧路漫 何时青鸟殷勤探梦相舞 银河落高处不胜寒铁马冰河今夜暂且缓羁旅游宦 尘务冗缠只求梦聚 今晚 好梦总短暂 梦成亦美满梦里的温婉 你一笑天地黯然见难别亦难 执手泪斑斑无语离散 总是悲欢（完） 读起来别扭吗？是不是心中毫无波澜？ 没错，这是因为这篇词似乎并非自己的真情实意，完全是一副 “少年不识愁滋味，为赋新词强说愁” 的模样。 这篇词其中有情感吗？有的，可是情感之间并没有灵魂在其中，不是自己的情感，亦不是某个人的情感。颇有些吊书袋之感。 在我迷茫乱写的日子里，突然有人和我说，古风歌无非两种写法: 一种 是写场景，写环境，写时代，是针对整体的创作. 另一种 是写个人，写故事，写形象，是针对个体的创作。 其中，最关键的是要有一条线索。 这条线索贯穿着全篇，可以是情感的变化，可以是时间的迁移，可以是故事的进展，可以是情境的更替等等。但是一般的词人往往忽视了其中的线索，写出来的东西就连自己都不知道在说些什么，更没有一个中心思想，这作品就落入下乘了。 这种线索，通常用来检视一篇词是否达到基本要求，当然也可用来自己反省自己。 也正是在这一篇渣词之后，氦核意识到了自己的蠢萌（雾），总算是进入了基础级词人的行列。 贰 盛开一支思路大招:情感爆发!情感上可不止这一点点小操作。上面提到的线索，在大佬眼里都是没用的枷锁，是大佬们在创作中完全不用考虑的。 词人们把这一种创作叫做“情感爆发” 。举个经典到几乎无人不知的例子： 《牵丝戏》词：Vagary主歌：嘲笑谁恃美扬威 没了心如何相配盘铃声清脆 帷幕间灯火幽微我和你 最天生一对 没了你才算原罪 没了心才好相配你褴褛我彩绘 并肩行过山与水你憔悴 我替你明媚 是你吻开笔墨 染我眼角珠泪演离合相遇悲喜为谁他们迂回误会 我却只由你支配问世间哪有更完美 副歌：兰花指捻红尘似水三尺红台 万事入歌吹唱别久悲不成悲 十分红处竟成灰愿谁记得谁 最好的年岁 你一牵我舞如飞 你一引我懂进退苦乐都跟随 举手投足不违背将谦卑 温柔成绝对 你错我不肯对 你懵懂我蒙昧心火怎甘心扬汤止沸你枯我不曾萎 你倦我也不敢累用什么暖你一千岁 风雪依稀秋白发尾灯火葳蕤 揉皱你眼眉假如你舍一滴泪 假如老去我能陪烟波里成灰 也去得完美风雪依稀秋白发尾（完） 这首词是“情感爆发”的典型作品。这首歌词看起来有明晰的思路吗？ 似乎全篇都笼罩在一种情绪之下，但是整体读来浑然一体，将一份感情描绘地淋漓尽致，反复渲染，让全曲在感情舒张中起伏，引人入胜。 为何这种看起来没有思路的歌词反倒博人感动呢？原因在于，虽然没有表面的思路，但这首歌有一条游于肌理之下的思维之线。 主歌前两段中用每个小小的行动点出两人天生一对，一唱一和，一引一舞，让人觉得如戏如画，毫无单调的感觉。 主歌第三段几句对比用情至深，似乎都是生活小事，但真爱没有细节与大局之分，面面俱到，事事如一的爱，几句话写出了那么深刻的感情，怎能不让人感动。 副歌升华，其实在这些爱的行动背后想表达自己长相守的夙愿，悠扬而曲折，多了层不顾一切的荡气回肠，戏腔也减少了词里的忧虑感。让人感动之余充满希望和欣慰。 1整个曲子是连贯的感情输出，引人入胜，微言深情，这便是这首歌的妙处。 当然，这只是个人有目的的浅薄理解，为了告诉在读的各位，歌词中抒发感情的妙法。这种情感肌理搭配歌曲起伏渲染才造就了这首歌的成功，这是后话。 再读一篇？多得很呢！ 《国境四方》填词：小驴1我愿生而彷徨 我愿生而动荡 我愿生而你便是我的王我对你臣服或仰望 亲吻你靴上的金色徽章我捧着孤勇一腔 饰演你隐秘而危险的欲望我等待着 被你禁锢永恒的饲养 虚拟世界的版图总在扩张 你是铩羽而归的勇将带着夜风的冰凉 利爪撕扯开我狡诈的伪装你踏过泥沼 与草木洪荒那时震彻的心脏 叫嚣着被征服也能同你站成一方 世上所有情绪于我都无关痛痒只被你掌控的颤栗触碰 终没顶将我裹挟进缚网我竟期盼被吞食被你仔细品尝唇舌擦过我带伤的肩膀 吻过我滚烫的胸膛 你是梦中虚妄 你是无上理想 你是坠落时陡升的翅膀你是剑上仅剩的锋芒 与我深夜荒野数过星光你是海上滔天风浪 将我拽入漩涡中共航你是我的 只是我的爱与恨同党 2也看过人情聚散莺飞草长 爱如同春日将化的霜看似温暖却近消亡 苟延残喘抓住我盲目眼光那时天真且从来不自量追随着想象流浪 以为青春里所有都比一生要绵长 我亦曾自我厌恶跌进深谷空港 不屑去信爱可以依傍还好你 有足够温柔与倔强留待着时间为我圈刻国境四方我也有野心昭昭叫喊痴狂 等你领我餐风饮浪 你是恢弘诗章 你是星辉晴朗 你是海啸中鲸鱼的脊梁你是夜色将至时微光 让人甘愿披荆斩棘前往你是想拥抱的力量 呼吸声缠绕着颤动心脏你是我的 是我一生只一次的跌宕 3过片：用你傲慢将我捆绑 用你鞋尖划过我脸庞用你手指 用你声音沙哑的嗓蛊惑我走烈火焠烧天堂 再坠入结冰汪洋让我记住你给的痛的模样 再不遗忘 4我也有滥情状 我也有不可讲 我也有最讽刺回忆的墙我决绝到曾想把世上 所有爱字谏言全都烧光而你大概与我相仿 爱才能如此肆虐膨胀我是你的 只是你的爱与恨同党 你在终止彷徨 你在结束动荡 你成为垂衣驭八荒的王你低下头与我相望 将金色徽章戴在我肩膀你眼中分明蛰藏 锁死我危险却迷人的欲望一口吞下 世上没有得不到疯狂（完） 这首词是一次很完整的情感爆发了，从开始，到酝酿，到高潮，最后享受高潮。 （咳咳咳希望不要被举报……） 假如没有歌曲的烘托，歌词也需要从心而写。 意象的特点可以成为肌理，情感连续变化可以成为肌理，思考的过程可以成为肌理，甚至一条肌理不够就上两条肌理…… 种种的写法就是为了引人入胜，不一定深刻，但一定要刻骨铭心。这便是“盛开一支思路”了。 叁 万言不在纸端不好的词千篇一律，好词各有各的妙处。这也正说明了写词其实是靠天吃饭，有多高的灵感和天赋就能写出多好的词。 而天赋和灵感体现在作词上，便是敏感的感觉和敏锐的表达。 下面看这样的一首词： 《匆匆那年》作词：林夕1匆匆那年 我们究竟说了几遍 再见之後再拖延可惜谁有没有爱过 不是一场七情上面的雄辩匆匆那年 我们一时匆忙撂下 难以承受的诺言只有等别人兑现 不怪那吻痕 还没积累成茧拥抱著冬眠 也没能羽化再成仙不怪这一段情 没空反覆再排练是岁月宽容 恩赐反悔的时间 如果再见不能红著眼 是否还能红著脸就像那年匆促 刻下永远一起 那样美丽的谣言如果过去还值得眷恋 别太快冰释前嫌谁甘心就这样 彼此无挂也无牵我们要互相亏欠 要不然凭何怀缅 2匆匆那年 我们见过太少世面 只爱看同一张脸那麼莫名其妙 那麼讨人欢喜 闹起来又太讨厌相爱那年活该匆匆 因为我们不懂 顽固的诺言只是分手的前言 不怪那天太冷 泪滴水成冰春风也一样 没吹进凝固的照片不怪每一个人 没能完整爱一遍是岁月善意 落下残缺的悬念 如果再见不能红著眼 是否还能红著脸就像那年匆促 刻下永远一起 那样美丽的谣言如果过去还值得眷恋 别太快冰释前嫌谁甘心就这样 彼此无挂也无牵如果再见不能红著眼 是否还能红著脸就像那年匆促 刻下永远一起 那样美丽的谣言如果过去还值得眷恋 别太快冰释前嫌谁甘心就这样 彼此无挂也无牵我们要互相亏欠 我们要藕断丝连（完） 怎么样，是不是一下子被这首词抓住了？ 好像歌词没用力，但是怎么这么戳心！ 再看一首！过瘾！ 《好梦如旧》词：并瓦1只求当年七分才力 将你描摹无虞难现锦绣字句，折煞玲珑词笔不甘愿默认是我江郎才尽陈言勿去又何用闲人提醒越记得清晰，越难求神似搔首至发落 方有一句得检点旧书册 已入古人歌夜半深雪对坐 满面尘世烟火问你能读懂几回合 副歌：不捧出肺腑怎知心头血犹热既相逢不妨挑灯呵手照山河有些话道破一半忽又沉默听寒寺钟声请野佛从不在意消磨却恐惧被埋没谁拨开春草寻底下两道车辙曲早离了口那琴弦还颤着愿我们侥幸被记得谁能记得 2爱和占有间界限有多细瘦是否小过眉峰里藏墨暗钩霜雪吹满头 也算是白首昔在眼前时 万言尚未够而今分两地 一字也觉偷何来满腹闲愁 难觅一眼风流理什么浮名身后留 副歌：若长相守不过你拈花我把酒酒醒后能否赏我个好梦如旧你不先去怎知我相随在后红尘白雪世上一走从不在意消磨却恐惧被埋没谁拨开春草寻底下两道车辙曲早离了口那琴弦还颤着愿我们侥幸被记得谁能记得（完） 是不是感觉差不多？似乎没有什么惊人的语句，但是却有惊人而优雅的力量让人感动。 没错，这就是好词。 绝妙的中国歌词，往往给人一种精致、巧妙的感觉。别的都不说，单凭这一句 霜雪吹满头，也算是白首 ，这首词就是其他地方都一塌糊涂，也绝对称得上是神作了。 《匆匆那年》妙处（开始胡说）1匆匆那年 我们究竟说了几遍 再见之后再拖延 开篇，不用说清究竟说了什么，“再见之后再拖延”把一切都暗示了。一句毫无痕迹，将纠结反复体现纸端。 1不怪这一段情 没空反覆再排练 是岁月宽容 恩赐反悔的时间 不去怪情，不责怪分手的事，反而感谢岁月的恩赐，妙啊。这便是领悟了感情的真谛，却不用说明：那年匆匆，要是自己能在成熟一点就好了。 1如果再见不能红着眼 是否还能红着脸 这句绝妙，红着眼是纠缠和感动，红着脸或是成熟或是稚嫩，感情的两端都有些美好，但是唯独中间的纠缠来得快去得快，感情也正因这纠缠而破碎。 123如果过去还值得眷恋 别太快冰释前嫌谁甘心就这样 彼此无挂也无牵我们要互相亏欠 要不然凭何怀缅 要互相亏欠，要不然凭何怀缅。担心无挂无牵而失去重要的人和回忆。这其实是心有所念的人才拥有的独特情感。虽说纠缠怀念，但是再也回不去的那段感情鲜明起来。 123那么莫名其妙 那么讨人欢喜 闹起来又太讨厌相爱那年活该匆匆 因为我们不懂 顽固的诺言只是分手的前言 感情总是不能过热不能过冷，但是那年的我们怎么会懂得这个分寸呢？当然更看不出信誓旦旦背后的危险了。“活该”二字传神 整首词其实有很多妙处。但是最妙的地方就在以过来人的视角回首过去，点到而不呻吟，感慨而不沉溺，感人至深，但也不带一丝尘务俗气。 想表达的事情，歌词没必要事无巨细一一陈述，为感情铺好了路，就不必赘述。反复强调想表达的意思，反而会削弱感情的浓度。这时候反而从看似无关紧要的地方旁敲侧击，可能会有奇效。 想表达的事情不写出来，曲一笔，“欲说还休。欲说还休，却道天凉好个秋。” 这便是抒情的变招“万言不在纸端” 了。 p.s.写不出“互相亏欠”或者“霜雪满头”的宝贝们还是慎用吧，有一说一的词作也是非常可爱真诚的 肆 后记第一期结束啦 欢迎转发和留言讨论 其实还有些想说的,简单的歌词写法我从网上找了一篇简单填词教程 月更公众号（拖走）氦核原创:heheoriginal]]></content>
      <tags>
        <tag>填词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计计算课程笔记（一）]]></title>
    <url>%2F2018%2F03%2F26%2FR%E8%AF%AD%E8%A8%80-%E7%BB%9F%E8%AE%A1%E8%AE%A1%E7%AE%97%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[2018 SC学习笔记（雾） 第1节12345a = seq(1,100,length.out = 12)aa&lt;20a[a&gt;20]a[a=20] function-which返回查询值的位置坐标12which(a&gt;90)a[which(a&gt;90)] logic condition &amp; | !逻辑判断:与或非1234b = rep(c(&apos;M&apos;,&apos;F&apos;),6)a &gt;80 &amp; b ==&apos;M&apos;a[a &gt;80 &amp; b ==&apos;M&apos;]which(a &gt;80 &amp; b ==&apos;M&apos;) clean environment删除，虽然我从来没用过。。。1rm(list = ls()) square root开平方根以及一些常见的运算12345sqrt(5)sqrt(a)log(a)tan(a)a^2 1.1检查向量的长度12345678length(a)a + ba + 1k = a + c(1,2,3)k - aa/b # 向量也可以直接做除法 插一句脚本语言尽量使用向量化的书写，循环 的速度比较慢考虑兼容性，并且考虑运算速度，还是使用向量更快 1.2向量的数据类型function-class1234class(a)typeof(a)d = &apos;wyh&apos;class(d) 1.3操作矩阵12345678mat = matrix(c(22,31,17,38,16,7),3,2)dim(mat) # 维度class(mat)nrow(mat) # 几行ncol(mat) # 几列mat[,1:2]mat[1:2]mat[1:2,] 在矩阵的前两行中挑出小于35的数123456789mat[1:2,]&lt;35mat[mat[1:2,]&lt;35]mat[which(mat[1:2,]&lt;35)] # 经典错误，没有搞清坐标对应的对象which(mat[1:2,]&lt;35)mat[which(mat&lt;35)]mat[1,mat[1,]&lt;35] 下面三段代码效果一样12345mat[1:2,][mat[1:2,]&lt;35]mat[1:2,][which(mat[1:2,]&lt;35)]mat[which(mat[1:2,]&lt;35,arr.ind = TRUE)]# arr.ind 这个参数是改变了返回位置值的方式which(mat[1:2,]&lt;35,arr.ind = TRUE) # 返回了行列的坐标 合并矩阵为向量1c(mat[,1],mat[,2]) 下面就开始瞎写了行列互换1t(mat) #transports??? 方阵1ze = matrix(0,4,4) 对角阵1diag(5) function-seq序列函数12seq(0,0,length = 5) # 序列函数matrix(c(1,seq(0,0,length = 5)),5,5) 12345n = 5mat1 = matrix(0,n,n)mat1[seq(1,n*n,n+1)] = 1mat1matrix(rep(c(1,rep(0,n)),n),n,n) 矩阵乘法123456rnorm(35,5)mat2 = matrix(rnorm(35,5),5,7)mat1%*%mat2 # 矩阵乘法a = matrix(1:24,6)b= t(a)p = a%*%b 矩阵求逆，秩12345678solve(p) #求逆，必须是满秩的矩阵qr()$rank#求矩阵秩p = matrix(c(5,1,2,3),2)ppinv = solve(p)pinv%*%pp%*%pinv# 双精度默认16位之后均是零 求行列式12det(p) #determinant?det 求特征值1234ee = eigen(p) #eigenvalueee$valuesprod(ee$values)# 连乘product,正好等于行列式 第2节确定双精度范围内是否相等1all.equal(a,b) 按行填需要调整参数byrow=TRUE1matrix(1:25,5,5,byrow = TRUE) 三维数组array12ary = array(1:24,c(2,3,4)) # c(2,3,4)是维度ary[,1,2] # 最后一位是维度 数据框data.frame，存储不同类型数据123456789101112a = matrix(c(1,2,3,4),2,2)a[4] = &apos;feng&apos;aname = c(&apos;yang&apos;,&apos;crow&apos;,&apos;ruby&apos;,&apos;weiss&apos;)age = c(19,28,17,14)sal = c(2000,1800,5000,2000)data = data.frame(name,age,sal)datasex = c(&apos;f&apos;,&apos;m&apos;,&apos;f&apos;,&apos;f&apos;)cbind(data,sex) 转换数据类型，类型不合的克星1as.numeric(&apos;2&apos;) 树状图数据如何存储-&gt;可嵌套的列表1234567891011info = list() #先定义一个空列表是精髓hhhinfo$wang5 = data.frame(age = 19, work = &apos;yes&apos;)info$zhang3 = matrix(c(1,2,3,4),2,2)class(info)class(&apos;wang5&apos;)info$li4 = list(age = 28, edu = &apos;cufe&apos;, sal = &apos;180/min&apos;)info$li4info$li4$agelength(info)names(info) 给列表追加新元素123info$zhao6 = &apos;Neimenggu&apos; #直接加就行，不用初始化info[[1]][1,2] = 999info$zhang3[1,2] = 999 第3节 apply族函数lapply，rapply 函数初体验给列表的每个元素取对数（其实什么操作都可以有）12345lapply(info,log) # 划重点info[[&apos;zhang3&apos;]] = list(b1 = 10,b2 = 7)rapply(info,log) #可递归的（r）rapply(info,log,how = &apos;unlist&apos;) #参数how的值 默认unlist, 还有replace 把函数改成x平方1rapply(info, function(x) x^2,how = &apos;replace&apos;) deal with matrix：caculate the mean of col/row123mat = matrix(1:24,4,6)rowMeans(mat)# only for matcolMeans(mat) 或者你还可以使用apply函数123apply(mat, 2, mean) # 2 means dim2 第二维apply(mat, 2, median)apply(mat, 2, var) change to array背景是：4 groups, 6 members, and 5 terms1ary = array(1:120,c(4,6,5)) caculate mean of score every term12345apply(ary, 3, mean)apply(ary, c(1,3), mean) # every group&apos;s mean in each term 4*5apply(ary, c(1,3), max)maymaxmin = function(x) max(x)-min(x)apply(ary, c(1,3), maymaxmin) apply一般只作用于二维1234567ary2 = array(rnorm(120),c(4,5,6))apply(ary2, 3, mean)apply(ary2, 1, colMeans)apply(ary2, 2, median)apply(ary2, 1, sort)apply(ary2, 1, sum)?apply if it isn’t a matrix, you can use ‘as.matrix’lapply (list) and there, simplify2array; tapply,and convenience functions sweep and aggregate. 接着看lapply用于列表1234567lst = list()lst$a = 1:10lst$b = 4:6lst$c = 9:24lapply(lst, sum)lapply(lst, mean)lapply(lst, length) var is not suitable with matrixtoo much dimsuse as.numberic 12345myfun = function(x) (x - mean(x))^2apply(ary, 1, myfun)lst$d = list()lst$d$d1 = c(2,3,4,6)lst$d$d2 = c(3,8,9,1,23) 递归rapply123rapply(lst, sum)rapply(lst, sum, how = &apos;replace&apos;)lst2 = rep(60,10) list can’t do plus-minus-caculation，but unlist can1unlist(lst2) - unlist(lst$a) 多元mapply1234mapply(function(x,y) x-y, lst2, lst$a) # multiple lapplymapply(&apos;-&apos;, lst2, lst$a)# &apos;+&apos; ,&apos;-&apos; ,&apos;*&apos; ,&apos;/&apos; , all those symbols are functionsmapply(sum, lst2, lst$a) if list2 has a total same distruction as list3两列表间计算12345678list2 = list()list2$a = 3list2$b = c(4,4)list3 = list()list3$c = 5list3$d = c(6,7)mapply(&apos;+&apos;, list2, list3)mapply(function(x,y) x+y, list2,list3) 第4节change of data type准备工作123456789101112a = matrix(1:24,4,6)t(a)matrix(a,2,12)matrix(a,2,)matrix(t(a),,2)b = list()b$b1 = c(1:4)b$b2 = c(5:8)b$b3 = c(9:12)matrix(unlist(b),3,4)do.call(cbind,b) # like apply function compare123a = 5b = 6a == b 注意’= =’is not right12345a != ba &gt; ba &lt; ba &gt;= ba &lt;= b two to more123456A = c(1,3,5)B = c(3,2,5)A == BA &gt; BA &gt; 2A &gt; c(1,2,3,4) # auto repeat 1/4 time, but warning R中出现warning一定不能放过 change uppercase and lowercase123&apos;feng&apos; == &apos;Feng&apos;tolower(&apos;Feng&apos;) == &apos;feng&apos;toupper(&apos;feng&apos;) 更厉害的两数1all.equal(1,exp(3)/exp(3)) # 忽略浮点数的判断 问：有没有人不及格?12345rec = c(1,99,61,74)any(rec &lt; 60) # 至少一条满足 if anyall(rec &gt; 60) # 全部满足 if all&apos;zhang3&apos; %in% c(&apos;zhang3&apos;,&apos;li4&apos;,&apos;wang5&apos;) # if intolower(&apos;zhang3&apos;) %in% tolower(c(&apos;ZHANG3&apos;,&apos;LI4&apos;,&apos;WANG5&apos;)) condition testa = 10if (a &lt; 10){ print(a)}else{ print(a+100)} if (2018%%4 == 0){ print(‘yes’)}{print(‘no’)} if (2018%%4 == 0){ print(‘yes’)}else{ print(‘no’)} 建立函数take care of the position of ‘{}’ and ‘else’ 要注意 输入是否合法，容错纠正机制；逻辑一定要清楚，最后输出可以使用list和dataframe 问题：如何判断闰年 123456789101112131415161718192021222324year = c(1900:2018)if (year%%400 == 0 | year%%4==0 &amp; year%%100!=0)&#123; print()&#125;else &#123;&#125;print(year[which((year%%400 == 0 | year%%4==0 &amp; year%%100!=0)==&apos;TRUE&apos;)])isleapyear = function(year)&#123; if(!is.numeric(year)) # 这个判断很关键 &#123; stop(&apos;you must specify a numerical input.&apos;) # 容错纠正机制 &#125; index = year%%400 == 0 || year%%4==0 &amp;&amp; year%%100!=0 out = year[index] return(out) #也可以使用list,dataframe&#125;yr = c(1900:2018)isleapyear(yr) 练习applyfamily function1?apply applydeal with matrix &amp; same type1234mat = matrix(1:24,4,6)ary = array(1:120,c(4,6,5))apply(mat, 2, mean) # means of second dimapply(ary, 3, mean) lapplylist1234567lst = list()lst$a = 1:10lst$b = 4:6lst$c = 9:24lapply(lst, sum) # deal with listlapply(lst, length)sapply(lst, sum) # output is vector rapply123rapply(lst, sum)rapply(lst, length)rapply(lst, sum, how = &apos;replace&apos;) tapplyirregular type 分类统计1234?tapplyfac &lt;- factor(rep_len(1:3, 17), levels = 1:5)table(fac)tapply(1:17, fac, sum) tapply(x,f,g) :x为向量,f为因子列,g为操作函数1234a = c(1:10)b = c(5:14)data = data.frame(a,b)tapply(data$a, data$b, sum) mapply对多个列表或者向量参数使用函数1234lst2 = rep(60,10) # list can&apos;t do plus-minus-caculationunlist(lst2) - unlist(lst$a) # but unlist canmapply(function(x,y) x-y, lst2, lst$a)mapply(sum, lst2, lst$a) 2.my function12345678910111213141516171819a = rnorm(100)summary(a)mysummary = function(lst)&#123; if(!is.numeric(lst)) &#123; stop(&apos;you must specify a numerical input.&apos;) # 容错纠正机制 &#125; min = min(lst) median = median(lst) mean = mean(lst) max = max(lst) var = var(lst) out = data.frame(min, median, mean, max, var) return(out)&#125;mysummary(a) 3.解方程法112345f &lt;- function(x,a,b,c) a*x^2+b*x+ca &lt;- 1; b &lt;- 5; c &lt;- 6delta = b^2-4*a*csolve1 = (-b + (b^2-4*a*c)^(1/2))/(2*a)solve2 = (-b - (b^2-4*a*c)^(1/2))/(2*a) 写出关键的地方12345678910111213test = function(a,b,c)&#123; delta = b^2-4*a*cif (delta == 0)&#123; return(&apos;只有一个根&apos;)&#125;else if(delta &lt; 0 ) &#123; return(&apos;有两个虚数解&apos;)&#125;else &#123; solve1 = (-b + (b^2-4*a*c)^(1/2))/(2*a) solve2 = (-b - (b^2-4*a*c)^(1/2))/(2*a) return(data.frame(&apos;有两个实根&apos;,solve1,solve2))&#125;&#125;test(a,b,c) 法2求一元二次方程ax^2+bx+c=0,设a=1,b=5,c=6,求x?123f3 &lt;- function(x,a,b,c) a*x^2+b*x+ca &lt;- 1; b &lt;- 5; c &lt;- 6result1 &lt;- uniroot(f3,c(0,-2),a=a,b=b,c=c,tol=0.0001) 用 uniroot 求解单个根123result2 &lt;- uniroot(f3,c(-4,-3),a=a,b=b,c=c,tol=0.0001)result1$rootresult2$root 最后再来看一下简单的函数函数mysummary12345678910111213141516171819a = rnorm(100)summary(a)mysummary = function(lst)&#123; if(!is.numeric(lst)) &#123; stop(&apos;you must specify a numerical input.&apos;) # 容错纠正机制 &#125; min = min(lst) median = median(lst) mean = mean(lst) max = max(lst) var = var(lst) out = data.frame(min, median, mean, max, var) return(out)&#125;mysummary(a) 解方程123456789101112131415161718192021222324252627282930313233343536373839404142434445#先输入三个向量vec1 = c(1,2,1)vec2 = c(2,3,2)vec3 = c(1,2,1)test = function(vec1,vec2,vec3)&#123; m = length(vec1) #将填入三个维度&apos;num&apos;,&apos;condition&apos;,&apos;value&apos; out = list() #循环遍历 for(i in 1:m)&#123; a = vec1[i] b = vec2[i] c = vec3[i] #检验输入数据是否合法 if(is.numeric(a)==FALSE|is.numeric(b)==FALSE|is.numeric(c)==FALSE)&#123; return(&apos;请输入数值型向量&apos;) &#125;else if(length(a)!=length(b)|length(a)!=length(c)|length(b)!=length(c))&#123; return(&apos;请输入相同长度的向量&apos;) &#125; #求根 delta = b^2-4*a*c if (delta == 0)&#123; out$number[i] = i out$condition[i] = &apos;只有一个根&apos; out$value[i] = -2*a/b &#125;else if(delta &lt; 0 ) &#123; out$number[i] = i out$condition[i] = &apos;有两个虚数解&apos; out$value[i] = &apos;无实数解&apos; &#125;else &#123; solve1 = (-b + (b^2-4*a*c)^(1/2))/(2*a) solve2 = (-b - (b^2-4*a*c)^(1/2))/(2*a) out$number[i] = i out$condition[i] = &apos;有两个根&apos; out$value[i] = data.frame(solve1,solve2) &#125; &#125; return(data.frame(out$num,out$condition,out$value))&#125;#实验检验test = test(vec1,vec2,vec3) 感谢大家，欢迎在评论区交流讨论！]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[突破自己，人生之赌]]></title>
    <url>%2F2018%2F03%2F07%2F3.4%E4%BA%BA%E7%94%9F%E4%B9%8B%E8%B5%8C%2F</url>
    <content type="text"><![CDATA[又是一个深夜，距离我上一次敏感地爆炸过去了将近一年了。之前因为对自我定位的怀疑而崩坏的内心，在现在并没有因为时间推移而完全抹去伤痕，反倒是更刻骨铭心了。 很久以前，在我小时候，曾经听说过“读书无用”诸如此类的论断。当时读书对于我来说，功课成绩优异能让我获得更多的目光和赞许，极大的满足了我生活的意义；闲书读的多了，满载冷知识的大脑也显得犹为活跃。这让我在一段时间里显得又会玩，又会学，简直就是一个学霸。 然而我并不是那样的学霸。后劲在屡屡受挫和毫无成就的学习面前，还是慢慢消逝了。虽然偶尔还是能够有些许进步，但也是少数情况。当时英语成绩在班里也算是倒数前五，英语老师都已经放弃我了，亲口对我说“你要是现在开始努力，高考还是能赶上的”这样的话。 直到高考我也没什么特别大的进步，每次考试总会在心里演一出戏给自己。高考很快就到了，破釜沉舟，我记得很清楚，自己在考场上，穿着不合时宜的长袖外套，最后竟在考场喷了鼻血。监考老师呆呆地看着第一排的我，我很淡定地用纸塞住鼻子，接着答卷。那时的我仿佛进入了模式，一个只有自己的模式，我连自己最熟悉的“内心活动”都忘记了。最后英语考试考了很高的分数。高到自己都惊呼卧槽。 我逐渐发现，人能成就某些东西，不是非得跳出自己的视角，用更高远的眼光去分析问题。这种方法固然有用，但是难以成就精深的业绩。真正厉害的武器，是我们一直很讨厌的定式思维，也叫做笃信。这种品质，是让自己试着沉浸在一件事或者一种情绪中甚至于无法自拔的状态。 人常说，不能钻牛角尖。但事实却不如此。钻死一处牛角尖，如果认定的事对了就能成功，错了就身败名裂。人生不是概率论，我们面对的只有0和1两种可能。因此与其徘徊不定，不如全部压在一边。 赌。信。 赢者继续前进，输者就输掉一切。这是个单纯的赔本买卖。可很不幸的是，人生除了赌，剩下的就是毒，一种慢性的毒。随着经历丰富，见识增广，这毒让人变得澄澈聪慧，最终看透一切，成了不可多得的聪明人。而聪明人在这社会上是最没用的，是不可能成功的。这个世界只属于占了大多数的勤奋的笨蛋。 我就是个笨蛋。我画过画，一塌糊涂。我弹过吉他，现在唱歌还经常跑调；我写过小说，节奏混乱不成文体……我做过无数半吊子的事。但那些我坚持下来的事，一定都有所进步。我自学吉他，别人三月能通，我花了四年自己摸索；我琢磨填词，幼稚，到搞笑，到调侃，到辛辣，爱恨情仇，风格一变再变，不知道被人劝退了多少回，可是我走了下来，总算收到了一些认可；我下棋，曾经多少年的小棋手，现在也是脑子不够用的大老爷们儿了，棋力虽然不长反退，但是总觉得自己也似乎明白所谓“两处有情方可断”了。 读书太少，让我们根本意识不到读书无用。读书越多，越觉得读书无用。读精一本书，胜过涉猎百本书。做人也一样，并非事事都要追求完美，事事都要坚持不懈努力奋斗，以我们普通人的水平也很难苛求完美。但是在需要完美的地方，绝不能含糊，就算知道自己事倍功半，也要拼尽全力。我相信自己，这份相信才是突破自己关键的钥匙。 今天中午吃饭，打饭阿姨不小心给我打了半份鱼。我是吃过鱼的，可以自豪的说我以前吃过鱼。但是技术很差，一条鱼能吃一年，而且还八成会被刺卡到。我还是试了试这条鱼，鱼做的很好，可惜我花了太多的时间和精力去处理它。最终我放弃了剩下的这大半条鱼，索性倒掉，不要它再出现在我眼里。 吃鱼这等美事，交给会享受的人来做吧。空手搏狮，还是要交给狮子。 希望大家都能得到神的祝福。恍然大悟有两种，我没有觉得自己越悟越聪明，也许更多是越悟越傻，但是越悟越真。 （完）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】R + Markdown简介]]></title>
    <url>%2F2018%2F03%2F04%2FR%E8%AF%AD%E8%A8%80%20%2B%20Markdown%2F</url>
    <content type="text"><![CDATA[简介r语言和markdown的使用（功能和前期准备） 简介markdown是什么markdown是由John Gruber提出的一种书写格式，于2004年12月推出它的1.0.1版本。到现在为止这个版本也一直没有更新过，也就是说这是一个基本稳定的版本。作者的博客地址在这里，其中包含了Markdown的语法说明，这个是英文版的。在说明markdown是什么之前，先看一个markdown文件的实例： 灵根育孕源流出 心性修持大道生诗曰 混沌未分天地乱，茫茫渺渺无人见。自从盘古破鸿蒙，开辟从兹清浊辨。覆载群生仰至仁，发明万物皆成善。欲知造化会元功，须看《西游释厄传》。 盖闻天地之数，有十二万九千六百岁为一元。将一元分为十二会，乃子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥之十二支也。每会该一万八百岁。且就一日而论：子时得阳气而丑则鸡鸣，寅不通光而卯则日出，辰时食后而巳则挨排， 由此可见，markdown的源码和最终输出的结果比起来，除了格式不一样外，只是多出了一些 标点符号。而这些标点符号就是markdown控制格式的方法。 如果说markdown有什么特征，那么markdown最大的特征就是简单。与微软的word相比，不知简单了多少倍，简直不是一个数量级的。标准的markdown中只是定义了10多种格式，并且格式的定义非常容易记忆，以至于你根本不需要化专门的时间来学习它。 那么有人会问，它会不会太简单了？的确有时候它太简单了，比如写作一些科技文献的时候，它提供的基本格式就不够用了。在这种情况下，出现了markdown的几种扩展，可以让人们来方便的处理这些格式。目前比较流行的扩展有pandoc扩展，其次是GitHub扩展。其中GitHub的扩展主要是用于在GutHub的网站上使用。 由于GitHub用户很多，所以这个扩展也很流行。而 pandoc 是一个强大的格式转换工具，基本上支持各类常见格式之间的转换，其中就包括有markdown。而markdown的格式超级简单，这就使得人们能够用超级简单的markdown来生成看起来复杂的其他格式，比如word、html、pdf等。这样，因为pandoc使得人们能够使用markdown来生成doc和pdf文档，因此受到广泛的欢迎。随着pandoc的流行，出现了很多以pandoc为基础的外围工具，以此来方便人们对pandoc的使用，比如在R语言中的几个软件包 Rmarkdown，knitr 等就使用pandoc作为底层的支持来实现从markdown到各种其他格式的转换。 另外关于这些基于pandoc的扩展，有些已经实现的非常先进，可以帮助作者完成不少以前需要手动完成的工作，比如自动生成文档的部分内容，自动插入图片等等。 基于以上这些原因，本文中也将以pandoc扩展为主要的关注点。 R语言本文的目的是为了讲解markdown，为什么还要提到R语言？R语言是什么东西，跟markdown有什么关系呢？ 如果你使用markdown只是进行文学类创作，比如写写小说什么的，那么你根本不需要使用到markdown的扩展就可以工作了。在这种情况下，你可以不去考虑makrdown扩展以及R的问题。 如果你要使用markdown进行科技文献，科技书籍的写作，那么如果你了解并能够使用R语言，将会对你的工作效率有巨大的提升。 那么R语言是什么？R语言是一种针对统计分析和数据科学的开源的统计语言，它是一种基于向量的高级函数式编程语言。近年来，由于大数据的兴起，越来越多的人开始关注R语言。每天都有大量的志愿者为R软件的发展贡献代码。因此R本身也是一个迅速发展的语言。 由于pandoc的兴起，R社区的开发者发现了这个工具给写作带来的巨大便利，因此开发了一些以pandoc为基础的软件包，其代表是Rmarkdown和knitr。这些软件包让文档或书籍的作者可以方便的从markdown文件动态的生成最终的文档。这些工具使得写作的效率大幅提高。这也是我们将要关注的内容。 rmarkdown和knitr前面介绍提到了rmarkdown和knitr是R的软件包，它们把能够在markdown中嵌入R语言，并且执行其中的R代码，而R代码的输出可以作为文档的一部分。甚至R代码生成的图片也可以自动嵌入到markdown文件中。 Rmarkdown + knitr可以执行R语言代码的这个功能使得markdown如虎添翼，具有非凡的表现力和极高的效率。含有可执行R代码的markdown文件有一个专门的扩展名，即rmd。下面我们看一个rmarkdown的实例： 下面的代码将会画出100个正态分布的散点图，画出的图会变成文件的插图： ` {.r echo=FALSE,fig.path=”./rfigures/p6172-“}plot(rnorm(100))``` 这里可以看到新生成的图片已经被插入文档中了，这样每次重新生成文档，文档的内容都会被更新，这样只要文档中的R代码没有错，该文档将永远保持使用最新的结果，你不需要为每次的代码改动来重新粘贴代码生成的结果。这将为您节省宝贵的时间。 为什么要使用markdown从上面的介绍可以看出，markdown最大的特征就是简单，其次是基于markdown的各种外围工具使得利用markdown进行创作非常高效。这个高效体现在 语法简单，作者基本上无需关心排版的问题，只要专心写作就可以了。 对于科技文献的写作，R软件提供了文章动态生成的过程。使得作者不必要手动去拷贝粘贴代码的结果或者生成的图片等。 易于修改，因为文档可以从markdown动态的生成。作者只需要修改markdown文件，然后就立刻能够看到最终的文章结果。 还有一个使用markdown的重要原因是，它是免费的，markdown只是一种格式定义，你可以使用你喜欢的编辑器来编写markdown文件，编写完的markdown文件可以通过免费的工具来转换成html、pdf、docx等。因此使用markdown你不必要花钱去使用某软的付费系统和付费软件。 怎样使用markdownmarkdown文件是通常的文本文件，你使用vi，emacs或者windows下的记事本都可以编写markdown。写好的markdown如果上传到GitHub，那么markdown文件会自动被转换成html显示出来。在本地可以通过perl，R或者其他的工具比如pandoc来将markdown文件转为html或PDF以及docx。 • perl，在John Gruber的网站，提供了一个Markdown.pl，这是标准的markdown解析工具，它可以把markdown转为html 使用方法是： ./Markdown.pl ./about.md 这样将会把md文件转换为html，输出在标准输出。 因为前面提到的，标准的markdown的功能有限，比如没有表格的支持等，所以一般使用中，不推荐直接使用这个perl脚本进行格式转换。 • pandoc，你也可以使用pandoc直接来将md文件转换为html 使用方法： pandoc -f markdown -t html -o ./about.html ./about.md pandoc也可以把md转换为pdf文件，使用方法为 pandoc -f markdown -t latex -o ./about.pdf ./about.md pandoc可以支持多种类型的转换，通常你需要熟悉pandoc的各种参数，你可以手动敲这些命令。不过使用R的包会使这个过程简化。 • R ，以下是利用R将当前目录下的about.md文件转换为about.html文档的方法：Rscript -e “rmarkdown::render(‘./about.md’)” 当然，为了上面的代码能够正确运行，你需要在安装R以后，安装rmarkdown这个包，这个包的安装方法是，在R的命令行键入 install.packages(&quot;rmarkdown&quot;)。 如果要生成pdf格式的文件，你需要在render后面指定更多的参数。这些后面再讨论。 那么rmarkdown的文件怎么编译呢，rmarkdown的文件，也就是.rmd文件就只能在R上编译（转换）了。编译的方法和编译markdown文件一样，只是这时候，文件的扩展名由md变成了rmd，这时候，rmd文件中的r代码就会被处理了，命令行如下： Rscript -e &quot;rmarkdown::render(&#39;./about.rmd&#39;)&quot; 在真正使用markdown进行创作之前，你还需要做一些准备工作。如果你比较着急的想试试，可以直接跳到后面一章去看看markdown的基础语法。 准备工作在真正使用markdown之前，我们还需要一些准备工作。这里的准备工作主要包括两个部分，一是选择一个编辑器，再就是选择一个编译工具，我们这里选择R语言。当然你也可以选择perl,但是我不推荐。 编辑器Linux下的同学都应该有自己趁手的工具，vim或者emacs随便找一个就可以了。这些工具下面应该还有些支持markdown语法的插件。对于vim来说，有一款插件叫 vim-markdown。这个插件只是提供了语法的高亮显示。 在Linux下使用VIM编辑markdown文件实际上是很不方便的，根本原因在于中文的切换问题。你在输入中文的时候，如果要键入一个vim的命令，需要退出中文输入，或者在输入法上输入命令然后直接键入回车键，这是很不方便的。因此，我实际的使用过程中，并不怎么使用vim。而是使用了另一款编辑器，即gedit。这个编辑器中集成了markdown语法的高亮显示。使用起来也比较直接。中文输入的不方便是我这个vim粉放弃vim的根本原因。不过除了markdown以外的其他的编辑基本在vim中进行。 另外现在有一些可以实时看到编辑效果的编辑器，比如Windows下的MarkdownPad, Mac下的Mou，Linux下的ReText等编辑器，大家也可以自己去找一下。选择一个适合自己的编辑器就好了。 pandoc如前所述，pandoc是一个格式转换工具，可以支持常见文档格式之间的相互转换。它支持很多的格式，使用pandoc —help可以看到它所支持的所有格式，这里列出其支持的格式供参考： xuyang@ubuntu15:~/blog$ pandoc —help pandoc [OPTIONS] [FILES] Input formats: docbook, docx, epub, haddock, html, json, latex, markdown, markdown_github, markdown_mmd, markdown_phpextra, markdown_strict, mediawiki, native, opml, org, rst, t2t,textile, twiki Output formats: asciidoc, beamer, context, docbook, docx, dokuwiki, dzslides, epub, epub3, fb2, haddock, html, html5, icml, json, latex, man, markdown, markdown_github, markdown_mmd, markdown_phpextra, markdown_strict, mediawiki, native, odt, opendocument, opml, org, pdf*, plain, revealjs, rst, rtf, s5, slideous, slidy, texinfo, textile [*for pdf output, use latex or beamer and -o FILENAME.pdf] 另外，pandoc还支持很多变成语言的语法高亮显示。 xuyang@ubuntu15:~/blog$ pandoc —version pandoc 1.13.2.1 Compiled with texmath 0.8.2.2, highlighting-kate 0.5.12. Syntax highlighting is supported for the following languages: abc, actionscript, ada, agda, apache, asn1, asp, awk, bash, bibtex, boo, c, changelog, clojure, cmake, coffee, coldfusion, commonlisp, cpp, cs, css, curry, d, diff, djangotemplate, dockerfile, dot, doxygen, doxygenlua, dtd, eiffel, email, erlang, fasm, fortran, fsharp, gcc, glsl, gnuassembler, go, haskell, haxe, html, idris, ini, isocpp, java, javadoc, javascript, json, jsp, julia, latex, lex, lilypond, literatecurry, literatehaskell, lua, m4, makefile, mandoc, markdown, mathematica, matlab, maxima, mediawiki, metafont, mips, modelines, modula2, modula3, monobasic, nasm, noweb, objectivec, objectivecpp, ocaml, octave, opencl, pascal, perl, php, pike, postscript, prolog, pure, python, r, relaxng, relaxngcompact, rest, rhtml, roff, ruby, rust, scala, scheme, sci, sed, sgml, sql, sqlmysql, sqlpostgresql, tcl, tcsh, texinfo, verilog, vhdl, xml, xorg, xslt, xul, yacc, yaml, zsh Default user data directory: /home/xuyang/.pandoc Copyright (C) 2006-2014 John MacFarlane Web: http://johnmacfarlane.net/pandoc This is free software; see the source for copying conditions. There is no warranty, not even for merchantability or fitness for a particular purpose. 关于pandoc的安装：• ubuntu 15.10 sudo apt-get install pandoc • ubuntu 15.10以前的版本 在15.10以前的版本，ubuntu缺省安装的pandoc版本太低，以至于Rmarkdown拒绝调用它。所以在以前的版本上，你需要自己安装pandoc的最新版本，安装方法是： sudo apt-get install cabal-install cabal update cabal install pandoc #记得把这个添加到目录 export PATH=$HOME/.cabal/bin:$PATH 使用这个方法安装时，需要花很长的时间，估计要1个小时吧。主要原因是需要下载大量的haskell的包。所以推荐大家在ubuntu 15.10以后的平台上使用。 另外，在用上面的方法安装pandoc的时候，如果使用国内的镜像，速度会快很多，国内镜像的使用方法可以参考清华镜像的使用方法1，这里把它记录下来供参考： 第一次使用时，先执行cabal update，待生成~/.cabal/config之后，使用ctrl+c中断这个update，然后修改~/.cabal/config。修改方法为，将下面这行注释掉（用—注释） remote-repo: hackage.haskell.org:http://hackage.haskell.org/packages/archive 修改为： remote-repo: mirrors.tuna.tsinghua.edu.cn:http://mirrors.tuna.tsinghua.edu.cn/hackage 然后再执行cabal update和cabal install pandoc • 其他的系统 暂时还没有尝试 texlivetexlive用来处理Latex，它把latex格式的文件转为pdf。如果需要从markdown生成pdf文件，或者是打算由多篇文章生成一本pdf格式的书籍，需要有texlive的支持。这里选择的是texlive，安装方法如下 sudo apt-get install texlive-full 这个安装也需要大约1小时左右，因为texlive也有很多的包需要安装。这样安装会多安装许多语言包，不过比起自己去一个一个选择包来说，这是最简单的方式。 另外，因为我们需要中文的支持，所以需要另一个latex引擎，xelatex，所以这个也需要安装，方法是 sudo apt-get install xelatex R安装• ubuntu 15.10直接安装就可以了 sudo apt-get install r-base rbase-dev 安装成功以后，你应该可以直接键入R命令，如下所示： xuyang@ubuntu15:~/blog$ R R version 3.2.2 (2015-08-14) — “Fire Safety”Copyright (C) 2015 The R Foundation for Statistical ComputingPlatform: x86_64-pc-linux-gnu (64-bit) R是自由软件，不带任何担保。在某些条件下你可以将其自由散布。用’license()’或’licence()’来看散布的详细条件。R是个合作计划，有许多人为之做出了贡献.用’contributors()’来看合作者的详细情况用’citation()’会告诉你如何在出版物中正确地引用R或R程序包。 用’demo()’来看一些示范程序，用’help()’来阅读在线帮助文件，或用’help.start()’通过HTML浏览器来看帮助文件。 用’q()’退出R. R包安装R安装以后，还需要安装一个R的扩展包才可以工作，其中的rmarkdown,knitr是必须的。安装的方法为，在R的命令行键入 install.packages(c(&#39;rmarkdown&#39;,&#39;knitr&#39;),repos=&quot;http://mirror.bjtu.edu.cn/cran/&quot;) 其中repos参数指定包的下载地址，这里指定的是国内的一个R镜像。 准备就绪有了以上准备工作（pandoc，texlive，R，rmarkdown）以后，你就可以开始r + markdown的工作了。后面开始学习markdown的基础语法。 https://mirrors4.tuna.tsinghua.edu.cn/help/hackage/ 本文地址: http://www.bagualu.net/wordpress/archives/6172 转载请注明]]></content>
  </entry>
  <entry>
    <title><![CDATA[委屈西红柿]]></title>
    <url>%2F2018%2F02%2F24%2F%E6%81%B6%E9%AD%94%E8%A5%BF%E7%BA%A2%E6%9F%BF%2F</url>
    <content type="text"><![CDATA[坐在电脑前，想写点什么。其实也没什么说的，但又总想说点什么，不说话憋得慌。当代青年是要发声的，不发声就没有“当代”的感觉。 我曾经不是这样的。当我还是个小孩子的时候，我是不喜欢说话的人。作为家里没有发言权的弱势群体，小孩子在各种需要意见的场合通常是沉默的。这沉默通常有好有坏：好在小孩子从此得以成为明眼人，把大人看不懂的事看明白并立刻想到和自己的关联；坏就坏在，小孩子在家里往往无奈屈服于家长们看似讲理的“道理”。 我小时候不懂这些。只知道我很讨厌吃炒西红柿，而我爸却很反感我挑食的习惯。倒不是说我爸做的有多难吃，现在吃起来那炒西红柿也是手艺高超，但是小时候却对西红柿丝毫没有感觉。如果饭菜中有西红柿的身影，我必定会食欲大减。倘若再让西红柿连同汤汁浇在米饭上，那顿饭恐怕直接就饱了。 作为土生土长的陕西人，我也思考过这个问题，我把原因归咎于西红柿的口味。老陕秉承“无辣不欢”的理念，一日三餐都泡在辣子中，而西红柿酸酸甜甜，两种香味完全没有干系，于是接受了正统关中辣食教育的我大概不喜欢这个蔬菜品种。我心想，长大大概就能接受了。 虽然还是抵触着，但是西红柿其中的营养也是可以被其他品种蔬菜水果取代的。初中时中午在学校食堂吃饭，我有了伙食决定权。从此，但凡是在学校吃的午餐，我便再未吃过西红柿。就连户县软面和三合一刀削面我都不再涉足，更别提西红柿盖浇面了。其实对于我这个面客来说很是折磨。西红柿不是圣女果，连同圣女果一起都是恶魔之果，有一段时间我在心里暗暗想到。 但是在家就不那么轻松了，家里隔三差五还是会炒西红柿。家里吃饭因为人少，流行“包干制”，每样菜不做多，够一家人吃刚刚好。因此吃饭要求就很严格，不能逮着一道菜猛吃，也最好不能剩菜。我爸依然把西红柿奉为上品，天天在我耳边念叨的就是“西红柿多有营养啊”之类，当然，我也斗不过他，只好吃掉。 那时候我还是个初中少年，能做的唯一的抵抗就是在舀西红柿炒鸡蛋时注意西红柿和鸡蛋的比例，并一定要将西红柿放在一顿饭的最后时刻吃。当我在最饿的时候，吃了最喜欢的菜，最后用西红柿拌饭来填饱肚子。当然，我当时还有一套严密的定义：为了享受美食叫吃饭，为了填饱肚子叫进食。所以我对西红柿的摄取是生命活动的需求，是进食需要，这样想来我的心理负担就没那么重了。 在高中时，我的伙食就比较轻松了，在学校食堂依然从来不点西红柿，回家我爸也由着我，基本不会做西红柿这道菜。偶尔凉拌一个西红柿蘸糖，我吃着还是很带劲的。仿佛忘了以前对西红柿的抵触。到了大学依然如此，回家以后总是去爷爷奶奶姥姥姥爷家蹭饭，吃家里做的饭就更少了，于是我也不再担心吃西红柿的问题，偶尔遇见了，也总有其他菜可以代替。 今年过年前，有个比赛让我到太原一个同学家去短住，同学家人待我非常热情，每顿饭都盛情款待，太原美食也吃了无数。不过印象深刻的一顿饭里，阿姨做了道西红柿炒鸡蛋，见我迟迟没动筷子，向我推荐了这道菜。我便往碗里舀了满满一勺，吃起来香甜可口。回家以后我习惯性地没有主动吃西红柿炒鸡蛋，但是却对讨厌吃西红柿的日子变得怀念了。可能我那时抵触的不仅仅是炒西红柿，抵触的是一种看不见的什么吧。这可能就是我为数不多可笑又可爱的叛逆时刻了。 （完）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[R语言与数据挖掘：公式；数据；方法]]></title>
    <url>%2F2018%2F02%2F08%2FR%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[简介r语言的代码 R语言特征对大小写敏感通常，数字，字母，. 和 _都是允许的(在一些国家还包括重音字母)。不过，一个命名必须以 . 或者字母开头，并且如果以 . 开头，第二个字符不允许是数字。基本命令要么是表达式（expressions）要么就是 赋值（assignments）。命令可以被 (\;)隔开，或者另起一行。基本命令可以通过大括弧({和}) 放在一起构成一个复合表达式（compound expression）。一行中，从井号(#)开始到句子收尾之间的语句就是是注释。R是动态类型、强类型的语言。R的基本数据类型有数值型（numeric）、字符型（character）、复数型（complex）和逻辑型（logical），对象类型有向量、因子、数组、矩阵、数据框、列表、时间序列。 基础指令程序辅助性操作：运行q()——退出R程序tab——自动补全ctrl+L——清空consoleESC——中断当前计算 调试查错browser() 和 debug()——设置断点进行，运行到此可以进行浏览查看（具体调试看browser（）帮助文档（c,n,Q））stop(‘your message here.’)——输入参数不正确时，停止程序执行cat（）——查看变量？ 帮助help(solve) 和 ?solve 等同??solve——检索所有与solve相关的信息help(“[[“) 对于特殊含义字符，加上双引号或者单引号变成字符串，也适用于有语法涵义的关键字 if，for 和 functionhelp(package=”rpart”)——查看某个包help.start()——得到html格式帮助help.search()——允许以任何方式（话题）搜索帮助文档example（topic）——查看某个帮助主题示例apropos（”keyword”）——查找关键词keyword相关的函数RSiteSearch(“onlinekey”， restrict=fuction)——用来搜索邮件列表文档、R手册和R帮助页面中的关键词或短语（互联网）RSiteSearch(‘neural networks’) 准备文件目录设置setwd（）——设置工作文件目录getwd（）——获取当前工作文件目录list.files()——查看当前文件目录中的文件 加载资源search()——通过search()函数，可以查看到R启动时默认加载7个核心包。基础函数：数学计算函数，统计计算函数，日期函数，包加载函数，数据处理函数，函数操作函数，图形设备函数 setRepositpries（）——选择软件库（CRAN，Bioconductor，R-Forge），寻找安装包的方法另看《【R笔记】寻找R的安装包》(.packages())——列出当前包(.packages(all.available=TRUE))——列出有效包install.packages（“”）——安装包library（）和require（）——加载R包（package）至工作空间 data（）——列出可以被获取到的存在的数据集(base包的数据集)data（，package=“nls”）——将nls包的datasets加载到数据库中 批处理文件和结果重定向source(“commands.R”)——执行commands.R （存放批处理命令的）脚本文件。cat(,file=””)——可以把R命令输出至外部文件，然后调用source函数进行批处理 do.call(，)——调用函数，第一个参数指示调用函数字符串名称，第二个参数包含调用所需参数的一个列表sink(“record.lis”)——把后续的输出结果从控制台重定向到外部文件 record.lis 中sink（）——把后续代码输出重新恢复到终端上展示 attach（）——将数据框中的变量链接到内存中，便于数据调用detach()——对应attach()，取消变量的链接，detach()里没有参数！_注：_ attach()和detach()均是在默认变量搜索路径表中由前向后找到第一个符合变量名称，因此之前若存在重名变量，有可能会出现问题！！！ 数据处理输入输出（读入输出数据、文件）assign(“x”,c(1,2,3)) 和 x &lt;- c(1,2,3) 和 c(1,2,3)-&gt;x ——向量赋值 read.table（”infantry.txt”, sep=”\t”， header=TRUE）——seq属性用其它字符分割，比如文本文件用空格（tab）分隔，header设置为文件中已经存在表头名称read.csv(“targets.csv”)——读入csv（Comma Seperated Values）文件，属性被逗号分割read.csv(url(““))——read.csv() 和 url()的合体，读存在网上的数据 x &lt;- scan(file=””)——手动输入数据，同时scan可以指定输入变量的数据类型，适合大数据文件scan(“data.dat”, what = list(“”, 0, 0))——what指定变量类型列表readLines(‘http:……’,n=10)——读取文本文件，将文档转为以行为单位存放的list格式，比如读取读取wikipedia的主页html文件的前十行 write.table(Data, file=”file.txt”, row.names = FALSE, quote=FALSE)——输出，quote为FALSE去掉字符串类型的双引号，write.table(stasum, “stasum.csv”,row.names = FALSE,col.name=FALSE,sep=”,”,append=TRUE)write.csv（data，file=”foo.csv”,row.names=FALSE）——写成csv格式,row.names=FALSE去掉行号 print（）——打印save.image（”./data.RData”）——把原本在计算机内存中（工作空间）活动的数据转存到硬盘中。load(“./RData”)——加载目录中的*.RData，把文档-词项矩阵从磁盘加载到内存中 数据查看通用对象R是一种基于对象（Object）的语言，对象具有很多属性（Attribute），其中一种重要的属性就是类（Class），最基本的类包括了数值（numeric）、逻辑（logical）、字符（character）、列表（list），符合类包括矩阵（matrix）、数组（array）、因子（factor）、数据框（dataframe）。 class() 和 data.class(object)——查看对象object的类或类型unclass()——消除对象object的类 基本数据类型mode()——查看基本数据类型length()——查看长度as.&lt;数据类型&gt;——改变对象的数据类型 特殊属性attributes()——查看对象object各种属性组成的列表attr(，“name”)——存取对象object的名为name的属性 混合类型逻辑类型+数值类型=数值类型逻辑类型+字符类型=字符类型数值类型+字符类型=字符类型 ls() 和 objects（）——查看当前工作空间中存在的对象（变量）rm(list=ls())——删除工作空间的所有对象methods(x)——查看x函数的源码，有些自带函数输入名称x可以直接看到，有一些需要调用methods方法才能查看函数x的源码，出现多重名，输入对应名称即可 str（）——查看数据（框）中的数据总体信息（比如样本个数、变量个数、属性变量名称、类型）nrow(dataframe)——查看数据集行数NROW（vector）——查看向量的行数，等于length(x)head(dataframe)——查看数据集前6行数据tail(dataframe)——查看数据集尾6行数据 向量特征逻辑向量运算：TRUE，FALSE——全部大写isTRUE(x)——判断x为TRUE*|，&amp;，！——或且非，注意是单个，不是&amp;&amp;！ANY，ALL——任意，全部 数组和矩阵train$vartrain$new[train$var == NA] &lt;- 1Data[is.na(Data)] &lt;- 0——数据框多维变量中给NA值赋值为0 apply（A，Margin，FUN，…）——A为矩阵，Margin设定待处理的维数，为1是横排（行），为2是竖排（列）做运算，Fun是运算函数sweep（x，2，apply（x，MARGIN=1，mean），FUN）——对数组或者矩阵进行运算。 MARGIN=1表示行，2表示列；STATS统计量，如apply（x，MARGIN=1，mean），FUN函数运算默认为减法，“/”除法y.vector&lt;-with(data,get(yval))——表示在data数据框中读取列名称为yval的向量。with(，)——提取数据框中的某些参数做运算，对于数据框运算很方便 绘图plot()——绘制图像plot(, , pch=as.integer()，col，xlab，ylab)——用factors区分图像点的类型pch（圆的，三角，叉），col是颜色类别，xlab或者ylab对应横纵轴标题legend(,legend=,pch=1:3，cex=1，col)——图例，是位置（比如右上），图例类别标签名，pch是图例对应标签的类别id（向量），调整字体比例大小，颜色设置，legend(“topright”, levels(), pch=1:length(levels(factors)))text(X，Y，labels=c(1,2,3),adj=1.2)——添加标注,X,Y是对应坐标的向量，labels是标记值，adj调整标注位置abline(h = ，lty=2)——低级绘图添加一条水平线h或者是回归模型直线，垂线v；lty为2表示绘制虚线abline(a,b)——画一条y=a+bx的直线points（x，y）————低级绘图，画个点，坐标为向量x，ylines（x，y）——低级绘图，画一条线，坐标为向量x，yaxis(side=1，at=seq(from=0.7，by=1.2，length.out=7)，labels=c（…）)——绘制坐标轴，低级绘图，side为2是纵坐标 barchart（）——lattice包预先要对数据汇总barplot()——绘制柱状图，vector可增加名称。也可以绘制直方图，和hist（）均分数据不太一样，需要用table（）统计各个子分段下样本数量后在画图。mosaicplot（x~y，main，color=T，xlab，ylab）——柱形对应关系图contour()——创建等高线persp(，expand=0.2）——创建3D图，expand扩展值设置为0.2，否则为全屏扩展image（volcano）——加载栅格（矩阵）图像 par（mfrow=c(1,2)，oma，mar）——mfrow设置图形输出窗口为1行2列，添加car包？oma是所有图像距离边框的距离(底部，左边，顶部，右边)，mar是每幅图像对边框的距离，默认是c(5, 4, 4, 2) + 0.1。lines(data)——（低级）原图中画线，data是由散点(x，y)组成rug（jitter（），side =2）——检验离群点数据，rug（）原图中执行绘图绘制在横坐标上，side为2是纵坐标，jitter()对绘制值略微调整，增加随机排序以避免标记值作图重合。 pairs(data)——数据框各个变量的散布图coplot(y~x|a+b)——多个变量时的散点图，在a，b（向量或是因子）的划分下的y与x的散点图scatterplotMatr()——散点图矩阵，car包 identify（）——交互式点选，单击图形中的点，将会输出对应数据的行号，右击结束交互stem(x，scale=1，width=80，atom=1e-08)——茎叶图,scale控制茎叶图的长度，为2即是以0~4为一组，5~9为一组将个位分成两部分，width是绘图宽度，atom是容差boxplot（）——箱图，研究变量的中心趋势，以及变量发散情况和离群值。上体顶部和底部为上下四分位数，中间粗线为中位数，上下伸出的垂直部分为数据的散步范围，最远点为1.5倍四分为点，超出后为异常点，用圆圈表示。boxplot(y~f,notch=TRUE,col=1:3,add=TRUE)#y是数据，f是由因子构成，notch是带有切口的箱型图，add=T图叠加到上一幅图。plot（f，y）——箱线图，f是因子，y是与f因子对应的数值bwplot（ ~ ，data，ylab）——lattice包的箱图，绘制不同factor下的y的箱图（条件绘图，在某个因子取值集合下的y值变化）bwplot（size~a1,data,panel=panel.bpplot,prob=seq(.01,.49,by=.01),datadensity=TRUE,ylab=’’）——Hmisc包的分位箱图earth.count(na.omit（x）,number=4,overlap=1/5)——连续变量x的离散化，把x转化为因子类型；number设置区间个数，overlap设置两个区间靠近边界的重合？每个区间的观测值相等stripplot(x1~y|x2)——lattice包的复杂箱图，存在两个因子x1,x2控制下的y, x2按照从左到右，从下到上的顺序排列，左下方的x2值较小 palette()——col取值对应的颜色， “black” “red” “green3” “blue” “cyan” “magenta” “yellow” “gray”colors（）——列出对应的颜色数组 qcc（）——qcc包，监控转化率型指标的质量监控图（P控制图），监控异常点，前提是二项分布足够大后趋于正态分布mosaic（，shade=T，legend=T）——绘制三级列联表，是三级列联表或者公式，vcd包 curve（sapply(x,)，，）——画曲线图，from和to设置横坐标取值范围 编辑optim(c(0,0),)——优化问题函数，c(0,0)是优化函数参数的初始值，返回值par是参数最优点值，value是参数的最优点时平方误差值，counts是返回执行输入函数func的次数以及梯度gradient的次数，convergence值为0表示有把握找到最优点，非0值时对应错误，message是一些其它信息。curve（sapply(x,)，，）——画曲线图，from和to设置横坐标取值范围 sample(length(x)，，replace=F)——采样，生成向量x的随机顺序的大小为的新向量；replace为False为不重复抽样，为True则重复抽样Round ——取整。精确ceiling()——取整，偏向数值小的floor() ——取整，偏向数值大的%/% ——整除 colnames(Data)[4]=”value”——更换某一列名edit（）——编辑数据表格fix（）——rm（x，y）——移除对象（变量）x和yna.exclude()——移除缺失数据整行na.omit()——删除缺失数据attr（na.omit（）,”na.action”）——返回向量a中元素为NA的下标na.fail（）——如果向量中至少包括1个NA值，则返回错误；如果不包括任何NA，则返回原有向量 merge(x = targets, y = infanty)——合并数据框，x和y是待合并数据框，相同属性字段也会合并在一起merge(x, y, by = intersect(names(x), names(y)),by.x = by, by.y = by, all = FALSE, all.x = all, all.y = all,sort = TRUE, suffixes = c(“.x”,”.y”),incomparables = NULL, …)merge函数参数的说明: x,y:用于合并的两个数据框 by,by.x,by.y:指定依据哪些行合并数据框,默认值为相同列名的列. all,all.x,all.y:指定x和y的行是否应该全在输出文件. sort:by指定的列是否要排序. suffixes:指定除by外相同列名的后缀. incomparables:指定by中哪些单元不进行合并. scale(x, center = TRUE, scale = TRUE)——中心化与标准化，center是中心化，scale是标准化。（全选：减去均值，再除以标准差） cut(x，breaks=c(0,10,30)，labels，ordered_result=F)——连续数据的离散化，将向量依据breaks区间分割为因子向量。labels设置返回因子向量的水平标签值，ordered_result为False生成的因子向量无大小意义，否则有大小意义 apply族函数apply(A，MARGIN，FUN，…)——处理对象A是矩阵或数组，MARGIN设定待计算的维数，FUN是某些函数，如mean，sum注：apply与其它函数不同，它并不能明显改善计算效率，因为它本身内置为循环运算。按列？lappy(dataframe，FUN，list(median,sd))——处理对象是向量、列表或其它对象，输出格式为列表listsapply(dataframe$Filed，FUN)——与lapply()相似，输出格式为矩阵（或数据框）按行？tapply(X, INDEX, FUN, simplify = TRUE) ——处理分组数据, INDEX和X是有同样长度的因子，simplify是逻辑变（量默认为T）aggregate(x~y+z, data，FUN)和by()——和tapply功能类似 其余参看：apply函数族 plyr库ddply(Data，.(user_id，item_id)，summarize，liulan=sum(liulan)）——split-apply-combine的一体化函数；.(user_id，item_id)作为每行的一对标识ID（因子），前面的“.”号省略数据框名称；summrize是一个函数fun；liulan是一个变量，最后生成的数据框只有user_id，item_id，liulan三列。详情参见例子 R语言利器之ddplytransform(x，y)——将x和y的列转换成·一个数据框。 reshape库（reshape2）melt（data，id.vars）——转换数据溶解。修改数据组织结构，创建一个数据矩阵，以id.var作为每行的编号，剩余列数据取值仅作为1列数值，并用原列名作为新数值的分类标记。cast（data, userid~itemid,value=”rattings”,fill=0）——统计转换数据，生成矩阵，公式~左边的作为行表名，右边的作为列表名。之后可以用cor（）计算每列数据之间的相关系数，并计算距离。acast 和 dcast（data, userid~itemid,value.var=”rattings”）——同上，reshape2包，acast最后生成数组，dcase生成数据框。参见 R语言进阶之4：数据整形（reshape） 字符串处理nchar()——获取字符串长度，它能够获取字符串的长度，它也支持字符串向量操作。注意它和length()的结果是有区别的？什么区别paste(“a”, “b”, sep=””)——字符串粘合，负责将若干个字符串相连结，返回成单独的字符串。其优点在于，就算有的处理对象不是字符型也能自动转为字符型。strsplit(A，split=’[,.]’) ——字符串分割，负责将字符串按照某种分割形式将其进行划分，它正是paste()的逆操作。substr(data,start,stop)——字符串截取，能对给定的字符串对象取出子集，其参数是子集所处的起始和终止位置。子集为从start到stop的下标区间grep()——字符串匹配，负责搜索给定字符串对象中特定表达式 ，并返回其位置索引。grepl()函数与之类似，但其后面的”l”则意味着返回的将是逻辑值regexpr（pattern,text）——从字符串text中提取特定的字符串的下标位置gregexpr（）——只查询匹配的第一个特定字符串的下标位置gsub(“a”,1,)——字符串替代，负责搜索字符串的特定表达式，并用新的内容加以替代。sub()函数——和gsub是类似的，但只替代第一个发现结果。chartr( )——字符串替换函数toupper( )、tolower( )及casefold( )——大小写转换函数其余参见：R语言中的字符串处理函数 控制流if—else——分支语句switch(index,case1,case2,casen)——index指示跳到第i个casei中for（i in ）——循环语句，通过控制变量iwhile——循环语句，通过设定循环范围repeat—break——循环语句，无限循环，由break跳出 特殊数据对象向量特性向量数组初始小标序号从1开始向量增加元素可以直接通过“vector[n+1]&lt;-0”方式增加 a&lt;-c()——向量初始化vector &lt;- numeric（）——创建初始向量个数，并赋初值为0length（vector）&lt;- leg——修改对象长度为legnames(vector) &lt;- c(“A”,”B”,”C”)——给向量起名称 vector[“A”]——通过名称访问对应元素 a == c(1, 99, 3)——比较每一个元素对应是否相等c（0，1）——创建向量，向量内元素类型应一致！seq（5，9）和 5：9 ——连续向量，等差数列seq（5，9，0.5）——以0.5为间隔创建seq(from,to,length,by) 数据索引which（is.na(var) == T）——返回对应数组序号which.max() 和 which.min()——返回数值类型中最大和最小元素下标subset（,，）——索引，是数据，是索引条件，colnames指定索引列名match（x，table，nomatch，incomparables）——匹配函数，返回x对应值在table中是否存在，并从1开始编号。x是查询对象，table是待匹配的向量，nomatch是不匹配项的设置值（默认为NA值），incomparables设置table表中不参加匹配的数值，默认为NULL %n% ——判断x中是否包含y，返回x对应的逻辑值 排序sort(x, decreasing = FALSE, na.last = NA, …)——排序，单变量排序，输出排序结果（不是序号）。na.last为TRUE，缺失值放在数据最后，为False 缺失值放在数据最前面，为NA，缺失数据将被移除sort.list()——排序输出序号值order()——排序，多个变量数据框排序，返回数据框序号数。order例子【结】结合ddply和transform函数，降序输出并，输出编号：ddply(dfx,.(group,sex),.fun=function(x){transform(x[order(x$age,decreasing=TRUE),c(1:3)],ind=1:length(group))}) rank()——秩排序，有重复数字的时候就用这个，根据数值之间的远近输出序号 rev()——依据下标从后往前倒排数据unique（）——返回无重复样本的数据集duplicated（x）——查找重复数据，重复序号返回为TRUE 比较大小pmin（x1,x2,…）——比较向量中的各元素，并把较小的元素组成新向量pmax（x1,x2,…）—— 向量间的交、并、补集union(x, y)——（并集）合并两组数据，x和y是没有重复的同一类数据，比如向量集intersect(x, y)——（交集）对两组数据求交集，x和y是没有重复的同一类数据，比如向量集setdiff(x, y)——（补集）x中与y不同的数据，x和y是没有重复的同一类数据，比如向量集，重复不同不记setequal(x, y)——判断x与y相同，返回逻辑变量，True为相同，False不同。x和y是没有重复的同一类数据，比如向量集is.element(x, y) 和 %n%——对x中每个元素，判断是否在y中存在，TRUE为x，y重共有的元素，Fasle为y中没有。x和y是没有重复的同一类数据，比如向量集 Vectorize()——将不能进行向量化预算的函数进行转化 矩阵array（data=NA,dim=length(data),dimnames=null）——数组、矩阵初始化,dim是数组各维的长度dimnames是数组维的名字，默认为空，array(1:20, dim=c(4,5))。数组是多维的，dim属性设置维数matrix(0, 3, 4)——0为赋初值，3行，4列，存储方式是先列后行！矩阵是二维的，用ncol和nrow设置矩阵的行数和列数。byrow设置存储方式（默认列优先），若为TRUE则以行优先dim（）&lt;- c(2,3)——设置矩阵为2行3列dimnames（）=list(c（），c（）)——设置参数行和列的名称，以列表的形式进行输入matrix[ ,4]——矩阵第4列as.vector(matrix)——将矩阵转换成向量a[“name1”,”name2”]——矩阵以行和列的名称来代替行列的下标，name1是行名，name2是列名 rbind（）——矩阵合并，按行合并，自变量宽度应该相等cbind（）——矩阵合并，安列合并，自变量高度应该相等 t()——矩阵转置det()——行列式solve（A，b）——求线性方程组Ax=bsolve（A）——求逆矩阵eigen（A） ——求距阵的特征值与特征向量，Ax=(Lambda)x，A$values是矩阵的特征值构成的向量，A$vectors是A的特征向量构成的矩阵*——矩阵中每个元素对应相乘%*%——矩阵相乘 因子因子和向量的区别：向量里面存的元素类型可以是字符型，而因子里面存的是整型数值对应因子的类别（levels）as.integer()——因子可以转化为整型levels()——查看因子类别gl（n，k，length）——因子,n为水平数，k为重复的次数，length为结果的长度factor(x，levels，labels)——因子as.factror()——将向量转化为无序因子，不能比较大小as.order()——将向量转化为有序因子is.factor()——判断是否为无序因子is.order()——判断是否为有序因子 列表和数据框list()——列表unlist()——列表转化为向量data.frame()——数据框names()——显示数据框的列名称dataframe[[2]] 和 dataframe[[“TheSec.Name”]] 和 dataframe$TheSec.Name——获取数据框第二列的元素值as.matrix()[，1]——把数据框转化为矩阵后，再去提取列向量 na和NULL的区别is.na()——判断na值存在，na是指该数值缺失但是存在。is.null（）——判断数据是否为NULL。NULL是指不存在，可以通过 train$var&lt;-NULL 的方法去掉属性变量var。 处理缺失数据na1、将缺失部分剔除2、用最高频率值来填补缺失值3、通过变量的相关关系来填补缺失值4、通过探索案例之间的相似性来填补缺失值 公式a:b——a和b的交互效应a+b——a和b的相加效应a*b——相加和交互效应（等价于a+b+a：b）-b——去掉b的影响1——y~1拟合一个没有因子影响的模型（仅仅是截距）-1——y~x-1表示通过原点的线性回归（等价于y~x+0或者0+y~x）^n——包含所有知道n阶的交互作用（a+b+c）^2==a+b+c+a:b+a:c+b:cpoly(a,n)——a的n阶多项式I(x1+x2)——表示模型y=b（x1+x2）+a 数理统计基础知识统计量mean（x，trim=0,na,rm=FALSE）——均值，trim去掉x两端观测值的便利，默认为0，即包括全部数据，na.rm=TRUE允许数据中有缺失weighted.mean(x，)——加权平均值，weigth表示对应权值median——中值quantile(x，probs=seq(,,))——计算百分位数，是五数总和的扩展，probs设置分位数分位点，用seq(0,1,0.2)设置，表示以样本值*20%为间隔划分数据。var（）——样本方差（n-1）sd——样本标准差（n-1）cov——协方差cor——相关矩阵fivenum(x,na.rm=TRUE)——五数总括：中位数，下上四分位数，最小值，最大值 数学函数sum（x,y,z，na.rm=FALSE）——x+y+z，na.rm为TURE可以忽略掉na值数据sum（x&gt;4）——统计向量x中数值大于4的个数rep（“LOVE！”，）——重复times次，rep(1:3，c（1，2，3）)表示1个1，2个2，3个3组成的序列sqrt（）——开平方函数2^2 和 **——“^”幂运算abs（）——绝对值函数‘%%’——表示求余‘%/%’——求商（整数） exp ： 2.71828…expm1 ： 当x的绝对值比1小很多的时候，它将能更加正确的计算exp(x)-1log ： 对数函数（自然对数）log10 ： 对数（底为10）函数（常用对数）log2 ： 对数（底为2）函数因为10&gt;e&gt;1，常用对数比自然对数更接近横坐标轴xlog1p()——log（1+p），用来解决对数变换时自变量p=0的情况。指数和对数的变换得出任何值的0次幂都是1特性：对数螺旋图。当图像呈指数型增长时，常对等式的两边同时取对数已转换成线性关系。 sin ： 正弦函数cos ： 余弦函数tan ： 正切函数asin ： 反正弦函数acos ： 反余弦函数atan ： 反正切函数sinh ： 超越正弦函数cosh ： 超越余弦函数tanh ： 超越正切函数asinh ： 反超越正弦函数acosh ： 反超越余弦函数atanh ： 反超越正切函数logb ： 和log函数一样log1px ： 当x的绝对值比1小很多的时候，它将能更加正确的计算log(1+x)gamma ： Γ函数（伽玛函数）lgamma ： 等同于log(gamma(x))ceiling ： 返回大于或等于所给数字表达式的最小整数floor ： 返回小于或等于所 给数字表达式的最大整数trunc ： 截取整数部分round ： 四舍五入signif(x,a) ： 数据截取函数 x：有效位 a：到a位为止圆周率用 ‘pi’表示 crossprod(A,B)——A %*% t(B) ，内积tcrosspeod(A,B)——t(A) %*% B，外积%*%——内积，a1b1+a2b2+…+anbn=abcos，crossprod(x)表示x与x的内积。||x||2，矩阵相乘%o%——外积，absin（矩阵乘法，叉积），tcrossprod(x,y)表示x与y的外积。表示矩阵中对应元素的乘积！向量内积（点乘）和向量外积（叉乘） 正态分布dnorm（x，mean=0,sd=1,log=FALSE）——正态分布的概率密度函数pnorm(x，mean=0,sd=1)——返回正态分布的分布函数·rnorm（n，mean=0.sd=1）——生成n个正态分布随机数构成的向量qnorm()——下分为点函数 qqnorm（data）——画出qq散点图qqline（data）——低水平作图，用qq图的散点画线qq.plot（，main=’’）——qq图检验变量是否为正态分布 简单分析summary()——描述统计摘要，和 Hmisc()包的describe()类似，会显示NA值，四分位距是第1个（25%取值小于该值）和第3个四分位数（75%取值小于该值）的差值（50%取值的数值），可以衡量变量与其中心值的偏离程度，值越大则偏离越大。 table($)——统计datafame数据中属性变量var的数值取值频数(NA会自动去掉！)，列联表table(, )——比较两个data_var，为列，为行，先列后行！xtabs(formular，data)——列联表ftable( table())——三维列联表prop.table()——统计所占百分比例prop.table(table(, )，)——比较两个data_var所占百分比，填1位按行百分计算，2为列计算margin.table( table()， )——计算列联表的边际频数（边际求和）,=1为按列变量addmargin.table（table()， ）——计算列联表的边际频数（边际求和）并求和,=1为按列变量 as.formula()——转换为一个R公式，是一个字符串循环时的判断语句：ifelse(, , )——if，else的变种，test是判断语句,其中的判断变量可以是一个向量！yes是True时的赋值，no是False时的赋值 hist(，prob=T，xlab=’横坐标标题’，main=’标题’，ylim=0:1，freq，breaks=seq(0,550,2))——prob=T表示是频率直方图，在直角坐标系中，用横轴每个小区间对应一个组的组距，纵轴表示频率与组距的比值，直方图面积之和为1；prob位FALSE表示频数直方图；ylim设置纵坐标的取值范围；freq为TRUE绘出频率直方图，counts绘出频数直方图，FALSE绘出密度直方图。breaks设置直方图横轴取点间隔，如seq(0,550,2)表示间隔为2，从0到550之间的数值。 density(,na.rm=T)——概率密度函数（核密度估计，非参数估计方法），用已知样本估计其密度,作图为lines(density(data),col=”blue”)ecdf（data）——经验分布函数,作图plot(ecdf(data),verticasl=FALSE,do.p=FALSE)，verticals为TRUE表示画竖线，默认不画。do.p=FALSE表示不画点处的记号 假设检验分布函数shapiro.test(data)——正态W检验方法，当p值大于a为正态分布ks.test(x,y)——经验分布的K-S检验方法，比较x与y的分布是否相同，y是与x比较的数据向量或者是某种分布的名称，ks.test(x, rnorm(length(x), mean(x), sd(x)))，或ks.test(x,”pnorm”,mean(x),sd(x)) chisq.test(x，y，p)——Pearson拟合优度X2（卡方）检验，x是各个区间的频数，p是原假设落在小区间的理论概率，默认值表示均匀分布,要检验其它分布，比如正态分布时先构造小区间，并计算各个区间的概率值，方法如下：brk&lt;-cut(x,br=c(-6,-4,-2,0,2,4,6,8))#切分区间A&lt;-table(brk)#统计频数 p&lt;-pnorm(c(-4,-2,0,2,4,6,8),mean(x),sd(x))#构造正态分布函数p&lt;-c(p[1],p[2]-p[1],p[3]-p[2],p[4]-p[3],p[5]-p[4],p[6]-p[5],p[7]-p[6])#计算各个区间概率值 chisq.test(A,p=p) 正态总体的均值方差t.test(x，y，alternative=c(“two.sided”,”less”,”greater”)，var.equal=FALSE)——单个正态总体均值μ或者两个正态总体均值差μ1-μ2的区间估计；alternative表示备择假设：two.side（默认）是双边检验，less表示H1:μ&lt;μ0，greater表示H1：μ&gt;μ0的单边检验(μ0表示原假设)；当var.equal=TRUE时，则是双样本方差相同的情况，默认为不同var.test(x，y)——双样本方差比的区间估计 独立性检验（原假设H0：X与Y独立）chisq.test(x,correct=FALSE)——卡方检验，x为矩阵，dim(x)=c(2,2)，对于大样本（频数大于5）fisher.test()——单元频数小于5，列联表为2*2 相关性检验（原假设H0：X与Y相互独立）cor.test（x,y,method=c(“pearson”,”kendall”,”spearman”)）——相关性检验，观察p-value小于0.05则相关。method选择相关性检验方法 秩rank()——秩统计量cor.test（）——秩相关检验：Spearman，Kendallwilcox.test(x,y=NULL，mu,alternative，paired=FALSE，exact=FALSE,correct=FALSE，conf.int=FALSE)——秩显著性检验（一个样本来源于总体的检验，显著性差异的检验），Wilcoxon秩和检验（非成对样本的秩次和检验）,mu是待检测参数，比如中值，paired逻辑变量，说明变量x，y是否为成对数据，exact说民是否精确计算P值，correct是逻辑变量，说明是否对p值采用连续性修正，conf.int是逻辑变量，给出相应的置信区间。 uniroot(f，interval=c(1,2))——求一元方程根的函数，f是方程，interval是求解根的区间内，返回值root为解optimize(）或 optimise（）——求一维变量函数的极小点nlm（f，p）——求解无约束问题，求解最小值，f是极小的目标函数，p是所有参数的初值，采用Newton型算法求极小，函数返回值是一个列表，包含极小值、极小点的估计值、极小点处的梯度、Hesse矩阵以及求解所需的迭代次数等。 显著性差异检验（方差分析，原假设：相同，相关性）mcnemar.test(x,y，correct=FALSE)——相同个体上的两次检验，检验两元数据的两个相关分布的频数比变化的显著性，即原假设是相关分布是相同的。y是又因子构成的对象，当x是矩阵时此值无效。binom.test(x，n，p，alternative=c(“two.sided”,”less”,”greater”)，conf.level=0.95)——二项分布，符号检验（一个样本来源于总体的检验，显著性差异的检验） aov（x~f）——计算方差分析表，x是与（因子）f对应因素水平的取值，用summary（）函数查看信息aov（x~A+B+A：B）——双因素方差，其中X~A+B中A和B是不同因素的水平因子（不考虑交互作用），A：B代表交互作用生成的因子p.adjust()——P值调整函数pairwise.t.test(x，g，p.adjust.method=”holm”)——多重t检验,p.adjust.method是P值的调整方法，其方法由p.adjust（）给出，默认值按Holm方法（”holm“）调整，若为”none“，表示P值不做任何调整。双因素交互作用时g=A：Bshapiro.test（x）——数据的正态W检验bartlett.test（x~f，data）——Bartlett检验，方差齐性检验kruskal.test（x~f，data）——Kruskal-Wallis秩和检验，非参数检验法，不满足正态分布friedman.test(x，f1，f2，data）——Friedman秩和检验，不满足正态分布和方差齐性，f1是不同水平的因子，f2是试验次数的因子 常用模型1、回归模型lm（y~.，）——线性回归模型，“.”代表数据中所有除y列以外的变量，变量可以是名义变量（虚拟变量，k个水平因子，生成k-1个辅助变量（值为0或1））summary（）——给出建模的诊断信息：1、数据拟合的残差（Residual standard error，RSE），残差应该符合N（0，1）正态的，值越小越好2、检验多元回归方程系数（变量）的重要性，t检验法，Pr&gt;|t|, Pr值越小该系数越重要（拒绝原假设）3、多元R方或者调整R2方，标识模型与数据的拟合程度，即模型所能解释的数据变差比例，R方越接近1模型拟合越好，越小，越差。调整R方考虑回归模型中参数的数量，更加严格4、检验解释变量x与目标变量y之间存在的依赖关系，统计量F，用p-value值，p值越小越好5、绘图检验plot()——绘制线性模型，和qq.plot误差的正态QQ图6、精简线性模型，向后消元法 线性回归模型基础lm（formula=x~y，data，subset）——回归分析，x是因变量（响应变量），y是自变量（指示变量），formular=y~x是公式，其中若是有x^2项时，应把公式改写为y~I(x^2)，subset为可选择向量，表示观察值的子集。例：lm(Y ~ X1 + X2 + I(X2^2) + X1:X2, data = data)predict(lm(y~x)，new，interval=“prediction”，level=0.95)——预测，new为待预测的输入数据，其类型必须为数据框data.frame，如new&lt;-data.frame(x=7)，interval=“prediction”表示同时要给出相应的预测区间predict(lm(y~x))——直接用用原模型的自变量做预测，生成估计值 筛选模型自变量lm.new&lt;-update(lm.sol，sqrt(.)~.)——修正原有的回归模型，将响应变量做开方变换update（, .~. - x1）——移除变量x1后的模型coef(lm.new)——提取回归系数 回归诊断1、正态性（QQ图）plot(x,which)——回归模型残差图，which=1~4分别代表画普通残差与拟合值的残差图，画正态QQ的残差图，画标准化残差的开方与拟合值的残差图，画Cook统norm.test（）——正态性检验，p-value&gt;0.05为正态 计量的残差图residuals()和resid()——残差rstandard()——标准化残差rstudent()——学生化残差influence.measures(model)——model是由lm或者glm构成的对象，对回归诊断作总括，返回列表中包括，广义线性模型也可以使用 anova（）——简单线性模型拟合的方差分析（确定各个变量的作用）anova（,）——比较两个模型（检验原假设为不同） 2、误差的独立性——car包提供Duerbin_Watson检验函数3、线性——car包crPlots（）绘制成分残差图（偏残差图）可以看因变量与自变量之间是否呈线性4、同方差性——car包ncvTest（）原假设为误差方差不变，若拒绝原假设，则说明存在异方差性5、多重共线性——car包中的vif（）函数计算VIF方差膨胀因子，一般vif&gt;2存在多重共线性问题 异常点分析（影响分析）hatvalues（）和hat（）——帽子矩阵dffits（）——DFFITS准则cooks.distance()——Cook统计量，值越大越有可能是异常值点covratio（）——COVRATIO准则 kappa（z，exact=FALSE）——多重共线性，计算矩阵的条件数k,若k]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寻找神奇|相信的心，就是你的魔法]]></title>
    <url>%2F2017%2F11%2F20%2F171120%E7%9B%B8%E4%BF%A1%E7%9A%84%E5%BF%83%E5%B0%B1%E6%98%AF%E4%BD%A0%E7%9A%84%E9%AD%94%E6%B3%95%2F</url>
    <content type="text"><![CDATA[寻找神奇|相信的心，就是你的魔法文|氦核这部作品是靠表情包认识的。在空间里某个不知名的说说中，我第一次看见了极具嘲讽意味的亚可。第一眼，我就认定，这个表情很适合我。 我写东西，为了得到最深刻的共鸣，专门选择刻画描绘世界上各种各样的痛苦。生老病死，五蕴盛，求不得，爱别离……怎么扎心怎么写，怎么细腻怎么写。于是，我的QQ头像就变成了一张嘲讽状的表情包。 这个头像用了很久，突然有一天，有人称赞我头像亚可好评，那时我才认识了活在表情包的这个角色。又过了很久，我看到一些剧照，觉得画风还不错，这才有了补番的想法。待我真正开始看时，已经半年有余了。 有人说小魔女学园这部动漫有些低龄向，画风党。我今天还是暂且避开这些，说说其他的。 先聊聊亚可。亚可是个什么样的人呢？要我说，她是真的粗糙。粗糙到像块土。生活不在意细节，说话不过大脑，没有形象感，甚至很多时候执拗到让人心生嫌弃。就是这么一个让人不是很喜欢，一身锋芒的中性孩子，却有着独特的魅力。她粗糙且大条地活着，但她明白自己应该做的是什么，理解周围朋友们的心情。她不在意细节，从来不在意自己目标以外的东西，有什么说什么总能切中肯綮。她沉不住气，只有三分钟热度，但是这三分钟热度里她一定是全力以赴。她执拗到悲哀，她相信着只要去做就一定能做到，她相信只要她相信就一定能够成为带给大家欢笑的魔女。这么一瞧，亚可的这些缺点也正是她的优点。 我看完这部动漫，觉得并没有那么魔幻，我觉得这动漫写得全都是生活，设定也轻松地接受了下来。小魔女学园里的魔女们，为了学习而学习着，没有什么突出的信念，有的也只是背负和延续传统。小魔女们为了学习魔法而学习魔法，考试就是学园里最大的考验。而一套严厉的师生关系更是助长了孩子们的麻木。魔女们有着自己的家族，这先天的条件也禁锢了她们的想法和欲望。功利主义在魔女间横行，用简单的评价来断定人的能力，拉帮结派的小圈子，这种种无谓的劣行完全可以看出，这是一个缺乏梦想的时代。是一个灰色的时代。 带来梦想的，首先是融合科学魔法的克罗娃老师。不谈她的计划，单说她这一套WIFI路由器魔力理论，将沉浸在迂腐和顽固中的传统粉碎。魔女们开始接受新的知识，新的理念，所有的进步都被好奇所推动着。令人欣喜的事，环境以肉眼可见的速度变化着，不仅魔女们理解着适应着改变，普通人也接受着改变和新事物。人们的梦想逐渐苏醒，不再纠结执着于现实和客观，在一些事以后也竟然开始相信了。 相信，就是魔法。相信，也是梦想的开端。这部动漫最终的意义，就在于叫醒我们睡着的梦想。近代，鲁迅荷戟独徬徨，战斗在麻木的中国人里，试图叫醒那些沉睡的人。他不停地骂着，骂着那些明明能够选择相信却选择了放弃或怀疑的知识分子们……那个时候充满了让革命家无力的死水和泥潭，充满了无辜的无知的人，充满了失去生活意义的悲哀者——活着若只是为了活着，苟在那里消磨时间，那种人……不提也罢。 当然，弊端也是有的，而且不小。梦想家，革命家，浪漫主义者，他们往往过于理想。他们在走向梦想的时候，会被居心叵测的人利用。闪光夏莉欧的表演，最终让自己也走向崩溃。梦想是多么容易聚集啊，只需要一点相信，就能收获一片温暖。影片和书籍用浅薄的梦想地打动了无数消费者，空想者江山代有。人们也因此厌倦了感动，泪点提高了，笑点变低了，人们习惯于快节奏地感动一下，回头给自己五星好评的页面一个截图，今晚的朋友圈大概会内容丰富。于是不再有人真的相信梦想，不再有人傻到执着于某件不可能的事。这些，都只能在作品里见到了。 总是这样。要是生活中有一个像亚可一样的人跟在你身边，恐怕人们会烦死吧。毕竟你有那么多事情要做，要为了考试学习，为了工作面试，为了评奖去参赛，为了活得更好更舒服而努力变强……但我还是希望，我们能这样对待身边的亚可——即便这个人总是搞砸，即便这个人拖你后腿，甚至即便她忘记了最开始的目标拉着你去做无关且无聊的事，也请你要耐着性子用心感受这个人的温度。这份温度，就是我们的明天。 我总说，人们努力工作，除了要填补自己，还要用梦想填补身处的世界。每个人的工作都有意义，工作填补着我们的心，让我们和生活实打实地接触着，不至于飘飘摇摇。每个人的生活也都不是完全独立的，我们受着环境的影响，我们不断调整自己以便适应这个环境，社会功利一点，我们也只能随波逐流。最最可悲的是，胜负对我们来说太重要了，我们都希望自己变得优秀和能干，不能接受自己最终平凡。 也许亚可最终平凡，但是她贵在平凡。正因为她的平凡，她才能找到自己的魔法。回过头来再看看那一句经典台词——相信的心就是你的魔法——还是有不小的吸引力让人想要去相信它的鬼扯的。用数学的眼光看，相信着但是实现不了会带来些许的失望，但一旦实现就能获得无穷的幸福，因此相信的“期望”是正无穷。 希望这点文字能成为某种力量。 （完）]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TVZ天火三玄变]]></title>
    <url>%2F2017%2F01%2F01%2F1130TVZ%E4%B8%89%E6%9D%BF%E6%96%A7%2F</url>
    <content type="text"><![CDATA[TVZ天火三玄变4.7.1鉴于天梯环境恶劣，TVZ对抗中虫族在运营上比人族更加强势，所以天梯虫族的九阳神功三开狗毒爆偷逼很多。 如何让这套九阳神功成为九阳豆浆机，就成了每个人族玩家应该思考的问题。 正常开，兵营气矿，气矿满采，兵营好时求稳拉第一个农民看对手开局，基地变星轨（都是基操，勿6）兵营造死神（建议送死神开）同时可以拍下二基地。 星轨变好路口补一个房子，探路农民停在三矿，两分十秒下vf，接着有气就给兵营挂上双倍挂件。采矿14农民时下二气（也是满采）。死神随便骚一骚，不建议送，停在另一个三矿或者跑一跑。 vf换上挂件，开始产火车（不停）。下vs挂科技挂件。农民不停，兵营好兄弟不停（大概能造三个）。 三分钟看三矿。 三分二十秒之前极限下va。vs造运输机。两火车带死神骚一骚，剩下的火车藏在家防绕狗。尽早开始堵二矿口，同时小心卡人口。。。（多么痛的领悟） 第一变 青莲变 极限你爹侠六火车（家里四个，外面俩）带运输机（刚造好）三枪兵（运营好一点可能更多些。。。） 前压 停火车，兵营和vs挂科技挂件（升级兴奋剂和隐形女妖）出枪兵和俩女妖，下三基地。vf起飞再造一个双倍挂件 五个女王是干不过火车侠带运输机的，对面送狗最好不过。 火车接着骚的时候，家里vf起飞再挂科技，出坦克。农民补两兵营（在空挂件上）和两be（正常运营攻防） 火车顶脸。可以死，但是要多换卵换狗换农民，杀女王。（卡毒爆虫巢timing，不会有毒爆虫出现。。。） 第二变 天地变 速接两女妖三基地变星轨时就可以下三四气。如果防守不够猛烈，小心飞龙开 家里开始出坦克，枪兵。6分十秒升攻防。两女妖出门，补到五兵营。下防空（防飞龙开） vs起飞挂双倍出运输机，三基地飞出来 女妖骑脸！！！杀杀杀 对面变眼虫 三基地下两气，造房子堵三矿路口，两兵营下双倍挂件 第三变 琉璃变 坦克连环拳两坦克出门，后续兵等在家 这波兵有啥打啥，坦克架住，清房子，压基地。谨记：毒爆来了就装船。。。 九分钟左右家里补到八兵营，双vf坦克，余钱拍基地。攻防不要忘。一百五十人口一拳超人。 总结没什么好总结了，干死z，找回属于泰伦的荣光！！！]]></content>
      <tags>
        <tag>文章</tag>
      </tags>
  </entry>
</search>
